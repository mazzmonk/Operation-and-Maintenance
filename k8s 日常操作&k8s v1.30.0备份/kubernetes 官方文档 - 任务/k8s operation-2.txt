索引:
----------------------------------------------------------------------------------------------------------------
第二部分(2/16)

# 配置 Pods 和 容器
## 为容器和 Pod 分配内存资源 - 包含 metrics server 安装配置
此章节展示如何将内存请求(request)和内存限制(limit)分配给一个容器.我们保障容器拥有它请求数量的内存,但不允许使用超过限制数量的内存.

说明: 可以通过 kubectl api-resources 来查看可以使用的资源状态
比如:
$ kubectl get limitranges
$ kubectl get resourcequotas
等等

* 创建命名空间
创建一个命名空间,以便将本练习中创建的资源与集群的其余部分隔离.
$ kubectl create namespace mem-example

* 指定内存请求和限制
要为容器指定内存请求,请在容器资源清单中包含 resources: requests 字段.同理,要指定内存限制,请包含 resources: limits.

创建一个拥有一个容器的 Pod.容器将会请求 100 MiB 内存,并且内存会被限制在 200 MiB 以内.这是 Pod 的配置文件:

memory-request-limit.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      requests:
        memory: "100Mi"
      limits:
        memory: "200Mi"
    command: ["stress"]
    args: ["--vm","1","--vm-bytes","150M","--vm-hang","1"]
---------------------------

创建 pod
$ kubectl apply -f memory-request-limit.yaml --namespace=mem-example

验证 Pod 中的容器是否已运行:
$ kubectl get pod memory-demo --namespace=mem-example

查看 Pod 相关的详细信息:
$ kubectl get pod memory-demo --output=yaml --namespace=mem-example

输出结果显示: 该 Pod 中容器的内存请求为 100 MiB,内存限制为 200 MiB.
...
resources:
  requests:
    memory: 100Mi
  limits:
    memory: 200Mi
...

运行 kubectl top 命令,获取该 Pod 的指标数据:
$ kubectl top pod memory-demo --namespace=mem-example

NAME          CPU(cores)   MEMORY(bytes)
memory-demo   93m          151Mi

要使用 kubectl top 必须有 metrics server 收集指标,优先安装配置,请优先阅读如下安装部分
^^^^^^^^^^^^^^^^^^^^
配置参考
https://github.com/kubernetes-sigs/metrics-server

* 配置 metrics server
$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml

注意: 配置文件中的 Deployment 部分中的 replicas: 2 这个数值需要按照实际的集群节点修改.

配置文件中这部分显示 metrics server 启动需要的参数,默认情况下需要使用证书
...
containers:
- args:
  - --cert-dir=/tmp
  - --secure-port=10250
  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  - --kubelet-use-node-status-port
  - --metric-resolution=15s
  image: registry.k8s.io/metrics-server/metrics-server:v0.7.1
...

在启动时候,有如下的报错:
$ kubectl describe pod metrics-server-57986d8677-s7dpv -n kube-system
...
Warning  Unhealthy  7s(x2 over 17s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500

$ kubectl logs metrics-server-57986d8677-s7dpv -n kube-system
...
Error from server: Get "https://192.168.1.13:10250/containerLogs/kube-system/metrics-server-57986d8677-s7dpv/metrics-server": remote error: tls: internal error

解决:
目前的 kubeadm 流程中,kubelet 的 Bootstrap 因为节点动态的原因,已经不再自动完成 Kubelet 服务端点的证书签发了,使用统一 CA 自行签署,或者恢复 Bootstrap 中的服务证书申请流程,也就能完成任务了.

通过 kubeadm 启动的 kubelet 配置文件是 /var/lib/kubelet/config.yaml,加入一行: serverTLSBootstrap: true
...
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
serverTLSBootstrap: true

重启 kubelet 服务,重新部署
$ kubectl apply -f high-availability-1.21+.yaml

会发现出现了新的 CSR
$ kubectl get csr
NAME        AGE     SIGNERNAME                      REQUESTOR           REQUESTEDDURATION   CONDITION
csr-dhpjn   5m25s   kubernetes.io/kubelet-serving   system:node:k8s02   <none>              Pending
csr-k8cth   6m52s   kubernetes.io/kubelet-serving   system:node:k8s01   <none>              Pending
csr-v2kcz   4m2s    kubernetes.io/kubelet-serving   system:node:k8s03   <none>              Pending

签证新的请求
$ kubectl certificate approve csr-dhpjn
certificatesigningrequest.certificates.k8s.io/csr-dhpjn approved

$ kubectl certificate approve csr-k8cth
certificatesigningrequest.certificates.k8s.io/csr-k8cth approved

$ kubectl certificate approve csr-v2kcz
certificatesigningrequest.certificates.k8s.io/csr-v2kcz approved

稍等片刻,再次执行 kubectl top nodes
$ kubectl top nodes

NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
k8s01   249m         12%    2024Mi          35%
k8s02   145m         7%     865Mi           22%
k8s03   150m         7%     959Mi           25%

vvvvvvvvvvvvvvvvvvvv

* 超过容器限制的内存
当节点拥有足够的可用内存时,容器可以使用其请求的内存.但是,容器不允许使用超过其限制的内存.如果容器分配的内存超过其限制,该容器会成为被终止的候选容器.如果容器继续消耗超出其限制的内存,则终止容器.如果终止的容器可以被重启,则 kubelet 会重新启动它,就像其他任何类型的运行时失败一样.

创建一个 Pod,尝试分配超出其限制的内存.这是一个 Pod 的配置文件,其拥有一个容器,该容器的内存请求为 50 MiB,内存限制为 100 MiB:

memory-request-limit-2.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo-2
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-2-ctr
    image: polinux/stress
    resources:
      requests:
        memory: "50Mi"
      limits:
        memory: "100Mi"
    command: ["stress"]
    args: ["--vm","1","--vm-bytes","250M","--vm-hang","1"]
---------------------------

$ kubectl apply -f memory-request-limit-2.yaml
$ kubectl get pod memory-demo-2 --namespace=mem-example

此时,容器可能正在运行或被杀死.重复前面的命令,直到容器被杀掉:

NAME            READY     STATUS      RESTARTS   AGE
memory-demo-2   0/1       OOMKilled   1          24s
...

   Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Mon,08 Jul 2024 09:47:51 +0000
      Finished:     Mon,08 Jul 2024 09:47:51 +0000
    Ready:          False
    Restart Count:  5
    Limits:
      memory:  100Mi
    Requests:
      memory:     50Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hp4gh(ro)
...

上述的过程反复,反复.

* 超过整个节点容量的内存
内存请求和限制是与容器关联的,但将 Pod 视为具有内存请求和限制,也是很有用的.Pod 的内存请求是 Pod 中所有容器的内存请求之和.同理,Pod 的内存限制是 Pod 中所有容器的内存限制之和.

Pod 的调度基于请求.只有当节点拥有足够满足 Pod 内存请求的内存时,才会将 Pod 调度至节点上运行.

创建一个 Pod,其内存请求超过了你集群中的任意一个节点所拥有的内存.这是该 Pod 的配置文件,其拥有一个请求 1000 GiB 内存的容器,这应该超过了你集群中任何节点的容量.

memory-request-limit-3.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo-3
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-3-ctr
    image: polinux/stress
    resources:
      requests:
        memory: "1000Gi"
      limits:
        memory: "1000Gi"
    command: ["stress"]
    args: ["--vm","1","--vm-bytes","150M","--vm-hang","1"]
---------------------------

$ kubectl apply -f memory-request-limit-3.yaml
$ kubectl get pod memory-demo-3 --namespace=mem-example
$ kubectl describe pod memory-demo-3 --namespace=mem-example

输出显示内存不足

Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  27s   default-scheduler  0/3 nodes are available: 3 Insufficient memory.preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.

* 内存单位
内存资源的基本单位是字节(byte).你可以使用这些后缀之一,将内存表示为 纯整数或定点整数: E、P、T、G、M、K、Ei、Pi、Ti、Gi、Mi、Ki.例如,下面是一些近似相同的值:

128974848,129e6,129M,123Mi

* 如果你没有指定内存限制
如果你没有为一个容器指定内存限制,则自动遵循以下情况之一:
.容器可无限制地使用内存.容器可以使用其所在节点所有的可用内存,进而可能导致该节点调用 OOM Killer.此外,如果发生 OOM Kill,没有资源限制的容器将被杀掉的可行性更大.
.运行的容器所在命名空间有默认的内存限制,那么该容器会被自动分配默认限制.集群管理员可用使用 LimitRange 来指定默认的内存限制.

* 内存请求和限制的目的
通过为集群中运行的容器配置内存请求和限制,你可以有效利用集群节点上可用的内存资源.通过将 Pod 的内存请求保持在较低水平,你可以更好地安排 Pod 调度.通过让内存限制大于内存请求,你可以完成两件事:
.Pod 可以进行一些突发活动,从而更好的利用可用内存.
.Pod 在突发活动期间,可使用的内存被限制为合理的数量.


## 为容器和 Pods 分配 CPU 资源
如何为容器设置 CPU request(请求) 和 CPU limit(限制).容器使用的 CPU 不能超过所配置的限制.如果系统有空闲的 CPU 时间,则可以保证给容器分配其所请求数量的 CPU 资源.

查看 metrics-server(或者其他资源指标 API metrics.k8s.io 服务提供者)是否正在运行,请键入以下命令:
$ kubectl get apiservices

如果资源指标 API 可用,则会输出将包含一个对 metrics.k8s.io 的引用.
NAME
v1beta1.metrics.k8s.io

* 指定 CPU 请求和 CPU 限制
要为容器指定 CPU 请求,请在容器资源清单中包含 resources: requests 字段.要指定 CPU 限制,请包含 resources:limits.

创建一个具有一个容器的 Pod.容器将会请求 0.5 个 CPU,而且最多限制使用 1 个 CPU.这是 Pod 的配置文件:

cpu-request-limit.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo
  namespace: cpu-example
spec:
  containers:
  - name: cpu-demo-ctr
    image: vish/stress
    resources:
      limits:
        cpu: "1"
      requests:
        cpu: "0.5"
    args:
    - -cpus
    - "2"
---------------------------

配置文件的 args 部分提供了容器启动时的参数.-cpus "2" 参数告诉容器尝试使用 2 个 CPU.

创建 Pod:
$ kubectl apply -f cpu-request-limit.yaml --namespace=cpu-example

验证所创建的 Pod 处于 Running 状态
$ kubectl get pod cpu-demo --namespace=cpu-example

查看显示关于 Pod 的详细信息:
$ kubectl get pod cpu-demo --output=yaml --namespace=cpu-example

输出显示 Pod 中的一个容器的 CPU 请求为 500 milliCPU,并且 CPU 限制为 1 个 CPU.
...
resources:
  limits:
    cpu: "1"
  requests:
    cpu: 500m
...

使用 kubectl top 命令来获取该 Pod 的指标:
$ kubectl top pod cpu-demo --namespace=cpu-example
NAME       CPU(cores)   MEMORY(bytes)
cpu-demo   992m         4Mi

回想一下,通过设置 -cpu "2",你将容器配置为尝试使用 2 个 CPU,但是容器只被允许使用大约 1 个 CPU.容器的 CPU 用量受到限制,因为该容器正尝试使用超出其限制的 CPU 资源.

说明:
CPU 使用率低于 1.0 的另一种可能的解释是,节点可能没有足够的 CPU 资源可用.回想一下,此练习的先决条件需要你的集群至少具有 1 个 CPU 可用.如果你的容器在只有 1 个 CPU 的节点上运行,则容器无论为容器指定的 CPU 限制如何,都不能使用超过 1 个 CPU.

CPU 单位
CPU 资源以 CPU 单位度量.Kubernetes 中的一个 CPU 等同于:
.1 个 AWS vCPU
.1 个 GCP核心
.1 个 Azure vCore
.裸机上具有超线程能力的英特尔处理器上的 1 个超线程

小数值是可以使用的.一个请求 0.5 CPU 的容器保证会获得请求 1 个 CPU 的容器的 CPU 的一半.你可以使用后缀 m 表示毫.例如 100m CPU、100 milliCPU 和 0.1 CPU 都相同.精度不能超过 1m.

CPU 请求只能使用绝对数量,而不是相对数量.0.1 在单核、双核或 48 核计算机上的 CPU 数量值是一样的.

* 设置超过节点能力的 CPU 请求
CPU 请求和限制与都与容器相关,但是我们可以考虑一下 Pod 具有对应的 CPU 请求和限制这样的场景.Pod 对 CPU 用量的请求等于 Pod 中所有容器的请求数量之和.同样,Pod 的 CPU 资源限制等于 Pod 中所有容器 CPU 资源限制数之和.

Pod 调度是基于资源请求值来进行的.仅在某节点具有足够的 CPU 资源来满足 Pod CPU 请求时,Pod 将会在对应节点上运行:

创建一个 Pod,该 Pod 的 CPU 请求对于集群中任何节点的容量而言都会过大.下面是 Pod 的配置文件,其中有一个容器.容器请求 100 个 CPU,这可能会超出集群中任何节点的容量.

cpu-request-limit-2.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: cpu-demo-2
  namespace: cpu-example
spec:
  containers:
  - name: cpu-demo-ctr-2
    image: vish/stress
    resources:
      limits:
        cpu: "100"
      requests:
        cpu: "100"
    args:
    - -cpus
    - "2"
---------------------------

创建 Pod
$ kubectl apply -f cpu-request-limit-2.yaml --namespace=cpu-example

查看该 Pod 的状态
$ kubectl get pod cpu-demo-2 --namespace=cpu-example

输出显示 Pod 状态为 Pending.也就是说,Pod 未被调度到任何节点上运行,并且 Pod 将无限期地处于 Pending 状态:
NAME         READY   STATUS    RESTARTS   AGE
cpu-demo-2   0/1     Pending   0          8s

查看有关 Pod 的详细信息,包含事件:
$ kubectl describe pod cpu-demo-2 --namespace=cpu-example

输出显示由于节点上的 CPU 资源不足,无法调度容器:
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  21s   default-scheduler  0/3 nodes are available: 3 Insufficient cpu.preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.

* 如果不指定 CPU 限制
如果你没有为容器指定 CPU 限制,则会发生以下情况之一:
.容器在可以使用的 CPU 资源上没有上限.因而可以使用所在节点上所有的可用 CPU 资源.
.容器在具有默认 CPU 限制的名字空间中运行,系统会自动为容器设置默认限制.集群管理员可以使用 LimitRange 指定 CPU 限制的默认值.

* 如果你设置了 CPU 限制但未设置 CPU 请求
如果你为容器指定了 CPU 限制值但未为其设置 CPU 请求,Kubernetes 会自动为其 设置与 CPU 限制相同的 CPU 请求值.类似的,如果容器设置了内存限制值但未设置 内存请求值,Kubernetes 也会为其设置与内存限制值相同的内存请求.

* CPU 请求和限制的初衷
通过配置你的集群中运行的容器的 CPU 请求和限制,你可以有效利用集群上可用的 CPU 资源.通过将 Pod CPU 请求保持在较低水平,可以使 Pod 更有机会被调度.通过使 CPU 限制大于 CPU 请求,你可以完成两件事:
.Pod 可能会有突发性的活动,它可以利用碰巧可用的 CPU 资源.
.Pod 在突发负载期间可以使用的 CPU 资源数量仍被限制为合理的数量.

## 配置 Pod 的服务质量
怎样配置 Pod 以让其归属于特定的 服务质量类(Quality of Service class,QoS class).Kubernetes 在 Node 资源不足时使用 QoS 类来就驱逐 Pod 作出决定.

Kubernetes 创建 Pod 时,会将如下 QoS 类之一设置到 Pod 上:
.Guaranteed
.Burstable
.BestEffort

* 创建名字空间
$ kubectl create namespace qos-example

* 创建一个 QoS 类为 Guaranteed 的 Pod
对于 QoS 类为 Guaranteed 的 Pod:
.Pod 中的每个容器都必须指定内存限制和内存请求.
.对于 Pod 中的每个容器,内存限制必须等于内存请求.
.Pod 中的每个容器都必须指定 CPU 限制和 CPU 请求.
.对于 Pod 中的每个容器,CPU 限制必须等于 CPU 请求.

这些限制同样适用于初始化容器和应用程序容器.临时容器(Ephemeral Container)无法定义资源,因此不受这些约束限制.

下面是包含一个 Container 的 Pod 清单.该 Container 设置了内存请求和内存限制,值都是 200 MiB.该 Container 设置了 CPU 请求和 CPU 限制,值都是 700 milliCPU:

qos-pod.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
---------------------------

创建 Pod
$ kubectl apply -f qos-pod.yaml --namespace=qos-example

查看 Pod 详情:
$ kubectl get pod qos-demo --namespace=qos-example --output=yaml

结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Guaranteed.结果也确认了 Pod 容器设置了与内存限制匹配的内存请求,设置了与 CPU 限制匹配的 CPU 请求.
...
spec:
  containers:
    ...
    resources:
      limits:
        cpu: 700m
        memory: 200Mi
      requests:
        cpu: 700m
        memory: 200Mi
    ...
status:
  qosClass: Guaranteed
...

说明:
如果某 Container 指定了自己的内存限制,但没有指定内存请求,Kubernetes 会自动为它指定与内存限制相等的内存请求.同样,如果容器指定了自己的 CPU 限制,但没有指定 CPU 请求,Kubernetes 会自动为它指定与 CPU 限制相等的 CPU 请求.

* 创建一个 QoS 类为 Burstable 的 Pod
如果满足下面条件,Kubernetes 将会指定 Pod 的 QoS 类为 Burstable:
.Pod 不符合 Guaranteed QoS 类的标准.
.Pod 中至少一个 Container 具有内存或 CPU 的请求或限制.- 注意这里是"或",即是某一个

下面是包含一个 Container 的 Pod 清单.该 Container 设置的内存限制为 200 MiB,内存请求为 100 MiB.

qos-pod-2.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
---------------------------

创建 Pod:
$ kubectl apply -f qos-pod-2.yaml --namespace=qos-example

查看 Pod 详情:
$ kubectl get pod qos-demo-2 --namespace=qos-example --output=yaml

结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable:
...
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-2-ctr
    resources:
      limits:
        memory: 200Mi
      requests:
        memory: 100Mi
  ...
status:
  qosClass: Burstable
...

* 创建一个 QoS 类为 BestEffort 的 Pod
对于 QoS 类为 BestEffort 的 Pod,Pod 中的 Container 必须没有设置内存和 CPU 限制或请求.

下面是包含一个 Container 的 Pod 清单.该 Container 没有设置内存和 CPU 限制或请求.

qos-pod-3.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
---------------------------

创建 Pod
$ kubectl apply -f qos-pod-3.yaml --namespace=qos-example

查看 Pod 详情:
$ kubectl get pod qos-demo-3 --namespace=qos-example --output=yaml

结果表明 Kubernetes 为 Pod 配置的 QoS 类为 BestEffort.

...
spec:
  containers:
    ...
    resources: {}
  ...
status:
  qosClass: BestEffort
...

* 创建包含两个容器的 Pod
下面是包含两个 Container 的 Pod 清单.一个 Container 指定内存请求为 200 MiB.另外一个 Container 没有指定任何请求或限制.

qos-pod-4.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-4
  namespace: qos-example
spec:
  containers:

  - name: qos-demo-4-ctr-1
    image: nginx
    resources:
      requests:
        memory: "200Mi"

  - name: qos-demo-4-ctr-2
    image: redis
---------------------------

注意此 Pod 满足 Burstable QoS 类的标准.也就是说它不满足 Guaranteed QoS 类标准,因为它的 Container 之一设有内存请求.

创建 Pod
$ kubectl apply -f qos-pod-4.yaml --namespace=qos-example

查看 Pod 详情:
$ kubectl get pod qos-demo-4 --namespace=qos-example --output=yaml

结果表明 Kubernetes 为 Pod 配置的 QoS 类为 Burstable:
...
spec:
  containers:
    ...
    name: qos-demo-4-ctr-1
    resources:
      requests:
        memory: 200Mi
    ...
    name: qos-demo-4-ctr-2
    resources: {}
    ...
status:
  qosClass: Burstable
...

* 检视 Pod 的 QoS 类
你也可以只查看你所需要的字段,而不是查看所有字段
$ kubectl --namespace=qos-example get pod qos-demo-4 -o jsonpath='{ .status.qosClass}{"\n"}'

* 环境清理
删除名字空间:
$ kubectl delete namespace qos-example

注意: 如果不单独删除 pod,则上述命令会级联删除,意味着将删除和 namespace 有关的所有内容


## 调整分配给容器的 CPU 和内存资源 - 需要开启 feature-gates 的 InPlacePodVerticalScaling,此特性用于在不重启 Pod 或其容器的情况下调整分配给运行中 Pod 容器的 CPU 和内存资源,通过 kubeadm 升级配置的方式失败,暂时搁置
特性状态:  Kubernetes v1.27 [alpha] - 目前 v1.30.0 仍然是 alpha

先查阅上一章 ### 配置 Pod 的服务质量

在不重启 Pod 或其容器的情况下调整分配给运行中 Pod 容器的 CPU 和内存资源.Kubernetes 节点会基于 Pod 的 requests 为 Pod 分配资源,并基于 Pod 的容器中指定的 limits 限制 Pod 的资源使用.

要为正在运行的 Pod 更改资源分配量,需要启用 InPlacePodVerticalScaling 特性门控.并让工作负载控制器(Deployment,StatefulSet,DaemonSet)创建一个具有不同资源需求的新 Pod.

对于原地调整 Pod 资源而言:
.针对 CPU 和内存资源的容器的 requests 和 limits 是可变更的.
.Pod 状态中 containerStatuses 的 allocatedResources 字段反映了分配给 Pod 容器的资源.
.Pod 状态中 containerStatuses 的 resources 字段反映了如同容器运行时所报告的、针对正运行的容器配置的实际资源 requests 和 limits.
.Pod 状态中 resize 字段显示上次请求待处理的调整状态.此字段可以具有以下值:
  .Proposed: 此值表示请求调整已被确认,并且请求已被验证和记录.
  .InProgress: 此值表示节点已接受调整请求,并正在将其应用于 Pod 的容器.
  .Deferred: 此值意味着在此时无法批准请求的调整,节点将继续重试.当其他 Pod 退出并释放节点资源时,调整可能会被真正实施.
  .Infeasible: 此值是一种信号,表示节点无法承接所请求的调整值.如果所请求的调整超过节点可分配给 Pod 的最大资源,则可能会发生这种情况.

你必须在控制平面和集群中的所有节点上启用 InPlacePodVerticalScaling 特性门控.

* 容器调整策略
调整策略允许更精细地控制 Pod 中的容器如何针对 CPU 和内存资源进行调整.例如,容器的应用程序可以处理 CPU 资源的调整而不必重启,但是调整内存可能需要应用程序重启,因此容器也必须重启.

为了实现这一点,容器规约允许用户指定 resizePolicy.针对调整 CPU 和内存可以设置以下重启策略:
.NotRequired: 在运行时调整容器的资源.
.RestartContainer: 重启容器并在重启后应用新资源.

如果未指定 resizePolicy[*].restartPolicy,则默认为 NotRequired.

说明:
如果 Pod 的 restartPolicy 为 Never,则 Pod 中所有容器的调整重启策略必须被设置为 NotRequired.

下面的示例显示了一个 Pod,其中 CPU 可以在不重启容器的情况下进行调整,但是内存调整需要重启容器.
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-5
  namespace: qos-example
spec:
  containers:
    - name: qos-demo-ctr-5
      image: nginx
      resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: RestartContainer
      resources:
        limits:
          memory: "200Mi"
          cpu: "700m"
        requests:
          memory: "200Mi"
          cpu: "700m"
---------------------------

说明:
在上述示例中,如果所需的 CPU 和内存请求或限制已更改,则容器将被重启以调整其内存.


## 为容器分派扩展资源
特性状态:  Kubernetes v1.30 [stable]

请优先阅读如下(为节点发布扩展资源)部分
^^^^^^^^^^^^^^^^^^^^
### 为节点发布扩展资源
如何为节点指定扩展资源(Extended Resource).扩展资源允许集群管理员发布节点级别的资源,这些资源在不进行发布的情况下无法被 Kubernetes 感知.

* 在你的一个节点上发布一种新的扩展资源
为在一个节点上发布一种新的扩展资源,需要发送一个 HTTP PATCH 请求到 Kubernetes API server.例如: 假设你的一个节点上带有四个 dongle 资源.下面是一个 PATCH 请求的示例,该请求为你的节点发布四个 dongle 资源.

PATCH /api/v1/nodes/<your-node-name>/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "add",
    "path": "/status/capacity/example.com~1dongle",
    "value": "4"
  }
]

注意: Kubernetes 不需要了解 dongle 资源的含义和用途.前面的 PATCH 请求告诉 Kubernetes 你的节点拥有四个你称之为 dongle 的东西.

启动一个代理(proxy),以便你可以很容易地向 Kubernetes API server 发送请求:
$ kubectl proxy

在另一个命令窗口中,发送 HTTP PATCH 请求.用你的节点名称替换 <your-node-name>:
$ curl --header "Content-Type: application/json-patch+json" \
  --request PATCH \
  --data '[{"op": "add","path": "/status/capacity/example.com~1dongle","value": "4"}]' \
  http://localhost:8001/api/v1/nodes/<your-node-name>/status

说明:
在前面的请求中,~1 为 patch 路径中 “/” 符号的编码.JSON-Patch 中的操作路径值被解析为 JSON 指针.

输出显示该节点的 dongle 资源容量(capacity)为 4:
  "status": {
    "capacity": {
      "cpu": "2",
      "ephemeral-storage": "15371208Ki",
      "example.com/dongle": "4",
      "hugepages-2Mi": "0",
      "memory": "4000664Ki",
      "pods": "110"
    },


描述你的节点:
$ kubectl describe node <your-node-name>

输出再次展示了 dongle 资源:
Capacity:
  cpu:                 2
  ephemeral-storage:   15371208Ki
  example.com/dongle:  4
  hugepages-2Mi:       0
  memory:              4000664Ki
  pods:                110

* 讨论
扩展资源类似于内存和 CPU 资源.例如,正如一个节点拥有一定数量的内存和 CPU 资源,它们被节点上运行的所有组件共享,该节点也可以拥有一定数量的 dongle 资源,这些资源同样被节点上运行的所有组件共享.此外,正如应用开发者可以创建请求一定数量的内存和 CPU 资源的 Pod,他们也可以创建请求一定数量 dongle 资源的 Pod.

扩展资源对 Kubernetes 是不透明的.Kubernetes 不知道扩展资源含义相关的任何信息.Kubernetes 只了解一个节点拥有一定数量的扩展资源.扩展资源必须以整形数量进行发布.例如,一个节点可以发布 4 个 dongle 资源,但是不能发布 4.5 个.

* 存储示例
假设一个节点拥有一种特殊类型的磁盘存储,其容量为 800 GiB.你可以为该特殊存储创建一个名称,如 example.com/special-storage.然后你就可以按照一定规格的块(如 100 GiB)对其进行发布.在这种情况下,你的节点将会通知它拥有八个 example.com/special-storage 类型的资源.
Capacity:
 ...
 example.com/special-storage: 8

如果你想要允许针对特殊存储任意(数量)的请求,你可以按照 1 字节大小的块来发布特殊存储.在这种情况下,你将会发布 800Gi 数量的 example.com/special-storage 类型的资源.
Capacity:
 ...
 example.com/special-storage:  800Gi

然后,容器就能够请求任意数量(多达 800Gi)字节的特殊存储.
Capacity:
 ...
 example.com/special-storage:  800Gi

* 清理
这里是一个从节点移除 dongle 资源发布的 PATCH 请求.

PATCH /api/v1/nodes/<your-node-name>/status HTTP/1.1
Accept: application/json
Content-Type: application/json-patch+json
Host: k8s-master:8080

[
  {
    "op": "remove",
    "path": "/status/capacity/example.com~1dongle",
  }
]

启动一个代理,以便你可以很容易地向 Kubernetes API 服务器发送请求:
$ kubectl proxy

在另一个命令窗口中,发送 HTTP PATCH 请求.用你的节点名称替换 <your-node-name>:
$ curl --header "Content-Type: application/json-patch+json" \
  --request PATCH \
  --data '[{"op": "remove","path": "/status/capacity/example.com~1dongle"}]' \
  http://localhost:8001/api/v1/nodes/<your-node-name>/status

验证 dongle 资源的发布已经被移除:
$ kubectl describe node <your-node-name> | grep dongle

应该看不到任何输出

vvvvvvvvvvvvvvvvvvvv

注意:
在进行如下步骤之前需要先在节点上发布扩展资源,就是进行上面的内容,否则在给 pod 部署资源的时候,pod 无法识别资源

* 给 Pod 分派扩展资源
要请求扩展资源,需要在你的容器清单中包括 resources:requests 字段.扩展资源可以使用任何完全限定名称,只是不能使用 *.kubernetes.io/.有效的扩展资源名的格式为 example.com/foo,其中 example.com 应被替换为 你的组织的域名,而 foo 则是描述性的资源名称.

下面是包含一个容器的 Pod 配置文件:

extended-resource-pod.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: extended-resource-demo
spec:
  containers:
  - name: extended-resource-demo-ctr
    image: nginx
    resources:
      requests:
        example.com/dongle: 3
      limits:
        example.com/dongle: 3
---------------------------

在配置文件中,你可以看到容器请求了 3 个 dongles.

创建 Pod:
$ kubectl apply -f extended-resource-pod.yaml

检查 Pod 是否运行正常:
$ kubectl get pod extended-resource-demo

描述 Pod:
$ kubectl describe pod extended-resource-demo

输出结果显示 dongle 请求如下:
Limits:
  example.com/dongle: 3
Requests:
  example.com/dongle: 3

说明:
上述的 pod 只会在有上述资源的节点才会建立,就是说只有部署了上一章中的结点资源的 node 才会创建 pod.

* 尝试创建第二个 Pod
下面是包含一个容器的 Pod 配置文件,容器请求了 2 个 dongles.

extended-resource-pod-2.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: extended-resource-demo-2
spec:
  containers:
  - name: extended-resource-demo-2-ctr
    image: nginx
    resources:
      requests:
        example.com/dongle: 2
      limits:
        example.com/dongle: 2
---------------------------
Kubernetes 将不能满足 2 个 dongles 的请求,因为第一个 Pod 已经使用了 4 个可用 dongles 中的 3 个.

尝试创建 Pod:
$ kubectl apply -f extended-resource-pod-2.yaml

描述 Pod:
$ kubectl describe pod extended-resource-demo-2

输出结果表明 Pod 不能被调度,因为没有一个节点上存在两个可用的 dongles.
Conditions:
  Type    Status
  PodScheduled  False
...
Events:
  ...
  Warning  FailedScheduling  64s(x2 over 2m43s)  default-scheduler  0/3 nodes are available: 3 Insufficient example.com/dongle.preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.

查看 Pod 的状态:
$ kubectl get pod extended-resource-demo-2

输出结果表明 Pod 虽然被创建了,但没有被调度到节点上正常运行.Pod 的状态为 Pending:
NAME                       READY     STATUS    RESTARTS   AGE
extended-resource-demo-2   0/1       Pending   0          6m

## 配置 Pod 以使用卷进行存储 - - 其中有关于 emptyDir 类型卷的使用
只要容器存在,容器的文件系统就会存在,因此当一个容器终止并重新启动,对该容器的文件系统改动将丢失.对于独立于容器的持久化存储,你可以使用卷.这对于有状态应用程序尤为重要,例如键值存储(如 Redis)和数据库.

* 为 Pod 配置卷
创建一个运行 Pod,该 Pod 仅运行一个容器并拥有一个类型为 emptyDir 的卷,在整个 Pod 生命周期中一直存在,即使 Pod 中的容器被终止和重启.以下是 Pod 的配置:

redis.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}            # 此种类型的卷只在容器存在的时候才会保存,比如重启容器,如果删除容器,会将目录及其中的内容全部删除
---------------------------

1.创建 Pod:
$ kubectl apply -f redis.yaml

2.验证 Pod 中的容器是否正在运行,然后留意 Pod 的更改:
kubectl get pod redis --watch

输出如下:
NAME      READY     STATUS    RESTARTS   AGE
redis     1/1       Running   0          13s

3.在另一个终端,用 Shell 连接正在运行的容器:
$ kubectl exec -it redis -- /bin/bash

4.在你的 Shell 中,切换到 /data/redis 目录下,然后创建一个文件:
root@redis:/data# cd /data/redis/
root@redis:/data/redis# echo Hello > test-file

5.在你的 Shell 中,列出正在运行的进程:
root@redis:/data/redis# apt-get update
root@redis:/data/redis# apt-get install procps
root@redis:/data/redis# ps aux

输出类似于:
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
redis        1  0.1  0.1  33308  3828 ?        Ssl  00:46   0:00 redis-server *:6379
root        12  0.0  0.0  20228  3020 ?        Ss   00:47   0:00 /bin/bash
root        15  0.0  0.0  17500  2072 ?        R+   00:48   0:00 ps aux

6.在你的 Shell 中,结束 Redis 进程:
root@redis:/data/redis# kill <pid>

其中 <pid> 是 Redis 进程的 ID(PID).

7.在你原先终端中,留意 Redis Pod 的更改.最终你将会看到和下面类似的输出:
NAME      READY     STATUS     RESTARTS   AGE
redis     1/1       Running    0          13s
redis     0/1       Completed  0         6m
redis     1/1       Running    1         6m

此时,容器已经终止并重新启动.这是因为 Redis Pod 的 restartPolicy 为 Always.
1.用 Shell 进入重新启动的容器中:
$ kubectl exec -it redis -- /bin/bash

2.在你的 Shell 中,进入到 /data/redis 目录下,并确认 test-file 文件是否仍然存在.
root@redis:/data/redis# cd /data/redis/
root@redis:/data/redis# ls
test-file

3.删除为此练习所创建的 Pod:
$ kubectl delete pod redis

## 配置 Pod 以使用 PersistentVolume 作为存储
配置 Pod 使用 PersistentVolumeClaim 作为存储.以下是该过程的总结:
1.你作为集群管理员创建由物理存储支持的 PersistentVolume.你不会将该卷与任何 Pod 关联.
2.你现在以开发人员或者集群用户的角色创建一个 PersistentVolumeClaim,它将自动绑定到合适的 PersistentVolume.
3.你创建一个使用以上 PersistentVolumeClaim 作为存储的 Pod.

* 在你的节点上创建一个 index.html 文件
打开集群中的某个节点的 Shell.如何打开 Shell 取决于集群的设置.例如,如果你正在使用 Minikube,那么可以通过输入 minikube ssh 来打开节点的 Shell.

在该节点的 Shell 中,创建一个 /mnt/data 目录:

# 这里假定你的节点使用 "sudo" 来以超级用户角色执行命令
$ sudo mkdir /mnt/data

在 /mnt/data 目录中创建一个 index.html 文件:

# 这里再次假定你的节点使用 "sudo" 来以超级用户角色执行命令
sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html"

测试 index.html 文件确实存在:
$ cat /mnt/data/index.html

输出应该是:
Hello from Kubernetes storage

现在你可以关闭节点的 Shell 了.

* 创建 PersistentVolume
创建一个 hostPath 类型的 PersistentVolume.Kubernetes 支持用于在单节点集群上开发和测试的 hostPath 类型的 PersistentVolume.hostPath 类型的 PersistentVolume 使用节点上的文件或目录来模拟网络附加存储.

在生产集群中,你不会使用 hostPath.集群管理员会提供网络存储资源,比如 Google Compute Engine 持久盘卷、NFS 共享卷或 Amazon Elastic Block Store 卷.集群管理员还可以使用 StorageClass 来设置动态制备存储.

下面是 hostPath PersistentVolume 的配置文件:

pv-volume.yaml
---------------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---------------------------

此配置文件指定卷位于集群节点上的 /mnt/data 路径.其配置还指定了卷的容量大小为 10 GB,访问模式为 ReadWriteOnce,这意味着该卷可以被单个节点以读写方式安装.此配置文件还在 PersistentVolume 中定义了 StorageClass 的名称为 manual.它将用于将 PersistentVolumeClaim 的请求绑定到此 PersistentVolume.

说明:
为了简化,本示例采用了 ReadWriteOnce 访问模式.然而对于生产环境,Kubernetes 项目建议改用 ReadWriteOncePod 访问模式.

创建 PersistentVolume:
$ kubectl apply -f pv-volume.yaml

查看 PersistentVolume 的信息:
$ kubectl get pv task-pv-volume

NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Available           manual         <unset>                          9s

* 创建 PersistentVolumeClaim
下一步是创建一个 PersistentVolumeClaim.Pod 使用 PersistentVolumeClaim 来请求物理存储.
创建一个 PersistentVolumeClaim,它请求至少 3 GB 容量的卷,该卷一次最多可以为一个节点提供读写访问.

下面是 PersistentVolumeClaim 的配置文件:

pv-claim.yaml
---------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
---------------------------

创建 PersistentVolumeClaim:
$ kubectl apply -f pv-claim.yaml

创建 PersistentVolumeClaim 之后,Kubernetes 控制平面将查找满足申领要求的 PersistentVolume.如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume,则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上.

再次查看 PersistentVolume 信息:
$ kubectl get pv task-pv-volume

现在输出的 STATUS 为 Bound.
NAME             CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                   STORAGECLASS   REASON    AGE
task-pv-volume   10Gi       RWO           Retain          Bound     default/task-pv-claim   manual                   2m
查看 PersistentVolumeClaim:
$ kubectl get pvc task-pv-claim

输出结果表明该 PersistentVolumeClaim 绑定了你的 PersistentVolume task-pv-volume.
NAME            STATUS    VOLUME           CAPACITY   ACCESSMODES   STORAGECLASS   AGE
task-pv-claim   Bound     task-pv-volume   10Gi       RWO           manual         30s

* 创建 Pod
下一步是创建一个使用你的 PersistentVolumeClaim 作为存储卷的 Pod.

下面是此 Pod 的配置文件:
pv-pod.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
---------------------------

注意 Pod 的配置文件指定了 PersistentVolumeClaim,但没有指定 PersistentVolume.对 Pod 而言,PersistentVolumeClaim 就是一个存储卷.

创建 Pod:
$ kubectl apply -f pv-pod.yaml

检查 Pod 中的容器是否运行正常:
$ kubectl get pod task-pv-pod

打开一个 Shell 访问 Pod 中的容器:
$ kubectl exec -it task-pv-pod -- /bin/bash

在Shell 中,验证 Nginx 是否正在从 hostPath 卷提供 index.html 文件:
# 一定要在上一步 "kubectl exec" 所返回的 Shell 中执行下面三个命令
apt update
apt install curl
curl http://localhost/

输出结果是你之前写到 hostPath 卷中的 index.html 文件中的内容:
Hello from Kubernetes storage

如果你看到此消息,则证明你已经成功地配置了 Pod 使用 PersistentVolumeClaim 的存储.

注意:
需要说明的是,上述的测试是在一个有 3 个节点的集群(1 master/k8s01,2 worker/(k8s02,k8s03))上完成,就有一个有趣的现象,在某一台机器上建立好本地挂载目录及文件 /mnt/data/index.html,及建立好 pv,pvc 以后,用 pod 挂载 pvc 的时候,在启动 pod 的那个节点上会建立 /mnt/data 目录,但是没有 index.html 文件
即: 在 k8s01 建立目录及文件 /mnt/data/index.html 用于 pv,pvc,但是 pod 启动在  k8s02 节点上,此节点会生成 /mnt/data 目录,但是没有 index.html 文件,当测试完成以后删除 pod,pvc,pv 以后,k8s02 上的 /mnt/data 目录仍然存在.

* 清理
删除 Pod、PersistentVolumeClaim 和 PersistentVolume 对象:
$ kubectl delete pod task-pv-pod
$ kubectl delete pvc task-pv-claim
$ kubectl delete pv task-pv-volume

如果你还没有连接到集群中节点的 Shell,可以按之前所做操作,打开一个新的 Shell.

在节点的 Shell 上,删除你所创建的目录和文件:
# 这里假定你使用 "sudo" 来以超级用户的角色执行命令
sudo rm /mnt/data/index.html
sudo rmdir /mnt/data

你现在可以关闭连接到节点的 Shell.

* 访问控制 - 配置上没有任何问题,但是不知道用什么方式确认 GID 在 pod 中的
使用组 ID(GID)配置的存储仅允许 Pod 使用相同的 GID 进行写入.GID 不匹配或缺失将会导致无权访问错误.为了减少与用户的协调,管理员可以对 PersistentVolume 添加 GID 注解.这样 GID 就能自动添加到使用 PersistentVolume 的任何 Pod 中.

使用 pv.beta.kubernetes.io/gid 注解的方法如下所示:
---------------------------
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
  annotations:
    pv.beta.kubernetes.io/gid: "1234"
---------------------------

当 Pod 使用带有 GID 注解的 PersistentVolume 时,注解的 GID 会被应用于 Pod 中的所有容器,应用的方法与 Pod 的安全上下文中指定的 GID 相同.每个 GID,无论是来自 PersistentVolume 注解还是来自 Pod 规约,都会被应用于每个容器中运行的第一个进程.

说明:
当 Pod 使用 PersistentVolume 时,与 PersistentVolume 关联的 GID 不会在 Pod 资源本身的对象上出现.

## 配置 Pod 使用投射卷作存储 - 将 secret、configMap、downwardAPI 和 serviceAccountToken 卷混合在一起作为投射卷
怎样通过 projected 卷将现有的多个卷资源挂载到相同的目录.当前,secret、configMap、downwardAPI 和 serviceAccountToken 卷可以被投射.

说明:
serviceAccountToken 不是一种卷类型.

* 为 Pod 配置投射卷
使用本地文件来创建用户名和密码 Secret,然后创建运行一个容器的 Pod,该 Pod 使用projected 卷将 Secret 挂载到相同的路径下.

1.创建 Secret:
# 创建包含用户名和密码的文件:  - 注意这里的的文件会被复制到 pod 中
$ echo -n "admin" > ./username.txt
$ echo -n "1f2d1e2e67df" > ./password.txt

# 在 Secret 中引用上述文件  - 在 yaml 文件中记录关键字 user/pass
$ kubectl create secret generic user --from-file=./username.txt
$ kubectl create secret generic pass --from-file=./password.txt

2.创建 Pod:

projected.yaml
---------------------------
apiVersion: v1
kind: Pod
metadata:
  name: test-projected-volume
spec:
  containers:
  - name: test-projected-volume
    image: busybox:1.28
    args:
    - sleep
    - "86400"
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:      #  注意这里表明是一个 projected 卷
      sources:
      - secret:
          name: user  # kubectl create secret 命令创建的关键字
      - secret:
          name: pass  # kubectl create secret 命令创建的关键字
---------------------------

$ kubectl apply -f projected.yaml

3.确认 Pod 中的容器运行正常,然后监视 Pod 的变化:
$ kubectl get --watch pod test-projected-volume

输出结果和下面类似:
NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s

4.在另外一个终端中,打开容器的 shell:
$ kubectl exec -it test-projected-volume -- /bin/sh

5.在 shell 中,确认 projected-volume 目录包含你的投射源:
$ ls /projected-volume/

* 清理
删除 Pod 和 Secret:
$ kubectl delete pod test-projected-volume
$ kubectl delete secret user pass

## 为 Pod 或容器配置安全上下文
安全上下文(Security Context)定义 Pod 或 Container 的特权与访问控制设置.安全上下文包括但不限于:
.自主访问控制(Discretionary Access Control): 基于用户 ID(UID)和组 ID(GID) 来判定对对象(例如文件)的访问权限.
.安全性增强的 Linux(SELinux): 为对象赋予安全性标签.
.以特权模式或者非特权模式运行.
.Linux 权能: 为进程赋予 root 用户的部分特权而非全部特权.
.AppArmor: 使用程序配置来限制个别程序的权能.
.Seccomp: 过滤进程的系统调用.
.allowPrivilegeEscalation: 控制进程是否可以获得超出其父进程的特权.此布尔值直接控制是否为容器进程设置 no_new_privs标志.当容器满足一下条件之一时,allowPrivilegeEscalation 总是为 true:
  .以特权模式运行,或者
  .具有 CAP_SYS_ADMIN 权能
.readOnlyRootFilesystem: 以只读方式加载容器的根文件系统.

以上条目不是安全上下文设置的完整列表 -- 请参阅 SecurityContext(https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.30/#securitycontext-v1-core)了解其完整列表.

* 为 Pod 设置安全性上下文
要为 Pod 设置安全性设置,可在 Pod 规约中包含 securityContext 字段.securityContext 字段值是一个 PodSecurityContext 对象.你为 Pod 所设置的安全性配置会应用到 Pod 中所有 Container 上.下面是一个 Pod 的配置文件,该 Pod 定义了 securityContext 和一个 emptyDir 卷:

security-context.yaml
------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo
spec:
  securityContext:        # 这里是 pod 层面
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    image: busybox:1.28
    command: [ "sh","-c","sleep 1h" ]
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
    securityContext:                    # 这里 container 层面
      allowPrivilegeEscalation: false
------------------------------------------

在配置文件中,runAsUser 字段指定 Pod 中的所有容器内的进程都使用用户 ID 1000 来运行.runAsGroup 字段指定所有容器中的进程都以主组 ID 3000 来运行.如果忽略此字段,则容器的主组 ID 将是 root(0).当 runAsGroup 被设置时,所有创建的文件也会划归用户 1000 和组 3000.由于 fsGroup 被设置,容器中所有进程也会是附组 ID 2000 的一部分.卷 /data/demo 及在该卷中创建的任何文件的属主都会是组 ID 2000.

创建该 Pod:
$ kubectl apply -f security-context.yaml

检查 Pod 的容器处于运行状态:
$ kubectl get pod security-context-demo

开启一个 Shell 进入到运行中的容器:
$ kubectl exec -it security-context-demo -- sh

在你的 Shell 中,列举运行中的进程:
$ ps

输出显示进程以用户 1000 运行,即 runAsUser 所设置的值:
PID   USER     TIME  COMMAND
    1 1000      0:00 sleep 1h
    6 1000      0:00 sh
...

在你的 Shell 中,进入 /data 目录列举其内容:
$ cd /data
$ ls -l
输出显示 /data/demo 目录的组 ID 为 2000,即 fsGroup 的设置值:
drwxrwsrwx 2 root 2000 4096 Jun  6 20:08 demo

在你的 Shell 中,进入到 /data/demo 目录下创建一个文件:
$ cd demo
$ echo hello > testfile

列举 /data/demo 目录下的文件:
$ ls -l
输出显示 testfile 的组 ID 为 2000,也就是 fsGroup 所设置的值:
-rw-r--r-- 1 1000 2000 6 Jun  6 20:08 testfile

运行下面的命令:
$ id

输出类似于:
uid=1000 gid=3000 groups=2000

从输出中你会看到 gid 值为 3000,也就是 runAsGroup 字段的值.如果 runAsGroup 被忽略,则 gid 会取值 0(root),而进程就能够与 root 用户组所拥有以及要求 root 用户组访问权限的文件交互.

退出你的 Shell:
$ exit

* 为 Pod 配置卷访问权限和属主变更策略
特性状态: Kubernetes v1.23 [stable]

默认情况下,Kubernetes 在挂载一个卷时,会递归地更改每个卷中的内容的属主和访问权限,使之与 Pod 的 securityContext 中指定的 fsGroup 匹配.对于较大的数据卷,检查和变更属主与访问权限可能会花费很长时间,降低 Pod 启动速度.你可以在 securityContext 中使用 fsGroupChangePolicy 字段来控制 Kubernetes 检查和管理卷属主和访问权限的方式.

fsGroupChangePolicy - fsGroupChangePolicy 定义在卷被暴露给 Pod 内部之前对其 内容的属主和访问许可进行变更的行为.此字段仅适用于那些支持使用 fsGroup 来 控制属主与访问权限的卷类型.此字段的取值可以是:
.OnRootMismatch:只有根目录的属主与访问权限与卷所期望的权限不一致时,才改变其中内容的属主和访问权限.这一设置有助于缩短更改卷的属主与访问 权限所需要的时间.
.Always:在挂载卷时总是更改卷中内容的属主和访问权限.

例如:
-------------------------------------------
securityContext:
  runAsUser: 1000
  runAsGroup: 3000
  fsGroup: 2000
  fsGroupChangePolicy: "OnRootMismatch"
-------------------------------------------
说明:
此字段对于 secret、 configMap 和 emptydir 这类临时性存储无效.

* 将卷权限和所有权更改委派给 CSI 驱动程序
特性状态: Kubernetes v1.26 [stable]

如果你部署了一个容器存储接口(CSI)驱动,而该驱动支持 VOLUME_MOUNT_GROUP NodeServiceCapability,在 securityContext 中指定 fsGroup 来设置文件所有权和权限的过程将由 CSI 驱动而不是 Kubernetes 来执行.在这种情况下,由于 Kubernetes 不执行任何所有权和权限更改,fsGroupChangePolicy 不会生效,并且按照 CSI 的规定,CSI 驱动应该使用所指定的 fsGroup 来挂载卷,从而生成了一个对 fsGroup 可读/可写的卷.

* 为 Container 设置安全性上下文
若要为 Container 设置安全性配置,可以在 Container 清单中包含 securityContext 字段.securityContext 字段的取值是一个 SecurityContext 对象.你为 Container 设置的安全性配置仅适用于该容器本身,并且所指定的设置在与 Pod 层面设置的内容发生重叠时,会重写 Pod 层面的设置.Container 层面的设置不会影响到 Pod 的卷.- 注意: 在一个 pod 的 yaml 文件中可以配置多个 container

下面是一个 Pod 的配置文件,其中包含一个 Container.Pod 和 Container 都有 securityContext 字段:

security-context-2.yaml
--------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-2
spec:
  securityContext:    # 这里 pod 层面
    runAsUser: 1000
  containers:
  - name: sec-ctx-demo-2
    image: busybox
    command: [ "sh","-c","sleep 1h" ]
    securityContext:             # 这里是 container 层面
      runAsUser: 2000
      allowPrivilegeEscalation: false
-----------------------------------------------------------

创建该 Pod:
$ kubectl apply -f security-context-2.yaml

验证 Pod 中的容器处于运行状态:
$ kubectl get pod security-context-demo-2

启动一个 Shell 进入到运行中的容器内:
$ kubectl exec -it security-context-demo-2 -- sh

在你的 Shell 中,列举运行中的进程:
$ ps aux

输出显示进程以用户 2000 运行.该值是在 Container 的 runAsUser 中设置的.该设置值重写了 Pod 层面所设置的值 1000.
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
2000         1  0.0  0.0   4336   764 ?        Ss   20:36   0:00 /bin/sh -c node server.js
2000         8  0.1  0.5 772124 22604 ?        Sl   20:36   0:00 node server.js
...

退出你的 Shell:
$ exit

* 为 Container 设置权能 - 包含查看 Linux 权能的方法,及 capabilities 字段
使用 Linux 权能,你可以赋予进程 root 用户所拥有的某些特权,但不必赋予其全部特权.要为 Container 添加或移除 Linux 权能,可以在 Container 清单的 securityContext 节包含 capabilities 字段.

首先,看一下不包含 capabilities 字段时候会发生什么.下面是一个配置文件,其中没有添加或移除容器的权能:

security-context-3.yaml
--------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-3
spec:
  containers:
  - name: sec-ctx-3
    image: busybox
    command: [ "sh","-c","sleep 1h" ]
-----------------------------------------

创建该 Pod:
$ kubectl apply -f security-context-3.yaml

验证 Pod 的容器处于运行状态:
$ kubectl get pod security-context-demo-3

启动一个 Shell 进入到运行中的容器:
$ kubectl exec -it security-context-demo-3 -- sh

在你的 Shell 中,列举运行中的进程:
$ ps aux

输出显示容器中进程 ID(PIDs):
PID   USER     TIME  COMMAND
    1 root      0:00 sh -c sleep 1h
    8 root      0:00 sh
   15 root      0:00 ps axu

在你的 Shell 中,查看进程 1 的状态:
$ cd /proc/1
$ cat status

输出显示进程的权能位图:
...
CapPrm:	00000000a80425fb
CapEff:	00000000a80425fb
...

记下进程权能位图,之后退出你的 Shell:
$ exit

接下来运行一个与前例中容器相同的容器,只是这个容器有一些额外的权能设置.

下面是一个 Pod 的配置,其中运行一个容器.配置为容器添加 CAP_NET_ADMIN 和 CAP_SYS_TIME 权能:

security-context-4.yaml
------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: security-context-demo-4
spec:
  containers:
  - name: sec-ctx-4
    image: busybox
    command: [ "sh","-c","sleep 1h" ]
    securityContext:
      capabilities:
        add: ["NET_ADMIN","SYS_TIME"]
----------------------------------------------------

创建 Pod:
$ kubectl apply -f security-context-4.yaml

启动一个 Shell,进入到运行中的容器:
$ kubectl exec -it security-context-demo-4 -- sh

在你的 Shell 中,查看进程 1 的权能:
$ cd /proc/1
$ cat status

输出显示的是进程的权能位图:
...
CapPrm:	00000000aa0435fb
CapEff:	00000000aa0435fb
...
比较两个容器的权能位图:

00000000a80425fb
00000000aa0435fb

在第一个容器的权能位图中,位 12 和 25 是没有设置的.在第二个容器中,位 12 和 25 是设置了的.位 12 是 CAP_NET_ADMIN 而位 25 则是 CAP_SYS_TIME.参见 capability.h(https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h)了解权能常数的定义.

说明:
Linux 权能常数定义的形式为 CAP_XXX.但是你在 container 清单中列举权能时,要将权能名称中的 CAP_ 部分去掉.例如,要添加 CAP_SYS_TIME,可在权能列表中添加 SYS_TIME.

* 为容器设置 Seccomp 配置
若要为容器设置 Seccomp 配置(Profile),可在你的 Pod 或 Container 清单的 securityContext 节中包含 seccompProfile 字段.该字段是一个 SeccompProfile 对象,包含 type 和 localhostProfile 属性.type 的合法选项包括 RuntimeDefault、Unconfined 和 Localhost.localhostProfile 只能在 type: Localhost 配置下才可以设置.该字段标明节点上预先设定的配置的路径,路径是相对于 kubelet 所配置的 Seccomp 配置路径(使用 --root-dir 设置)而言的.

* 为 Container 赋予 SELinux 标签
若要给 Container 设置 SELinux 标签,可以在 Pod 或 Container 清单的 securityContext 节包含 seLinuxOptions 字段.seLinuxOptions 字段的取值是一个 SELinuxOptions 对象.

说明:
要指定 SELinux,需要在宿主操作系统中装载 SELinux 安全性模块.

* 管理对 /proc 文件系统的访问
特性状态:  Kubernetes v1.12 [alpha]

对于遵循 OCI 运行时规范的运行时,容器默认运行模式下,存在多个被屏蔽且只读的路径.这样做的结果是在容器的 mount 命名空间内会存在这些路径,并且这些路径的工作方式与容器是隔离主机时类似,但容器进程无法写入它们.被屏蔽的和只读的路径列表如下:

被屏蔽的路径:
/proc/asound
/proc/acpi
/proc/kcore
/proc/keys
/proc/latency_stats
/proc/timer_list
/proc/timer_stats
/proc/sched_debug
/proc/scsi
/sys/firmware

只读的路径:
/proc/bus
/proc/fs
/proc/irq
/proc/sys
/proc/sysrq-trigger

对于某些 Pod,你可能希望绕过默认的路径屏蔽.最常见的情况是你尝试在 Kubernetes 容器内(在 Pod 内)运行容器.

securityContext 字段 procMount 允许用户请求容器的 /proc 为 Unmasked,或者由容器进程以读写方式挂载.这一设置也适用于不在 /proc 内的 /sys/firmware 路径.
...
securityContext:
  procMount: Unmasked

说明:
将 procMount 设置为 Unmasked 需要将 Pod 规约中的 spec.hostUsers 的值设置为 false.换句话说: 希望使用未被屏蔽的 /proc 或 /sys 路径的容器也必须位于 user 命名空间中.Kubernetes v1.12 到 v1.29 没有强制执行该要求.

* 讨论
Pod 的安全上下文适用于 Pod 中的容器,也适用于 Pod 所挂载的卷(如果有的话).尤其是,fsGroup 和 seLinuxOptions 按下面的方式应用到挂载卷上:
.fsGroup: 支持属主管理的卷会被修改,将其属主变更为 fsGroup 所指定的 GID,并且对该 GID 可写.进一步的细节可参阅 属主变更设计文档.

.seLinuxOptions: 支持 SELinux 标签的卷会被重新打标签,以便可被 seLinuxOptions 下所设置的标签访问.通常你只需要设置 level 部分.该部分设置的是赋予 Pod 中所有容器及卷的 多类别安全性(Multi-Category Security,MCS)标签.

警告:
在为 Pod 设置 MCS 标签之后,所有带有相同标签的 Pod 可以访问该卷.如果你需要跨 Pod 的保护,你必须为每个 Pod 赋予独特的 MCS 标签.

* 清理
删除之前创建的所有 Pod:
$ kubectl delete pod security-context-demo
$ kubectl delete pod security-context-demo-2
$ kubectl delete pod security-context-demo-3
$ kubectl delete pod security-context-demo-4

## 为 Pod 配置服务账号
Kubernetes 提供两种完全不同的方式来为客户端提供支持,这些客户端可能运行在你的集群中,也可能与你的集群的控制面相关,需要向 API 服务器完成身份认证.

服务账号(Service Account) 为 Pod 中运行的进程提供身份标识,并映射到 ServiceAccount 对象.当你向 API 服务器执行身份认证时,你会将自己标识为某个用户(User).Kubernetes 能够识别用户的概念,但是 Kubernetes 自身并不提供 User API.

本服务是关于 ServiceAccount 的,而 ServiceAccount 则确实存在于 Kubernetes 的 API 中.本章节为你展示为 Pod 配置 ServiceAccount 的一些方法.

* 使用默认的服务账号访问 API 服务器
当 Pod 与 API 服务器联系时,Pod 会被认证为某个特定的 ServiceAccount(例如: default).在每个名字空间中,至少存在一个 ServiceAccount.

每个 Kubernetes 名字空间至少包含一个 ServiceAccount: 也就是该名字空间的默认服务账号,名为 default.如果你在创建 Pod 时没有指定 ServiceAccount,Kubernetes 会自动将该名字空间中名为 default 的 ServiceAccount 分配给该 Pod.

你可以检视你刚刚创建的 Pod 的细节.例如:
$ kubectl get pods/<podname> -o yaml

在输出中,你可以看到字段 spec.serviceAccountName.当你在创建 Pod 时未设置该字段时,Kubernetes 自动为 Pod 设置这一属性的取值.
Pod 中运行的应用可以使用这一自动挂载的服务账号凭据来访问 Kubernetes API.
当 Pod 被身份认证为某个 ServiceAccount 时,其访问能力取决于所使用的鉴权插件和策略.

** 放弃 API 凭据的自动挂载
如果你不希望 kubelet 自动挂载某 ServiceAccount 的 API 访问凭据,你可以选择不采用这一默认行为.通过在 ServiceAccount 对象上设置 automountServiceAccountToken: false,可以放弃在 /var/run/secrets/kubernetes.io/serviceaccount/token(容器中文件) 处自动挂载该服务账号的 API 凭据.

例如:
--------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
automountServiceAccountToken: false
...
---------------------------------------

你也可以选择不给特定 Pod 自动挂载 API 凭据:
---------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  serviceAccountName: build-robot
  automountServiceAccountToken: false
  ...
----------------------------------------

如果 ServiceAccount 和 Pod 的 .spec 都设置了 automountServiceAccountToken 值,则 Pod 上 spec 的设置优先于服务账号的设置.

* 使用多个服务账号
每个名字空间都至少有一个 ServiceAccount: 名为 default 的默认 ServiceAccount 资源.你可以用下面的命令列举你当前名字空间 中的所有 ServiceAccount 资源:
$ kubectl get serviceaccounts

输出类似于:
NAME      SECRETS    AGE
default   1          1d

你可以像这样来创建额外的 ServiceAccount 对象:
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-robot
EOF

ServiceAccount 对象的名字必须是一个有效的 DNS 子域名.

如果你查询服务账号对象的完整信息,如下所示:
$ kubectl get serviceaccounts/build-robot -o yaml

输出类似于:
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2019-06-16T00:12:34Z
  name: build-robot
  namespace: default
  resourceVersion: "272500"
  uid: 721ab723-13bc-11e5-aec2-42010af0021e

你可以使用鉴权插件来设置服务账号的访问许可.
要使用非默认的服务账号,将 Pod 的 spec.serviceAccountName 字段设置为你想用的服务账号名称.
只能在创建 Pod 时或者为新 Pod 指定模板时,你才可以设置 serviceAccountName.你不能更新已经存在的 Pod 的 .spec.serviceAccountName 字段.

说明:
.spec.serviceAccount 字段是 .spec.serviceAccountName 的已弃用别名.如果要从工作负载资源中删除这些字段,请在 Pod 模板上将这两个字段显式设置为空.

清理
如果你尝试了创建前文示例中所给的 build-robot ServiceAccount,你可以通过运行下面的命令来完成清理操作:
$ kubectl delete serviceaccount/build-robot

* 手动为 ServiceAccount 创建 API 令牌
假设你已经有了一个前文所提到的名为 "build-robot" 的服务账号.你可以使用 kubectl 为该 ServiceAccount 获得一个有时限的 API 令牌:
$ kubectl create token build-robot

这一命令的输出是一个令牌,你可以使用该令牌来将身份认证为对应的 ServiceAccount.你可以使用 kubectl create token 命令的 --duration 参数来请求特定的令牌有效期(实际签发的令牌的有效期可能会稍短一些,也可能会稍长一些).

当启用了 ServiceAccountTokenNodeBinding 和 ServiceAccountTokenNodeBindingValidation(feature-gate中的特性,需要开启,) 特性,并将 KUBECTL_NODE_BOUND_TOKENS 环境变量设置为 true 时,可以创建一个直接绑定到 Node 的服务账号令牌:
$ KUBECTL_NODE_BOUND_TOKENS=true kubectl create token build-robot --bound-object-kind Node --bound-object-name node-001 --bound-object-uid 123...456

此令牌将有效直至其过期或关联的 Node 或服务账户被删除.

说明:
Kubernetes 在 v1.22 版本之前自动创建用来访问 Kubernetes API 的长期凭据.这一较老的机制是基于创建令牌 Secret 对象来实现的,Secret 对象可被挂载到运行中的 Pod 内.在最近的版本中,包括 Kubernetes v1.30,API 凭据可以直接使用 TokenRequest API 来获得,并使用一个投射卷(projected)挂载到 Pod 中.使用此方法获得的令牌具有受限的生命期长度,并且能够在挂载它们的 Pod 被删除时自动被废弃.

你仍然可以通过手动方式来创建服务账号令牌 Secret 对象,例如你需要一个永远不过期的令牌时.不过,使用 TokenRequest 子资源来获得访问 API 的令牌的做法仍然是推荐的方式.

** 手动为 ServiceAccount 创建长期有效的 API 令牌
如果你需要为 ServiceAccount 获得一个 API 令牌,你可以创建一个新的、带有特殊注解 kubernetes.io/service-account.name 的 Secret 对象.

$ kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: build-robot-secret
  annotations:
    kubernetes.io/service-account.name: build-robot
type: kubernetes.io/service-account-token
EOF

如果你通过下面的命令来查看 Secret:
$ kubectl get secret/build-robot-secret -o yaml

你可以看到 Secret 中现在包含针对 "build-robot" ServiceAccount 的 API 令牌.

鉴于你所设置的注解,控制面会自动为该 ServiceAccount 生成一个令牌,并将其保存到相关的 Secret 中.控制面也会为已删除的 ServiceAccount 执行令牌清理操作.
$ kubectl describe secrets/build-robot-secret

输出类似于这样:
Name:           build-robot-secret
Namespace:      default
Labels:         <none>
Annotations:    kubernetes.io/service-account.name: build-robot
                kubernetes.io/service-account.uid: da68f9c6-9d26-11e7-b84e-002dc52800da

Type:   kubernetes.io/service-account-token

Data
====
ca.crt:         1338 bytes
namespace:      7 bytes
token:          ...

说明:
这里将 token 的内容省略了.
注意在你的终端或者计算机屏幕可能被旁观者看到的场合,不要显示 kubernetes.io/service-account-token 的内容.
当你删除一个与某 Secret 相关联的 ServiceAccount 时,Kubernetes 的控制面会自动清理该 Secret 中长期有效的令牌.

说明:
如果你使用以下命令查看 ServiceAccount:
$ kubectl get serviceaccount build-robot -o yaml

在 ServiceAccount API 对象中看不到 build-robot-secret Secret,.secrets 字段,因为该字段只会填充自动生成的 Secret.

* 为服务账号添加 ImagePullSecrets
首先,生成一个 imagePullSecret;  接下来,验证该 Secret 已被创建.例如:
.按为 Pod 设置 imagePullSecret 所描述的,生成一个镜像拉取 Secret:
$ kubectl create secret docker-registry myregistrykey --docker-server=<registry name> \
        --docker-username=DUMMY_USERNAME --docker-password=DUMMY_DOCKER_PASSWORD \
        --docker-email=DUMMY_DOCKER_EMAIL

.检查该 Secret 已经被创建.
$ kubectl get secrets myregistrykey

输出类似于这样:
NAME             TYPE                              DATA    AGE
myregistrykey    kubernetes.io/.dockerconfigjson   1       1d

** 将镜像拉取 Secret 添加到服务账号
接下来更改名字空间的默认服务账号,将该 Secret 用作 imagePullSecret.
$ kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "myregistrykey"}]}'

你也可以通过手动编辑该对象来实现同样的效果:
$ kubectl edit serviceaccount/default

sa.yaml 文件的输出类似于:
你所选择的文本编辑器会被打开,展示如下所示的配置:
--------------------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: 2021-07-07T22:02:39Z
  name: default
  namespace: default
  resourceVersion: "243024"
  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6
--------------------------------------------------

使用你的编辑器,删掉包含 resourceVersion 主键的行,添加包含 imagePullSecrets: 的行并保存文件.对于 uid 而言,保持其取值与你读到的值一样.

当你完成这些变更之后,所编辑的 ServiceAccount 看起来像是这样:
---------------------------------------------
apiVersion: v1
imagePullSecrets:
- name: myregistrykey
kind: ServiceAccount
metadata:
  creationTimestamp: 2021-07-07T22:02:39Z
  name: default
  namespace: default
  resourceVersion: "243024"
  uid: 052fb0f4-3d50-11e5-b066-42010af0d7b6
-----------------------------------------------

** 检查 imagePullSecrets 已经被设置到新 Pod 上
现在,在当前名字空间中创建新 Pod 并使用默认 ServiceAccount 时,新 Pod 的 spec.imagePullSecrets 会被自动设置.
$ kubectl run nginx --image=<registry name>/nginx --restart=Never
$ kubectl get pod nginx -o=jsonpath='{.spec.imagePullSecrets[0].name}{"\n"}'

输出为:
myregistrykey

* 服务账号令牌卷投射
特性状态:  Kubernetes v1.20 [stable]

说明:
为了启用令牌请求投射,你必须为 kube-apiserver 设置以下命令行参数:
--service-account-issuer  - 目前测试环境使用的 --service-account-issuer=https://kubernetes.default.svc.cluster.local
定义服务账号令牌发放者的身份标识(Identifier).你可以多次指定 --service-account-issuer 参数,对于需要变更发放者而又不想带来业务中断的场景,这样做是有用的.如果这个参数被多次指定,其第一个参数值会被用来生成令牌,而所有参数值都会被用来确定哪些发放者是可接受的.你所运行的 Kubernetes 集群必须是 v1.22 或更高版本才能多次指定 --service-account-issuer.

--service-account-key-file  - 目前测试环境使用的 --service-account-key-file=/etc/kubernetes/pki/sa.pub
给出某文件的路径,其中包含 PEM 编码的 x509 RSA 或 ECDSA 私钥或公钥,用来检查 ServiceAccount 的令牌.所指定的文件中可以包含多个秘钥,并且你可以多次使用此参数,每个参数值为不同的文件.多次使用此参数时,由所给的秘钥之一签名的令牌会被 Kubernetes API 服务器认为是合法令牌.

--service-account-signing-key-file  - 目前测试环境使用的 --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
指向某文件的路径,其中包含当前服务账号令牌发放者的私钥.此发放者使用此私钥来签署所发放的 ID 令牌.

--api-audiences(可以省略) - 目前测试环境无
为 ServiceAccount 令牌定义其受众(Audiences).服务账号令牌身份检查组件会检查针对 API 访问所使用的令牌,确认令牌至少是被绑定到这里所给的受众之一.如果 api-audiences 被多次指定,则针对所给的多个受众中任何目标的令牌都会被 Kubernetes API 服务器当做合法的令牌.如果你指定了 --service-account-issuer 参数,但沒有設置 --api-audiences,则控制面认为此参数的默认值为一个只有一个元素的列表,且该元素为令牌发放者的 URL.
-------------------------------------------------

kubelet 还可以将 ServiceAccount 令牌投射到 Pod 中.你可以指定令牌的期望属性,例如受众和有效期限.这些属性在 default ServiceAccount 令牌上无法配置.当 Pod 或 ServiceAccount 被删除时,该令牌也将对 API 无效.

你可以使用类型为 ServiceAccountToken 的投射卷来为 Pod 的 spec 配置此行为.

来自此投射卷的令牌是一个 JSON Web Token(JWT).此令牌的 JSON 载荷遵循明确定义的模式,绑定到 Pod 的令牌的示例载荷如下:

{
  "aud": [  # 匹配请求的受众,或当没有明确请求时匹配 API 服务器的默认受众
    "https://kubernetes.default.svc"
  ],
  "exp": 1731613413,
  "iat": 1700077413,
  "iss": "https://kubernetes.default.svc",# 匹配传递到 --service-account-issuer 标志的第一个值
  "jti": "ea28ed49-2e11-4280-9ec5-bc3d1d84661a",# ServiceAccountTokenJTI 特性必须被启用才能出现此申领
  "kubernetes.io": {
    "namespace": "kube-system",
    "node": {  # ServiceAccountTokenPodNodeInfo 特性必须被启用,API 服务器才会添加此节点引用申领
      "name": "127.0.0.1",
      "uid": "58456cb0-dd00-45ed-b797-5578fdceaced"
    },
    "pod": {
      "name": "coredns-69cbfb9798-jv9gn",
      "uid": "778a530c-b3f4-47c0-9cd5-ab018fb64f33"
    },
    "serviceaccount": {
      "name": "coredns",
      "uid": "a087d5a0-e1dd-43ec-93ac-f13d89cd13af"
    },
    "warnafter": 1700081020
  },
  "nbf": 1700077413,
  "sub": "system:serviceaccount:kube-system:coredns"
}

** 启动使用服务账号令牌投射的 Pod
要为某 Pod 提供一个受众为 vault 并且有效期限为 2 小时的令牌,你可以定义一个与下面类似的 Pod 清单:

pod-projected-svc-token.yaml
-------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    volumeMounts:
    - mountPath: /var/run/secrets/tokens
      name: vault-token
  serviceAccountName: build-robot
  volumes:
  - name: vault-token
    projected:
      sources:
      - serviceAccountToken:
          path: vault-token
          expirationSeconds: 7200
          audience: vault
----------------------------------------------

创建此 Pod:
$ kubectl create -f pod-projected-svc-token.yaml

kubelet 组件会替 Pod 请求令牌并将其保存起来;通过将令牌存储到一个可配置的路径以使之在 Pod 内可用;在令牌快要到期的时候刷新它.kubelet 会在令牌存在期达到其 TTL 的 80% 的时候或者令牌生命期超过 24 小时的时候主动请求将其轮换掉.

应用负责在令牌被轮换时重新加载其内容.通常而言,周期性地(例如,每隔 5 分钟) 重新加载就足够了,不必跟踪令牌的实际过期时间.

* 发现服务账号分发者
特性状态:  Kubernetes v1.21 [stable]

如果你在你的集群中已经为 ServiceAccount 启用了令牌投射,那么你也可以利用其发现能力.Kubernetes 提供一种方式来让客户端将一个或多个外部系统进行联邦,作为标识提供者(Identity Provider),而这些外部系统的角色是依赖方(Relying Party).

说明:
分发者的 URL 必须遵从 OIDC 发现规范.实现上,这意味着 URL 必须使用 https 模式,并且必须在路径 {service-account-issuer}/.well-known/openid-configuration 处给出 OpenID 提供者(Provider)的配置信息.

如果 URL 没有遵从这一规范,ServiceAccount 分发者发现末端末端就不会被注册也无法访问.

## 从私有仓库拉取镜像 - 重要
使用 Secret 从私有的镜像仓库或代码仓库拉取镜像来创建 Pod.

参考: https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/pull-image-private-registry/

## 配置存活、就绪和启动探针 - 重要,包括了 kubelet 通过容器中的命令来判断容器健康的标准
这篇文章介绍如何给容器配置存活(Liveness)、就绪(Readiness)和启动(Startup)探针.

按照时间流动
startup Probe(启动探针) -> readiness Probe(就绪探针) -> liveness Probe(存活探针)
重要:
1. 如果配置了启动探针,存活探针和就绪探针成功之前不会重启
2. 存活探针与就绪性探针相互间不等待对方成功

kubelet 使用存活探针来确定什么时候要重启容器.例如,存活探针可以探测到应用死锁(应用在运行,但是无法继续执行后面的步骤)情况.重启这种状态下的容器有助于提高应用的可用性,即使其中存在缺陷.

存活探针的常见模式是为就绪探针使用相同的低成本 HTTP 端点,但具有更高的 failureThreshold.这样可以确保在硬性终止 Pod 之前,将观察到 Pod 在一段时间内处于非就绪状态.

kubelet 使用就绪探针可以知道容器何时准备好接受请求流量,当一个 Pod 内的所有容器都就绪时,才能认为该 Pod 就绪.这种信号的一个用途就是控制哪个 Pod 作为 Service 的后端.若 Pod 尚未就绪,会被从 Service 的负载均衡器中剔除.

kubelet 使用启动探针来了解应用容器何时启动.如果配置了这类探针,存活探针和就绪探针成功之前不会重启,确保这些探针不会影响应用的启动.启动探针可以用于对慢启动容器进行存活性检测,避免它们在启动运行之前就被杀掉.

注意:
存活探针是一种从应用故障中恢复的强劲方式,但应谨慎使用.你必须仔细配置存活探针,确保它能真正标示出不可恢复的应用故障,例如死锁.

说明:
错误的存活探针可能会导致级联故障.这会导致在高负载下容器重启;例如由于应用无法扩展,导致客户端请求失败;以及由于某些 Pod 失败而导致剩余 Pod 的工作负载增加.了解就绪探针和存活探针之间的区别,以及何时为应用配置使用它们非常重要.

* 定义存活命令
许多长时间运行的应用最终会进入损坏状态,除非重新启动,否则无法被恢复.Kubernetes 提供了存活探针来发现并处理这种情况.

在本练习中,你会创建一个 Pod,其中运行一个基于 registry.k8s.io/busybox 镜像的容器.下面是这个 Pod 的配置文件.

exec-liveness.yaml
----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:          # 这里的命令如果返回 0,表示命令执行成功,kubelet 认为这个容器是正常的
        - cat             # 注意 kubelet 是通过返回值来判断,并且默认返回为 0 是健康的
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
-------------------------------------------------------------

在这个配置文件中,可以看到 Pod 中只有一个 Container.periodSeconds 字段指定了 kubelet 应该每 5 秒执行一次存活探测.initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 5 秒.kubelet 在容器内执行命令 cat /tmp/healthy 来进行探测.如果命令执行成功并且返回值为 0,kubelet 就会认为这个容器是健康存活的.如果这个命令返回非 0 值,kubelet 会杀死这个容器并重新启动它.

当容器启动时,执行如下的命令:
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600"

这个容器生命的前 30 秒,/tmp/healthy 文件是存在的.所以在这最开始的 30 秒内,执行命令 cat /tmp/healthy 会返回成功代码.30 秒之后,执行命令 cat /tmp/healthy 就会返回失败代码.

创建 Pod:
$ kubectl apply -f exec-liveness.yaml

在 30 秒内,查看 Pod 的事件:
$ kubectl describe pod liveness-exec

输出结果表明还没有存活探针失败:
Type    Reason     Age   From               Message
----    ------     ----  ----               -------
Normal  Scheduled  11s   default-scheduler  Successfully assigned default/liveness-exec to node01
Normal  Pulling    9s    kubelet,node01    Pulling image "registry.k8s.io/busybox"
Normal  Pulled     7s    kubelet,node01    Successfully pulled image "registry.k8s.io/busybox"
Normal  Created    7s    kubelet,node01    Created container liveness
Normal  Started    7s    kubelet,node01    Started container liveness

35 秒之后,再来看 Pod 的事件: - 5+30秒
$ kubectl describe pod liveness-exec

在输出结果的最下面,有信息显示存活探针失败了,这个失败的容器被杀死并且被重建了.
Type     Reason     Age                From               Message
----     ------     ----               ----               -------
Normal   Scheduled  57s                default-scheduler  Successfully assigned default/liveness-exec to node01
Normal   Pulling    55s                kubelet,node01    Pulling image "registry.k8s.io/busybox"
Normal   Pulled     53s                kubelet,node01    Successfully pulled image "registry.k8s.io/busybox"
Normal   Created    53s                kubelet,node01    Created container liveness
Normal   Started    53s                kubelet,node01    Started container liveness
Warning  Unhealthy  10s(x3 over 20s)  kubelet,node01    Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
Normal   Killing    10s                kubelet,node01    Container liveness failed liveness probe,will be restarted

再等 30 秒,确认这个容器被重启了:
$ kubectl get pod liveness-exec

输出结果显示 RESTARTS 的值增加了 1.请注意,一旦失败的容器恢复为运行状态,RESTARTS 计数器就会增加 1:
NAME            READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m

https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

* 定义一个存活态 HTTP 请求接口
另外一种类型的存活探测方式是使用 HTTP GET 请求.下面是一个 Pod 的配置文件,其中运行一个基于 registry.k8s.io/e2e-test-images/agnhost 镜像的容器.

http-liveness.yaml
---------------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/e2e-test-images/agnhost:2.40
    args:
    - liveness
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
----------------------------------------------------------------

在这个配置文件中,你可以看到 Pod 也只有一个容器.periodSeconds 字段指定了 kubelet 每隔 3 秒执行一次存活探测.initialDelaySeconds 字段告诉 kubelet 在执行第一次探测前应该等待 3 秒.kubelet 会向容器内运行的服务(服务在监听 8080 端口)发送一个 HTTP GET 请求来执行探测.如果服务器上 /healthz 路径下的处理程序返回成功代码,则 kubelet 认为容器是健康存活的.如果处理程序返回失败代码,则 kubelet 会杀死这个容器并将其重启.

返回大于或等于 200 并且小于 400 的任何代码都标示成功,其它返回代码都标示失败.

你可以访问 server.go 阅读服务的源码.容器存活期间的最开始 10 秒中,/healthz 处理程序返回 200 的状态码.之后处理程序返回 500 的状态码.
-------------------------------------------------------------------------------
http.HandleFunc("/healthz",func(w http.ResponseWriter,r *http.Request) {
    duration := time.Now().Sub(started)
    if duration.Seconds() > 10 {
        w.WriteHeader(500)
        w.Write([]byte(fmt.Sprintf("error: %v",duration.Seconds())))
    } else {
        w.WriteHeader(200)
        w.Write([]byte("ok"))
    }
})
-----------------------------------------------------------------------------

kubelet 在容器启动之后 3 秒开始执行健康检测.所以前几次健康检查都是成功的.但是 10 秒之后,健康检查会失败,并且 kubelet 会杀死容器再重新启动容器.

创建一个 Pod 来测试 HTTP 的存活检测:
$ kubectl apply -f http-liveness.yaml

10 秒之后,通过查看 Pod 事件来确认存活探针已经失败,并且容器被重新启动了.
$ kubectl describe pod liveness-http

在 1.13 之后的版本中,设置本地的 HTTP 代理环境变量不会影响 HTTP 的存活探测.

* 定义 TCP 的存活探测(livenessProbe)
第三种类型的存活探测是使用 TCP 套接字.使用这种配置时,kubelet 会尝试在指定端口和容器建立套接字链接.如果能建立连接,这个容器就被看作是健康的,如果不能则这个容器就被看作是有问题的.

tcp-liveness-readiness.yaml
-----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: registry.k8s.io/goproxy:0.1
    ports:
    - containerPort: 8080
    readinessProbe:   # 就绪探针
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
    livenessProbe:    # 存活探针
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
--------------------------------------------

如你所见,TCP 检测的配置和 HTTP 检测非常相似.下面这个例子同时使用就绪和存活探针.kubelet 会在容器启动 15 秒后发送第一个就绪探针.探针会尝试连接 goproxy 容器的 8080 端口.如果探测成功,这个 Pod 会被标记为就绪状态,kubelet 将继续每隔 10 秒运行一次探测.

除了就绪探针,这个配置包括了一个存活探针.kubelet 会在容器启动 15 秒后进行第一次存活探测.与就绪探针类似,存活探针会尝试连接 goproxy 容器的 8080 端口.如果存活探测失败,容器会被重新启动.
$ kubectl apply -f tcp-liveness-readiness.yaml

15 秒之后,通过看 Pod 事件来检测存活探针:
$ kubectl describe pod goproxy

* 定义 gRPC 存活探针(livenessProbe)  - 此功能用于检查具有 grpc 服务的应用
特性状态:  Kubernetes v1.27 [stable]

如果你的应用实现了 gRPC 健康检查协议,kubelet 可以配置为使用该协议来执行应用存活性检查.你必须启用 GRPCContainerProbe 特性门控 才能配置依赖于 gRPC 的检查机制.

参考:
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/

* 使用命名端口
对于 HTTP 和 TCP 存活检测可以使用命名的 port(gRPC 探针不支持使用命名端口).

例如:
--------------------------------
ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port
--------------------------------

* 使用启动探针(startupProbe)保护慢启动容器
有时候,会有一些现有的应用在启动时需要较长的初始化时间.要这种情况下,若要不影响对死锁作出快速响应的探测,设置存活探测参数是要技巧的.技巧就是使用相同的命令来设置启动探测,针对 HTTP 或 TCP 检测,可以通过将 failureThreshold * periodSeconds 参数设置为足够长的时间来应对最糟糕情况下的启动时间.

这样,前面的例子就变成了:
------------------------------
ports:
- name: liveness-port
  containerPort: 8080

livenessProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 1
  periodSeconds: 10

startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30    # 表示失败次数
  periodSeconds: 10       # 检测之间的间隔
-----------------------------

幸亏有启动探测,应用将会有最多 5 分钟(30 * 10 = 300s)的时间来完成其启动过程.一旦启动探测成功一次,存活探测任务就会接管对容器的探测,对容器死锁作出快速响应.如果启动探测一直没有成功,容器会在 300 秒后被杀死,并且根据 restartPolicy 来执行进一步处置.

这里是一段使用 kubeadm 启动 etcd pod 的官方配置
-----------------------------------------------------------
startupProbe:                     # 启动探针
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 15

livenessProbe:                    # 存活探针
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 15
-------------------------------------------------------------

* 定义就绪探针(readinessProbe)
有时候,应用会暂时性地无法为请求提供服务.例如,应用在启动时可能需要加载大量的数据或配置文件,或是启动后要依赖等待外部服务.在这种情况下,既不想杀死应用,也不想给它发送请求.Kubernetes 提供了就绪探针来发现并缓解这些情况.容器所在 Pod 上报还未就绪的信息,并且不接受通过 Kubernetes Service 的流量.

说明:
就绪探针在容器的整个生命周期中保持运行状态.

注意:
存活探针与就绪性探针相互间不等待对方成功.如果要在执行就绪性探针之前等待,应该使用 initialDelaySeconds 或 startupProbe.

就绪探针的配置和存活探针的配置相似.唯一区别就是要使用 readinessProbe 字段,而不是 livenessProbe 字段.
-----------------------------
readinessProbe:
  exec:
    command:
    - cat
    - /tmp/healthy
  initialDelaySeconds: 5
  periodSeconds: 5
-------------------------------

HTTP 和 TCP 的就绪探针配置也和存活探针的配置完全相同.

就绪和存活探测可以在同一个容器上并行使用.两者共同使用,可以确保流量不会发给还未就绪的容器,当这些探测失败时容器会被重新启动.

* 配置探针
Probe 有很多配置字段,可以使用这些字段精确地控制启动、存活和就绪检测的行为:
. initialDelaySeconds: 容器启动后要等待多少秒后才启动启动、存活和就绪探针.如果定义了启动探针,则存活探针和就绪探针的延迟将在启动探针已成功之后才开始计算.如果 periodSeconds 的值大于 initialDelaySeconds,则 initialDelaySeconds 将被忽略(意思是说,如果间隔时间 > 初始化的时间,初始化时间将被忽略).默认是 0 秒,最小值是 0.

. periodSeconds: 执行探测的时间间隔(单位是秒).默认是 10 秒.最小值是 1.

. timeoutSeconds: 探测的超时后等待多少秒.默认值是 1 秒.最小值是 1.

. successThreshold: 探针在失败后,被视为成功的最小连续成功数.默认值是 1.存活和启动探测的这个值必须是 1.最小值是 1.

. failureThreshold: 探针连续失败了 failureThreshold 次之后,Kubernetes 认为总体上检查已失败: 容器状态未就绪、不健康、不活跃. 默认值为 3,最小值为 1.对于启动探针或存活探针而言,如果至少有 failureThreshold 个探针已失败,Kubernetes 会将容器视为不健康并为这个特定的容器触发重启操作.kubelet 遵循该容器的 terminationGracePeriodSeconds 设置.对于失败的就绪探针,kubelet 继续运行检查失败的容器,并继续运行更多探针;因为检查失败,kubelet 将 Pod 的 Ready 状况设置为 false.

. terminationGracePeriodSeconds: 为 kubelet 配置从为失败的容器触发终止操作到强制容器运行时停止该容器之前等待的宽限时长.默认值是继承 Pod 级别的 terminationGracePeriodSeconds 值(如果不设置则为 30 秒),最小值为 1.
注意:
如果就绪态探针的实现不正确,可能会导致容器中进程的数量不断上升.如果不对其采取措施,很可能导致资源枯竭的状况.

** HTTP 探测
HTTP Probes 允许针对 httpGet 配置额外的字段:
. host: 连接使用的主机名,默认是 Pod 的 IP.也可以在 HTTP 头中设置 "Host" 来代替.
. scheme: 用于设置连接主机的方式(HTTP 还是 HTTPS).默认是 "HTTP".
. path: 访问 HTTP 服务的路径.默认值为 "/".
. httpHeaders: 请求中自定义的 HTTP 头.HTTP 头字段允许重复.
. port: 访问容器的端口号或者端口名.如果数字必须在 1～65535 之间.

对于 HTTP 探测,kubelet 发送一个 HTTP 请求到指定的端口和路径来执行检测 除非 httpGet 中的 host 字段设置了,否则 kubelet 默认是给 Pod 的 IP 地址发送探测.如果 scheme 字段设置为了 HTTPS,kubelet 会跳过证书验证发送 HTTPS 请求.大多数情况下,不需要设置 host 字段.这里有个需要设置 host 字段的场景,假设容器监听 127.0.0.1,并且 Pod 的 hostNetwork 字段设置为了 true.那么 httpGet 中的 host 字段应该设置为 127.0.0.1.可能更常见的情况是如果 Pod 依赖虚拟主机,你不应该设置 host 字段,而是应该在 httpHeaders 中设置 Host.

针对 HTTP 探针,kubelet 除了必需的 Host 头部之外还发送两个请求头部字段:
. User-Agent: 默认值是 kube-probe/1.30,其中 1.30 是 kubelet 的版本号.
. Accept: 默认值 */*.

你可以通过为探测设置 httpHeaders 来重载默认的头部字段值.例如:
----------------------------------------------
livenessProbe:
  httpGet:
    httpHeaders:
      - name: Accept
        value: application/json

startupProbe:
  httpGet:
    httpHeaders:
      - name: User-Agent
        value: MyUserAgent
------------------------------------------------

你也可以通过将这些头部字段定义为空值,从请求中去掉这些头部字段.
--------------------------------
livenessProbe:
  httpGet:
    httpHeaders:
      - name: Accept
        value: ""

startupProbe:
  httpGet:
    httpHeaders:
      - name: User-Agent
        value: ""
---------------------------------

说明:
当 kubelet 使用 HTTP 探测 Pod 时,仅当重定向到同一主机时,它才会遵循重定向.如果 kubelet 在探测期间收到 11 个或更多重定向,则认为探测成功并创建相关事件:
Events:
  Type     Reason        Age                     From               Message
  ----     ------        ----                    ----               -------
  Normal   Scheduled     29m                     default-scheduler  Successfully assigned default/httpbin-7b8bc9cb85-bjzwn to daocloud
  Normal   Pulling       29m                     kubelet            Pulling image "docker.io/kennethreitz/httpbin"
  Normal   Pulled        24m                     kubelet            Successfully pulled image "docker.io/kennethreitz/httpbin" in 5m12.402735213s
  Normal   Created       24m                     kubelet            Created container httpbin
  Normal   Started       24m                     kubelet            Started container httpbin
  Warning  ProbeWarning  4m11s(x1197 over 24m)  kubelet            Readiness probe warning: Probe terminated redirects

如果 kubelet 收到主机名与请求不同的重定向,则探测结果将被视为成功,并且 kubelet 将创建一个事件来报告重定向失败.

** TCP 探测
对于 TCP 探测而言,kubelet 在节点上(不是在 Pod 里面)发起探测连接,这意味着你不能在 host 参数上配置服务名称,因为 kubelet 不能解析服务名称.

** 探针层面的 terminationGracePeriodSeconds
特性状态:  Kubernetes v1.28 [stable]

在 1.25 及以上版本中,用户可以指定一个探针层面的 terminationGracePeriodSeconds 作为探针规约的一部分.当 Pod 层面和探针层面的 terminationGracePeriodSeconds 都已设置,kubelet 将使用探针层面设置的值.

当设置 terminationGracePeriodSeconds 时,请注意以下事项:
. kubelet 始终优先选用探针级别 terminationGracePeriodSeconds 字段(如果它存在于 Pod 上).
. 如果你已经为现有 Pod 设置了 terminationGracePeriodSeconds 字段并且不再希望使用针对每个探针的终止宽限期,则必须删除现有的这类 Pod.

例如:
---------------------------------------------------
spec:
  terminationGracePeriodSeconds: 3600  # Pod 级别设置
  containers:
  - name: test
    image: ...

    ports:
    - name: liveness-port
      containerPort: 8080

    livenessProbe:
      httpGet:
        path: /healthz
        port: liveness-port
      failureThreshold: 1
      periodSeconds: 60
      # 重载 Pod 级别的 terminationGracePeriodSeconds
      terminationGracePeriodSeconds: 60
------------------------------------------------------------

探针层面的 terminationGracePeriodSeconds 不能用于就绪态探针.这一设置将被 API 服务器拒绝.

## 将 Pod 分配给节点 - 重要
将 Kubernetes Pod 指派给 Kubernetes 集群中的特定节点

* 给节点添加标签
列出你的集群中的节点,包括这些节点上的标签:
$ kubectl get nodes --show-labels

输出类似如下:
NAME    STATUS   ROLES           AGE   VERSION   LABELS
k8s01   Ready    control-plane   10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s02   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s02,kubernetes.io/os=linux
k8s03   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s03,kubernetes.io/os=linux

从你的节点中选择一个,为它添加标签:
$ kubectl label nodes <your-node-name> disktype=ssd

<your-node-name> 是你选择的节点的名称.

验证你选择的节点确实带有 disktype=ssd 标签:
$ kubectl get nodes --show-labels

输出类似如下:
NAME    STATUS   ROLES           AGE   VERSION   LABELS
k8s01   Ready    control-plane   10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s02   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s02,kubernetes.io/os=linux
k8s03   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s03,kubernetes.io/os=linux

在前面的输出中,你可以看到  k8s02 节点有 disktype=ssd 标签.

* 创建一个将被调度到你选择的节点的 Pod
此 Pod 配置文件描述了一个拥有节点选择器 disktype: ssd 的 Pod.这表明该 Pod 将被调度到有 disktype=ssd 标签的节点.

pod-nginx.yaml
------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  nodeSelector:
    disktype: ssd
------------------------------------

1. 使用该配置文件创建一个 Pod,该 Pod 将被调度到你选择的节点上:
$ kubectl create -f pod-nginx.yaml

2. 验证 Pod 确实运行在你选择的节点上:
$ kubectl get pods --output=wide

输出类似如下:
NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s

* 创建一个会被调度到特定节点上的 Pod
你也可以通过设置 nodeName 将某个 Pod 调度到特定的节点.

pod-nginx-specific-node.yaml
-------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: k8s03 # 调度 Pod 到特定的节点
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
--------------------------------------------
使用此配置文件来创建一个 Pod,该 Pod 将只能被调度到 k8s03 节点.

## 用节点亲和性把 Pod 分配到节点
使用节点亲和性把 Kubernetes Pod 分配到特定节点.

在进行下面的步骤之前,先阅读如下部分
^^^^^^^^^^^^^^^^^^^^
### 将 Pod 指派给节点 - 详细的节点选择方式,标签,亲和性,nodeName字段,Pod 拓扑分布约束 等等
参考: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/
约束一个 Pod 以便限制其只能在特定的节点上运行,或优先在特定的节点上运行.有几种方法可以实现这点,推荐的方法都是用 标签选择算符来进行选择.通常这样的约束不是必须的,因为调度器将自动进行合理的放置(比如,将 Pod 分散到节点上,而不是将 Pod 放置在可用资源不足的节点上等等).但在某些情况下,你可能需要进一步控制 Pod 被部署到哪个节点.例如,确保 Pod 最终落在连接了 SSD 的机器上,或者将来自两个不同的服务且有大量通信的 Pod 被放置在同一个可用区.

可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度:
. 与节点标签匹配的 nodeSelector
. 亲和性与反亲和性
. nodeName 字段
. Pod 拓扑分布约束

* 节点标签
与很多其他 Kubernetes 对象类似,节点也有标签.你可以手动地添加标签.Kubernetes 也会为集群中所有节点添加一些标准的标签.

说明:
这些标签的取值是取决于云提供商的,并且是无法在可靠性上给出承诺的.例如,kubernetes.io/hostname 的取值在某些环境中可能与节点名称相同,而在其他环境中会取不同的值.

* 节点隔离/限制
通过为节点添加标签,你可以准备让 Pod 调度到特定节点或节点组上.你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性、安全性或监管属性的节点上.

如果使用标签来实现节点隔离,建议选择节点上的 kubelet 无法修改的标签键.这可以防止受感染的节点在自身上设置这些标签,进而影响调度器将工作负载调度到受感染的节点.

NodeRestriction 准入插件防止 kubelet 使用 node-restriction.kubernetes.io/ 前缀设置或修改标签.(即此前缀的标签不会被修改)

要使用该标签前缀进行节点隔离:
1. 确保你在使用节点鉴权机制并且已经启用了 NodeRestriction 准入插件.

说明:
. 检查 kube-apiserver 启动参数中是否有 --enable-admission-plugins=NodeRestriction
. 要查看哪些插件是被启用的:
kube-apiserver -h | grep enable-admission-plugins
. 在 Kubernetes 1.30 中,默认启用的插件有:
CertificateApproval,CertificateSigning,CertificateSubjectRestriction,DefaultIngressClass,DefaultStorageClass,DefaultTolerationSeconds,LimitRanger,MutatingAdmissionWebhook,NamespaceLifecycle,PersistentVolumeClaimResize,PodSecurity,Priority,ResourceQuota,RuntimeClass,ServiceAccount,StorageObjectInUseProtection,TaintNodesByCondition,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook

2. 将带有 node-restriction.kubernetes.io/ 前缀的标签添加到 Node 对象,然后在节点选择算符中使用这些标签.例如,example.com.node-restriction.kubernetes.io/fips=true 或 example.com.node-restriction.kubernetes.io/pci-dss=true.

* nodeSelector
nodeSelector 是节点选择约束的最简单推荐形式.你可以将 nodeSelector 字段添加到 Pod 的规约中设置你希望目标节点所具有的节点标签.Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上.

* 亲和性与反亲和性
nodeSelector 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上.亲和性和反亲和性扩展了你可以定义的约束类型.使用亲和性与反亲和性的一些好处有:
. 亲和性、反亲和性语言的表达能力更强.nodeSelector 只能选择拥有所有指定标签的节点.亲和性、反亲和性为你提供对选择逻辑的更强控制能力.
. 你可以标明某规则是“软需求”或者“偏好”,这样调度器在无法找到匹配节点时仍然调度该 Pod.
. 你可以使用节点上(或其他拓扑域中)运行的其他 Pod 的标签来实施调度约束,而不是只能使用节点本身的标签.这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起.

亲和性功能由两种类型的亲和性组成:
. 节点亲和性功能类似于 nodeSelector 字段,但它的表达能力更强,并且允许你指定软规则.
. Pod 间亲和性/反亲和性允许你根据其他 Pod 的标签来约束 Pod.

** 节点亲和性
节点亲和性概念上类似于 nodeSelector,它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上.节点亲和性有两种:
. requiredDuringSchedulingIgnoredDuringExecution:  调度器只有在规则被满足的时候才能执行调度.此功能类似于 nodeSelector,但其语法表达能力更强.
. preferredDuringSchedulingIgnoredDuringExecution:  调度器会尝试寻找满足对应规则的节点.如果找不到匹配的节点,调度器仍然会调度该 Pod.

说明:
在上述类型中,IgnoredDuringExecution 意味着如果节点标签在 Kubernetes 调度 Pod 后发生了变更,Pod 仍将继续运行.

你可以使用 Pod 规约中的 .spec.affinity.nodeAffinity 字段来设置节点亲和性.例如,考虑下面的 Pod 规约:

pod-with-node-affinity.yaml
---------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:   # 这里是必须的
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:    # 注意这里
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1    # 这里的 2 个条件,满足 1 个即成立
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0
-------------------------------------------------

在这一示例中,所应用的规则如下:
. 节点必须包含一个键名为 topology.kubernetes.io/zone 的标签,并且该标签的取值必须为 antarctica-east1 或 antarctica-west1.
. 节点最好具有一个键名为 another-node-label-key 且取值为 another-node-label-value 的标签.

你可以使用 operator 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符.你可以使用 In、NotIn、Exists、DoesNotExist、Gt 和 Lt 之一作为操作符.

操作符的具体信息见下面

NotIn 和 DoesNotExist 可用来实现节点反亲和性行为.你也可以使用节点污点 将 Pod 从特定节点上驱逐.

说明:
如果你同时指定了 nodeSelector 和 nodeAffinity,两者必须都要满足,才能将 Pod 调度到候选节点上.

如果你在与 nodeAffinity 类型关联的 nodeSelectorTerms 中指定多个条件,只要其中一个 nodeSelectorTerms 满足(各个条件按逻辑或操作组合)的话,Pod 就可以被调度到节点上.

如果你在与 nodeSelectorTerms 中的条件相关联的单个 matchExpressions 字段中指定多个表达式,则只有当所有表达式都满足(各表达式按逻辑与操作组合)时,Pod 才能被调度到节点上. - ? 这里无法理解

*** 节点亲和性权重
你可以为 preferredDuringSchedulingIgnoredDuringExecution 亲和性类型的每个实例设置 weight 字段,其取值范围是 1 到 100.当调度器找到能够满足 Pod 的其他调度请求的节点时,调度器会遍历节点满足的所有的偏好性规则,并将对应表达式的 weight 值加和.

最终的加和值会添加到该节点的其他优先级函数的评分之上.在调度器为 Pod 作出调度决定时,总分最高的节点的优先级也最高.

例如,考虑下面的 Pod 规约:
pod-with-affinity-preferred-weight.yaml
-----------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: with-affinity-preferred-weight
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: # 这里说明,是必须满足的条件
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: label-1
            operator: In
            values:
            - key-1
      - weight: 50
        preference:
          matchExpressions:
          - key: label-2
            operator: In
            values:
            - key-2
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0
---------------------------------------------------------

如果存在两个候选节点,都满足 preferredDuringSchedulingIgnoredDuringExecution 规则,其中一个节点具有标签 label-1:key-1,另一个节点具有标签 label-2:key-2,调度器会考察各个节点的 weight 取值,并将该权重值添加到节点的其他得分值之上,

说明:
如果你希望 Kubernetes 能够成功地调度此例中的 Pod,你必须拥有打了 kubernetes.io/os=linux 标签的节点.

*** 逐个调度方案中设置节点亲和性
特性状态:  Kubernetes v1.20 [beta]

在配置多个调度方案时,你可以将某个方案与节点亲和性关联起来,如果某个调度方案仅适用于某组特殊的节点时,这样做是很有用的.要实现这点,可以在调度器配置中为 NodeAffinity 插件的 args 字段添加 addedAffinity.例如:
--------------------------------------------------------
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
  - schedulerName: foo-scheduler
    pluginConfig:
      - name: NodeAffinity
        args:
          addedAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: scheduler-profile
                  operator: In
                  values:
                  - foo
---------------------------------------------------------------

这里的 addedAffinity 除遵从 Pod 规约中设置的节点亲和性之外,还适用于将 .spec.schedulerName 设置为 foo-scheduler.换言之,为了匹配 Pod,节点需要满足 addedAffinity 和 Pod 的 .spec.NodeAffinity.

由于 addedAffinity 对最终用户不可见,其行为可能对用户而言是出乎意料的.应该使用与调度方案名称有明确关联的节点标签.

说明:
DaemonSet 控制器为 DaemonSet 创建 Pod,但该控制器不理会调度方案.DaemonSet 控制器创建 Pod 时,默认的 Kubernetes 调度器负责放置 Pod,并遵从 DaemonSet 控制器中设置的 nodeAffinity 规则.

** Pod 间亲和性与反亲和性 - 关于标签选择算符,默认的节点标签,注解,污点 - 重要
Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点,而不是基于节点上的标签.

Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod,则这个 Pod 应该(或者在反亲和性的情况下不应该)运行在 X 上” 这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域,Y 则是 Kubernetes 尝试满足的规则.

你通过标签选择算符(https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors - 重要)的形式来表达规则(Y),并可根据需要指定选关联的名字空间列表 Pod 在 Kubernetes 中是名字空间作用域的对象,因此 Pod 的标签也隐式地具有名字空间属性.针对 Pod 标签的所有标签选择算符都要指定名字空间,Kubernetes 会在指定的名字空间内寻找标签.

你会通过 topologyKey(在 yaml 文件中设置) 来表达拓扑域(X)的概念,其取值是系统用来标示域的节点标签键.相关示例可参见常用标签、注解和污点.(https://kubernetes.io/zh-cn/docs/reference/labels-annotations-taints/#node-role-kubernetes-io-control-plane - 重要)

说明:
Pod 间亲和性和反亲和性都需要相当的计算量,因此会在大规模集群中显著降低调度速度.我们不建议在包含数百个节点的集群中使用这类设置.

说明:
Pod 反亲和性需要节点上存在一致性的标签.换言之,集群中每个节点都必须拥有与 topologyKey 匹配的标签.如果某些或者所有节点上不存在所指定的 topologyKey 标签,调度行为可能与预期的不同.

*** Pod 间亲和性与反亲和性的类型
与节点亲和性类似,Pod 的亲和性与反亲和性也有两种类型:
. requiredDuringSchedulingIgnoredDuringExecution
. preferredDuringSchedulingIgnoredDuringExecution

例如,你可以使用 requiredDuringSchedulingIgnoredDuringExecution 亲和性来告诉调度器,将两个服务的 Pod 放到同一个云提供商可用区内,因为它们彼此之间通信非常频繁.类似地,你可以使用 preferredDuringSchedulingIgnoredDuringExecution 反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中.

要使用 Pod 间亲和性,可以使用 Pod 规约中的 .affinity.podAffinity 字段.对于 Pod 间反亲和性,可以使用 Pod 规约中的 .affinity.podAntiAffinity 字段.

*** 调度一组具有 Pod 间亲和性的 Pod
如果当前正被调度的 Pod 在具有自我亲和性的 Pod 序列中排在第一个,那么只要它满足其他所有的亲和性规则,它就可以被成功调度.这是通过以下方式确定的: 确保集群中没有其他 Pod 与此 Pod 的名字空间和标签选择算符匹配;  该 Pod 满足其自身定义的条件,并且选定的节点满足所指定的所有拓扑要求.这确保即使所有的 Pod 都配置了 Pod 间亲和性,也不会出现调度死锁的情况.

*** Pod 亲和性示例
在进行示例之前,需要先学习关于区域的内容
^^^^^^^^^^^^^^^^^^^^
参考: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/

### Pod 拓扑分布约束 - 关于PodTopologySpread 简介(https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/)
你可以使用 拓扑分布约束(Topology Spread Constraints) 来控制 Pod 在集群内故障域之间的分布,例如区域(Region)、可用区(Zone)、节点和其他用户自定义拓扑域.这样做有助于实现高可用并提升资源利用率.

你可以将集群级约束设为默认值,或为个别工作负载配置拓扑分布约束.

* 动机
假设你有一个最多包含二十个节点的集群,你想要运行一个自动扩缩的 工作负载,请问要使用多少个副本？ 答案可能是最少 2 个 Pod,最多 15 个 Pod. 当只有 2 个 Pod 时,你倾向于这 2 个 Pod 不要同时在同一个节点上运行:  你所遭遇的风险是如果放在同一个节点上且单节点出现故障,可能会让你的工作负载下线.

除了这个基本的用法之外,还有一些高级的使用案例,能够让你的工作负载受益于高可用性并提高集群利用率.

随着你的工作负载扩容,运行的 Pod 变多,将需要考虑另一个重要问题.假设你有 3 个节点,每个节点运行 5 个 Pod.这些节点有足够的容量能够运行许多副本;  但与这个工作负载互动的客户端分散在三个不同的数据中心(或基础设施可用区) 现在你可能不太关注单节点故障问题,但你会注意到延迟高于自己的预期,在不同的可用区之间发送网络流量会产生一些网络成本.

你决定在正常运营时倾向于将类似数量的副本调度 到每个基础设施可用区,且你想要该集群在遇到问题时能够自愈.

Pod 拓扑分布约束使你能够以声明的方式进行配置.

* topologySpreadConstraints 字段
Pod API 包括一个 spec.topologySpreadConstraints 字段.这个字段的用法如下所示:
----------------------------------------------------------
---
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  # 配置一个拓扑分布约束
  topologySpreadConstraints:
    - maxSkew: <integer>                  # 必须 > 0,跟随 whenUnsatisfiable 语义,定义开始
      minDomains: <integer>               # 可选,必须 > 0,如果未指定,类似于 = 1
      topologyKey: <string>               # 关键,节点标签的 key
      whenUnsatisfiable: <string>         # DoNotSchedule(默认)|ScheduleAnyway
      labelSelector: <object>             # 关键,用于查找匹配的 Pod
      matchLabelKeys: <list>              # 可选; 自从 v1.27 开始成为 Beta.Pod 标签键的列表,用于选择需要计算分布方式的 Pod                                            # 集合当有多个需要匹配的 key 组成的列表
      nodeAffinityPolicy: [Honor|Ignore]  # 可选; 自从 v1.26 开始成为 Beta.表示我们在计算 Pod 拓扑分布偏差时将如何处理 Pod
                                          # 的 nodeAffinity/nodeSelector
      nodeTaintsPolicy: [Honor|Ignore]    # 可选; 自从 v1.26 开始成为 Beta.表示我们在计算 Pod 拓扑分布偏差时将如何处理节点污点.
  ### 其他 Pod 字段置于此处
----------------------------------------------------------------------------

你可以运行 kubectl explain Pod.spec.topologySpreadConstraints 或参阅 Pod API 参考的调度(https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#%E8%B0%83%E5%BA%A6)一节,了解有关此字段的更多信息.

** 分布约束定义
你可以定义一个或多个 topologySpreadConstraints 条目以指导 kube-scheduler 如何将每个新来的 Pod 与跨集群的现有 Pod 相关联.这些字段包括:
. maxSkew 描述这些 Pod 可能被不均匀分布的程度.你必须指定此字段且该数值必须大于零.其语义将随着 whenUnsatisfiable 的值发生变化:
  . 如果你选择 whenUnsatisfiable: DoNotSchedule,则 maxSkew 定义目标拓扑中匹配 Pod 的数量与全局最小值(符合条件的域中匹配的最小 Pod 数量,如果符合条件的域数量小于 MinDomains 则为零) 之间的最大允许差值.例如,如果你有 3 个可用区,分别有 2、2 和 1 个匹配的 Pod,则 MaxSkew 设为 1,且全局最小值为 1. - 当存在多个域满足的时候,必须满足,最大的不均匀程度不能操作 maxSkew 的值,比如: 当 maxSkew 为 1,则
如果有 3 个域,分布的 pod 为 [0,2,1],当有新的 pod 加入,则,可能是
[1,2,1] - 不均匀值为 1(2 - 1),0(1 - 1),这里的最大的不均衡值为 1,等于 maxSkew,符合需求
[0,3,1] - 不均匀值为 3(3 - 0),1(1 - 0),2(3 - 1),这里的最大的不均衡值为 3,大于 maxSkew,不符合需求
[0,2,2] - 不均匀值为 2(2 - 0),0(2 - 2),这里的最大不均衡值为 2,大于 maxSkew,不符合需求
这三种情况,
  . 如果你选择 whenUnsatisfiable: ScheduleAnyway,则该调度器会更为偏向能够降低偏差值的拓扑域.

. minDomains 表示符合条件的域的最小数量.此字段是可选的.域是拓扑的一个特定实例.符合条件的域是其节点与节点选择器匹配的域.
说明:
在 Kubernetes v1.30 之前,minDomains 字段只有在启用 MinDomainsInPodTopologySpread 特性门控时才可用(自 v1.28 起默认启用) 在早期的 Kubernetes 集群中,此特性门控可能被显式禁用或此字段可能不可用.

  . 指定的 minDomains 值必须大于 0.你可以结合 whenUnsatisfiable: DoNotSchedule 仅指定 minDomains.
  . 当符合条件的、拓扑键匹配的域的数量小于 minDomains 时,拓扑分布将“全局最小值”(global minimum)设为 0,然后进行 skew 计算.“全局最小值”是一个符合条件的域中匹配 Pod 的最小数量,如果符合条件的域的数量小于 minDomains,则全局最小值为零.
  . 当符合条件的拓扑键匹配域的个数等于或大于 minDomains 时,该值对调度没有影响.
  . 如果你未指定 minDomains,则约束行为类似于 minDomains 等于 1.

. topologyKey 是节点标签的键.如果节点使用此键标记并且具有相同的标签值,则将这些节点视为处于同一拓扑域中.我们将拓扑域中(即键值对)的每个实例称为一个域.调度器将尝试在每个拓扑域中放置数量均衡的 Pod.另外,我们将符合条件的域定义为其节点满足 nodeAffinityPolicy 和 nodeTaintsPolicy 要求的域.

. whenUnsatisfiable 指示如果 Pod 不满足分布约束时如何处理:
  . DoNotSchedule(默认)告诉调度器不要调度.
  . ScheduleAnyway 告诉调度器仍然继续调度,只是根据如何能将偏差最小化来对节点进行排序.

. labelSelector 用于查找匹配的 Pod.匹配此标签的 Pod 将被统计,以确定相应拓扑域中 Pod 的数量.

. matchLabelKeys 是一个 Pod 标签键的列表,用于选择需要计算分布方式的 Pod 集合.这些键用于从 Pod 标签中查找值,这些键值标签与 labelSelector 进行逻辑与运算,以选择一组已有的 Pod,通过这些 Pod 计算新来 Pod 的分布方式.matchLabelKeys 和 labelSelector 中禁止存在相同的键.未设置 labelSelector 时无法设置 matchLabelKeys.Pod 标签中不存在的键将被忽略.null 或空列表意味着仅与 labelSelector 匹配.

借助 matchLabelKeys,你无需在变更 Pod 修订版本时更新 pod.spec.控制器或 Operator 只需要将不同修订版的标签键设为不同的值.调度器将根据 matchLabelKeys 自动确定取值.例如,如果你正在配置一个 Deployment,则你可以使用由 Deployment 控制器自动添加的、以 pod-template-hash 为键的标签来区分同一个 Deployment 的不同修订版.

    topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: foo
          matchLabelKeys:
            - pod-template-hash
说明:
matchLabelKeys 字段是 1.27 中默认启用的一个 Beta 级别字段.你可以通过禁用 MatchLabelKeysInPodTopologySpread 特性门控来禁用此字段.

. nodeAffinityPolicy 表示我们在计算 Pod 拓扑分布偏差时将如何处理 Pod 的 nodeAffinity/nodeSelector.选项为:
  . Honor: 只有与 nodeAffinity/nodeSelector 匹配的节点才会包括到计算中.
  . Ignore: nodeAffinity/nodeSelector 被忽略.所有节点均包括到计算中.
如果此值为 nil,此行为等同于 Honor 策略.

说明:
nodeAffinityPolicy 是 1.26 中默认启用的一个 Beta 级别字段.你可以通过禁用 NodeInclusionPolicyInPodTopologySpread 特性门控来禁用此字段.

. nodeTaintsPolicy 表示我们在计算 Pod 拓扑分布偏差时将如何处理节点污点.选项为:
  . Honor: 包括不带污点的节点以及污点被新 Pod 所容忍的节点.
  . Ignore: 节点污点被忽略.包括所有节点.
如果此值为 null,此行为等同于 Ignore 策略.

说明:
nodeTaintsPolicy 是一个 Beta 级别字段,在 1.26 版本默认启用.你可以通过禁用 NodeInclusionPolicyInPodTopologySpread 特性门控来禁用此字段.

当 Pod 定义了不止一个 topologySpreadConstraint,这些约束之间是逻辑与的关系.kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点.

** 节点标签
拓扑分布约束依赖于节点标签来标识每个节点所在的拓扑域.例如,某节点可能具有标签:
  region: us-east-1
  zone: us-east-1a

说明:
为了简便,此示例未使用众所周知的标签键 topology.kubernetes.io/zone 和 topology.kubernetes.io/region.但是,建议使用那些已注册的标签键,而不是此处使用的私有(不合格)标签键 region 和 zone.
你无法对不同上下文之间的私有标签键的含义做出可靠的假设.

假设你有一个 4 节点的集群且带有以下标签:
NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    <none>   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    <none>   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    <none>   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    <none>   2m43s   v1.16.0   node=node4,zone=zoneB

那么,从逻辑上看集群如下:
zoneA   zoneB
Node1   Node3
Node2   Node4

* 一致性 - 重要
你应该为一个组中的所有 Pod 设置相同的 Pod 拓扑分布约束.
通常,如果你正使用一个工作负载控制器,例如 Deployment,则 Pod 模板会帮你解决这个问题.如果你混合不同的分布约束,则 Kubernetes 会遵循该字段的 API 定义; 但是,该行为可能更令人困惑,并且故障排除也没那么简单.

你需要一种机制来确保拓扑域(例如云提供商区域)中的所有节点具有一致的标签.为了避免你需要手动为节点打标签,大多数集群会自动填充知名的标签,例如 kubernetes.io/hostname.检查你的集群是否支持此功能.

* 拓扑分布约束示例
** 示例: 一个拓扑分布约束
根据官方的例子修改

# 添加标签,尽量使用官方提供的默认标签或者给定的标签,
# 参考: https://kubernetes.io/zh-cn/docs/reference/labels-annotations-taints/#node-role-kubernetes-io-control-plane
$ kubectl label nodes k8s02 topology.kubernetes.io/zone=zoneB
$ kubectl label nodes k8s03 topology.kubernetes.io/zone=zoneC
$ kubectl label nodes k8s01 topology.kubernetes.io/zone=zoneA

假设你拥有一个 3 节点集群,其中标记为 foo: bar 的 3 个 Pod 分别位于 node1(master)、node2(worker) 和 node3(worker) 中:
topology.kubernetes.io/zone=zoneA   topology.kubernetes.io/zone=zoneB   topology.kubernetes.io/zone=zoneC
    node1                               node2                               node3
                                        pod                                 pod
                                        pod

如果你希望新来的 Pod 均匀分布在现有的可用区域,则可以按如下设置其清单:

one-constraint.yaml
-----------------------------------
kind: Pod
apiVersion: v1
metadata:
  name: pod4-test-zone
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: Never
-----------------------------------------------

从此清单看,topologyKey: zone 意味着均匀分布将只应用于存在标签键值对为 zone: <any value> 的节点(没有 zone 标签的节点将被跳过).如果调度器找不到一种方式来满足此约束,则 whenUnsatisfiable: DoNotSchedule 字段告诉该调度器将新来的 Pod 保持在 pending 状态.

如果该调度器将这个新来的 Pod 放到可用区 A,则 Pod 的分布将成为 [1,2,1].这意味着实际偏差是 1(计算公式为 2 - 1)或者为 0(1 - 1),这符合 maxSkew: 1 的约定.
如果该调度器将这个新来的 Pod 放到可用区 B,则 Pod 的分布将成为 [0,3,1].这意味着实际偏差是 2(计算公式为 3 - 1)或者为 3(3 - 0)或者为 1(1 - 0),这违反了 maxSkew: 1 的约定.
如果该调度器将这个新来的 Pod 放到可用区 C,则 Pod 的分布将成为 [0,2,2].这意味着实际偏差是 2(计算公式为 3 - 1)或者为 0(2 - 2),这违反了 maxSkew: 1 的约定.

说明:
这里有个有意思的现象,就是 master 节点也被作为可分配节点加入了计算,master 节点上有这样的污点
taints:
  - effect: PreferNoSchedule
    key: node-role.kubernetes.io/master

最终显示的结果如下
NAME             READY   STATUS    RESTARTS   AGE   IP               NODE    NOMINATED NODE   READINESS G
ATES
pod1-test-zone   1/1     Running   0          13m   10.244.235.129   k8s03   <none>           <none>
pod2-test-zone   1/1     Running   0          13m   10.244.235.130   k8s03   <none>           <none>
pod3-test-zone   1/1     Running   0          13m   10.244.236.130   k8s02   <none>           <none>
pod4-test-zone   1/1     Running   0          12m   10.244.73.67     k8s01   <none>           <none>

** 示例: 多个拓扑分布约束
这里没法用上面的例子,按照下面的配置,pod 最终仍然会分配到 zoneA 上的  node1 上,因此回到官方的例子,如下

假设你拥有一个 4 节点集群,其中 3 个标记为 foo: bar 的 Pod 分别位于 node1、node2 和 node3 上:
   zoneA                  zoneB
Node1   Node2          Node3    Node4
pod     pod            pod      pod

可以组合使用 2 个拓扑分布约束来控制 Pod 在节点和可用区两个维度上的分布:

two-constraints.yaml
----------------------------------------
kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  - maxSkew: 1
    topologyKey: node
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.1
-----------------------------------------------

在这种情况下,为了匹配第一个约束,新的 Pod 只能放置在可用区 B 中;而在第二个约束中,新来的 Pod 只能调度到节点 node4 上.该调度器仅考虑满足所有已定义约束的选项,因此唯一可行的选择是放置在节点 node4 上.

说明:
注意上述的 2 个 maxSkew 必须是交集的关系,才满足条件

** 示例: 有冲突的拓扑分布约束
多个约束可能导致冲突.假设有一个跨 2 个可用区的 3 节点集群:
   zoneA                     zoneB
Node1   Node2          Node3
pod     	  pod              pod
pod                            pod

如果你将 two-constraints.yaml(来自上一个示例的清单)应用到这个集群,你将看到 Pod mypod 保持在 Pending 状态.出现这种情况的原因为: 为了满足第一个约束,Pod mypod 只能放置在可用区 B 中; 而在第二个约束中,Pod mypod 只能调度到节点 node2 上.两个约束的交集将返回一个空集,且调度器无法放置该 Pod.

为了应对这种情形,你可以提高 maxSkew 的值或修改其中一个约束才能使用 whenUnsatisfiable: ScheduleAnyway.根据实际情形,例如若你在故障排查时发现某个漏洞修复工作毫无进展,你还可能决定手动删除一个现有的 Pod.

*** 与节点亲和性和节点选择算符的相互作用
如果 Pod 定义了 spec.nodeSelector 或 spec.affinity.nodeAffinity,调度器将在偏差计算中跳过不匹配的节点.

** 示例: 带节点亲和性的拓扑分布约束
假设你有一个跨可用区 A 到 C 的 5 节点集群:
   zoneA                  zoneB                 zoneC
Node1   Node2          Node3   Node4            Node5
pod     pod            pod

而且你知道可用区 C 必须被排除在外.在这种情况下,可以按如下方式编写清单,以便将 Pod mypod 放置在可用区 B 上,而不是可用区 C 上.同样,Kubernetes 也会一样处理 spec.nodeSelector.

one-constraint-with-nodeaffinity.yaml
--------------------------------------------
kind: Pod
apiVersion: v1
metadata:
  name: mypod
  labels:
    foo: bar
spec:
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: zone     # 如果没有这个值的节点将被忽略
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        foo: bar
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:   # 必须满足
        nodeSelectorTerms:
        - matchExpressions:
          - key: zone
            operator: NotIn
            values:
            - zoneC
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.1
----------------------------------------------------------

* 隐式约定
这里有一些值得注意的隐式约定:
. 只有与新来的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者.
. 调度器会忽略没有任何 topologySpreadConstraints[*].topologyKey 的节点.这意味着:
  1. 位于这些节点上的 Pod 不影响 maxSkew 计算,在上面的例子中,假设节点 node1 没有标签 "zone",则 2 个 Pod 将被忽略,因此新来的 Pod 将被调度到可用区 A 中.
  2. 新的 Pod 没有机会被调度到这类节点上.在上面的例子中,假设节点 node5 带有拼写错误的标签 zone-typo: zoneC(且没有设置 zone 标签).节点 node5 接入集群之后,该节点将被忽略且针对该工作负载的 Pod 不会被调度到那里.
. 注意,如果新 Pod 的 topologySpreadConstraints[*].labelSelector 与自身的标签不匹配,将会发生什么.在上面的例子中,如果移除新 Pod 的标签,则 Pod 仍然可以放置到可用区 B 中的节点上,因为这些约束仍然满足.然而,在放置之后,集群的不平衡程度保持不变.可用区 A 仍然有 2 个 Pod 带有标签 foo: bar,而可用区 B 有 1 个 Pod 带有标签 foo: bar.如果这不是你所期望的,更新工作负载的 topologySpreadConstraints[*].labelSelector 以匹配 Pod 模板中的标签.

* 集群级别的默认约束 - 如果需要修改默认的调度策略,参考: 调度器配置(https://kubernetes.io/zh-cn/docs/reference/scheduling/config/)
为集群设置默认的拓扑分布约束也是可能的.如果需要设置,需要参考上面的"调度器配置"的官方链接
默认拓扑分布约束在且仅在以下条件满足时才会被应用到 Pod 上:
. Pod 没有在其 .spec.topologySpreadConstraints 中定义任何约束.
. Pod 隶属于某个 Service、ReplicaSet、StatefulSet 或 ReplicationController.

默认约束可以设置为调度方案中 PodTopologySpread 插件参数的一部分.约束的设置采用如前所述的 API,只是 labelSelector 必须为空.选择算符是根据 Pod 所属的 Service、ReplicaSet、StatefulSet 或 ReplicationController 来设置的.

配置的示例可能看起来像下面这个样子:
----------------------------------------------------
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
--------------------------------------------------------

** 内置默认约束 - 重要
特性状态:  Kubernetes v1.24 [stable]

如果你没有为 Pod 拓扑分布配置任何集群级别的默认约束,kube-scheduler 的行为就像你指定了以下默认拓扑约束一样:
----------------------------------------------------
defaultConstraints:
  - maxSkew: 3
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
  - maxSkew: 5
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: ScheduleAnyway
----------------------------------------------------

此外,原来用于提供等同行为的 SelectorSpread 插件默认被禁用.

说明:
对于分布约束中所指定的拓扑键而言,PodTopologySpread 插件不会为不包含这些拓扑键的节点评分.这可能导致在使用默认拓扑约束时,其行为与原来的 SelectorSpread 插件的默认行为不同.
如果你的节点不会同时设置 kubernetes.io/hostname 和 topology.kubernetes.io/zone 标签,你应该定义自己的约束而不是使用 Kubernetes 的默认约束.

如果你不想为集群使用默认的 Pod 分布约束,你可以通过设置 defaultingType 参数为 List,并将 PodTopologySpread 插件配置中的 defaultConstraints 参数置空来禁用默认 Pod 分布约束: - 具体如何操作见前面的"调度器配置"的官方链接
---------------------------------------------------
apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
    pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints: []
          defaultingType: List
----------------------------------------------------

* 比较 podAffinity 和 podAntiAffinity
在 Kubernetes 中,Pod 间亲和性和反亲和性控制 Pod 彼此的调度方式(更密集或更分散).

podAffinity
    吸引 Pod;你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中.
podAntiAffinity
    驱逐 Pod.如果将此设为 requiredDuringSchedulingIgnoredDuringExecution 模式,则只有单个 Pod 可以调度到单个拓扑域;如果你选择 preferredDuringSchedulingIgnoredDuringExecution,则你将丢失强制执行此约束的能力.

要实现更细粒度的控制,你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下,从而实现高可用性或节省成本.这也有助于工作负载的滚动更新和平稳地扩展副本规模.

* 已知局限性
. 当 Pod 被移除时,无法保证约束仍被满足.例如,缩减某 Deployment 的规模时,Pod 的分布可能不再均衡.
. 你可以使用 Descheduler 来重新实现 Pod 分布的均衡.
. 具有污点的节点上匹配的 Pod 也会被统计.
. 该调度器不会预先知道集群拥有的所有可用区和其他拓扑域.拓扑域由集群中存在的节点确定.在自动扩缩的集群中,如果一个节点池(或节点组)的节点数量缩减为零,而用户正期望其扩容时,可能会导致调度出现问题.因为在这种情况下,调度器不会考虑这些拓扑域,直至这些拓扑域中至少包含有一个节点.

你可以通过使用感知 Pod 拓扑分布约束并感知整个拓扑域集的集群自动扩缩工具来解决此问题.

vvvvvvvvvvvvvvvvvvvv

考虑下面的 Pod 规约:
pod-with-pod-affinity.yaml
--------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: registry.k8s.io/pause:2.0
-------------------------------------------------------------

注意:
1. 上述的例子是在已经有 pod 并且有标签 security=S1/S2 的情况下才能生效,就是说,如果是第一次部署这个 pod,必须加上标签,否则 pod 只会一直是 pending 的状态.
2. 注意 spec.affinity.podAffinity(用于 pod 亲和性),与之对应的 spec.affinity.nodeAffinity(用于 node 亲和性)
3. spec.affinity.podAntiAffinity(用于 pod 反亲和性),没有与之对应的 node 反亲和性的关键字,用的是 NotIn 和 DoesNotExist 计算符
4. 这里有几个现象需要说明,当后续的 pod 配置文件里加了 security=S1/S2的标签,并不会将此标签计算到此次的部署计划中,比如:
   . zoneA                zoneB
     pod1(security: S1)
              此时,不管新的 pod2 是否自己有 security: S1/S2 的标签,只要使用上面的配置文件,将都会部署在 zoneA
   . zoneA                zoneB
     pod1(security: S1)
     pod2(security: S2)
              此时,不管新的 pod3 是否自己有 security: S1/S2 的标签,只要使用上面的配置文件,将都会部署在 zoneA,不管 zoneA 中是否存在
     security: S2 的 pod,可能与 weight: 100 有关
5. 不论上述的 podAffinity 与 podAntiAffinity 如何组合,都需要经过测试,否则可能不能达到预期的结果,比如上面的第二种情况

metadata:
  name: with-pod1-affinity
  labels:
    security: S1
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
...


本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则.Pod 亲和性规则配置为 requiredDuringSchedulingIgnoredDuringExecution,而 Pod 反亲和性配置为 preferredDuringSchedulingIgnoredDuringExecution.

亲和性规则规定,只有节点属于特定的区域 且该区域中的其他 Pod 已打上 security=S1 标签时,调度器才可以将示例 Pod 调度到此节点上.例如,如果我们有一个具有指定区域(称之为 "Zone V")的集群,此区域由带有 topology.kubernetes.io/zone=V 标签的节点组成,那么只要 Zone V 内已经至少有一个 Pod 打了 security=S1 标签,调度器就可以将此 Pod 调度到 Zone V 内的任何节点.相反,如果 Zone V 中没有带有 security=S1 标签的 Pod,则调度器不会将示例 Pod 调度给该区域中的任何节点.

反亲和性规则规定,如果节点属于特定的区域 且该区域中的其他 Pod 已打上 security=S2 标签,则调度器应尝试避免将 Pod 调度到此节点上.例如,如果我们有一个具有指定区域(我们称之为 "Zone R")的集群,此区域由带有 topology.kubernetes.io/zone=R 标签的节点组成,只要 Zone R 内已经至少有一个 Pod 打了 security=S2 标签,调度器应避免将 Pod 分配给 Zone R 内的任何节点.相反,如果 Zone R 中没有带有 security=S2 标签的 Pod,则反亲和性规则不会影响将 Pod 调度到 Zone R.

原则上,topologyKey 可以是任何合法的标签键.出于性能和安全原因,topologyKey 有一些限制:
. 对于 Pod 亲和性而言,在 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 中,topologyKey 不允许为空.
. 对于 requiredDuringSchedulingIgnoredDuringExecution 要求的 Pod 反亲和性,准入控制器 LimitPodHardAntiAffinityTopology 要求 topologyKey 只能是 kubernetes.io/hostname.如果你希望使用其他定制拓扑逻辑,你可以更改准入控制器或者禁用之.

除了 labelSelector 和 topologyKey,你也可以指定 labelSelector 要匹配的名字空间列表,方法是在 labelSelector 和 topologyKey 所在层同一层次上设置 namespaces.如果 namespaces 被忽略或者为空,则默认为 Pod 亲和性/反亲和性的定义所在的名字空间.

*** 名字空间选择算符
特性状态:  Kubernetes v1.24 [stable]
用户也可以使用 namespaceSelector 选择匹配的名字空间,namespaceSelector 是对名字空间集合进行标签查询的机制.亲和性条件会应用到 namespaceSelector 所选择的名字空间和 namespaces 字段中所列举的名字空间之上.注意,空的 namespaceSelector({})会匹配所有名字空间,而 null 或者空的 namespaces 列表以及 null 值 namespaceSelector 意味着“当前 Pod 的名字空间”.

*** matchLabelKeys
特性状态:  Kubernetes v1.29 [alpha]
说明:
matchLabelKeys 字段是一个 Alpha 级别的字段,在 Kubernetes 1.30 中默认被禁用.当你想要使用此字段时,你必须通过 MatchLabelKeysInPodAffinity 特性门控启用它.

*** mismatchLabelKeys
特性状态:  Kubernetes v1.29 [alpha]
说明:
mismatchLabelKeys 字段是一个 Alpha 级别的字段,在 Kubernetes 1.30 中默认被禁用.当你想要使用此字段时,你必须通过 MatchLabelKeysInPodAffinity 特性门控启用它.

*** 更实际的用例
Pod 间亲和性与反亲和性在与更高级别的集合(例如 ReplicaSet、StatefulSet、 Deployment 等)一起使用时,它们可能更加有用.这些规则使得你可以配置一组工作负载,使其位于所定义的同一拓扑中; 例如优先将两个相关的 Pod 置于相同的节点上.

以一个三节点的集群为例.你使用该集群运行一个带有内存缓存(例如 Redis)的 Web 应用程序.在此例中,还假设 Web 应用程序和内存缓存之间的延迟应尽可能低.你可以使用 Pod 间的亲和性和反亲和性来尽可能地将该 Web 服务器与缓存并置.

在下面的 Redis 缓存 Deployment 示例中,副本上设置了标签 app=store.podAntiAffinity 规则告诉调度器避免将多个带有 app=store 标签的副本部署到同一节点上.因此,每个独立节点上会创建一个缓存实例.
-----------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:3.2-alpine
-----------------------------------------------------------

下例的 Deployment 为 Web 服务器创建带有标签 app=web-store 的副本.Pod 亲和性规则告诉调度器将每个副本放到存在标签为 app=store 的 Pod 的节点上.Pod 反亲和性规则告诉调度器决不要在单个节点上放置多个 app=web-store 服务器.
-----------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.16-alpine
-----------------------------------------------------------------

创建前面两个 Deployment 会产生如下的集群布局,每个 Web 服务器与一个缓存实例并置,并分别运行在三个独立的节点上.
node-1	    	node-2	    	node-3
webserver-1	webserver-2	webserver-3
cache-1	    	cache-2	   		cache-3

总体效果是每个缓存实例都非常可能被在同一个节点上运行的某个客户端访问,这种方法旨在最大限度地减少偏差(负载不平衡)和延迟.

* nodeName
nodeName 是比亲和性或者 nodeSelector 更为直接的形式.nodeName 是 Pod 规约中的一个字段.如果 nodeName 字段不为空,调度器会忽略该 Pod,而指定节点上的 kubelet 会尝试将 Pod 放到该节点上.使用 nodeName 规则的优先级会高于使用 nodeSelector 或亲和性与非亲和性的规则.

使用 nodeName 来选择节点的方式有一些局限性:
. 如果所指代的节点不存在,则 Pod 无法运行,而且在某些情况下可能会被自动删除.
. 如果所指代的节点无法提供用来运行 Pod 所需的资源,Pod 会失败,而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行.
. 在云环境中的节点名称并不总是可预测的,也不总是稳定的.

警告:
nodeName 旨在供自定义调度器或需要绕过任何已配置调度器的高级场景使用.如果已分配的 Node 负载过重,绕过调度器可能会导致 Pod 失败.你可以使用节点亲和性或 nodeselector 字段将 Pod 分配给特定 Node,而无需绕过调度器.

下面是一个使用 nodeName 字段的 Pod 规约示例:
--------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: kube-01
---------------------------

上面的 Pod 只能运行在节点 kube-01 之上.

* Pod 拓扑分布约束
你可以使用 拓扑分布约束(Topology Spread Constraints)来控制 Pod 在集群内故障域之间的分布,故障域的示例有区域(Region)、可用区(Zone)、节点和其他用户自定义的拓扑域.这样做有助于提升性能、实现高可用或提升资源利用率.

* 操作符
下面是你可以在上述 nodeAffinity 和 podAffinity 的 operator 字段中可以使用的所有逻辑运算符.
操作符	                                        行为
In	          			标签值存在于提供的字符串集中
NotIn	        	    标签值不包含在提供的字符串集中
Exists	      		  对象上存在具有此键的标签
DoesNotExist	      对象上不存在具有此键的标签

以下操作符只能与 nodeAffinity 一起使用.
操作符		行为
Gt	  	字段值将被解析为整数,并且该整数小于通过解析此选择算符命名的标签的值所得到的整数
Lt	  	字段值将被解析为整数,并且该整数大于通过解析此选择算符命名的标签的值所得到的整数

说明:
Gt 和 Lt 操作符不能与非整数值一起使用.如果给定的值未解析为整数,则该 Pod 将无法被调度.另外,Gt 和 Lt 不适用于 podAffinity.
vvvvvvvvvvvvvvvvvvvv

* 给节点添加标签
列出你的集群中的节点,包括这些节点上的标签:
$ kubectl get nodes --show-labels

输出类似如下:
NAME    STATUS   ROLES           AGE   VERSION   LABELS
k8s01   Ready    control-plane   10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s02   Ready    <none>            10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s02,kubernetes.io/os=linux
k8s03   Ready    <none>            10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s03,kubernetes.io/os=linux

从你的节点中选择一个,为它添加标签:
$ kubectl label nodes <your-node-name> disktype=ssd

<your-node-name> 是你选择的节点的名称.

验证你选择的节点确实带有 disktype=ssd 标签:
$ kubectl get nodes --show-labels

输出类似如下:
NAME    STATUS   ROLES           AGE   VERSION   LABELS
k8s01   Ready    control-plane   10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
k8s02   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s02,kubernetes.io/os=linux
k8s03   Ready    <none>          10d   v1.30.0   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s03,kubernetes.io/os=linux

在前面的输出中,你可以看到  k8s02 节点有 disktype=ssd 标签.

* 依据强制的节点亲和性调度 Pod
下面清单描述了一个 Pod,它有一个节点亲和性配置 requiredDuringSchedulingIgnoredDuringExecution,disktype=ssd.这意味着 pod 只会被调度到具有 disktype=ssd 标签的节点上.

pod-nginx-required-affinity.yaml
---------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
-------------------------------------------------------------

1. 执行(Apply)此清单来创建一个调度到所选节点上的 Pod:
$ kubectl apply -f pod-nginx-required-affinity.yaml

2. 验证 Pod 已经在所选节点上运行:
$ kubectl get pods --output=wide

输出类似于此:
NAME     READY     STATUS    RESTARTS   AGE    IP           NODE
nginx    1/1       Running   0          13s    10.200.0.4   worker0

* 使用首选的节点亲和性调度 Pod
本清单描述了一个 Pod,它有一个节点亲和性设置 preferredDuringSchedulingIgnoredDuringExecution,disktype: ssd.这意味着 Pod 将首选具有 disktype=ssd 标签的节点.

pod-nginx-preferred-affinity.yaml
---------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
---------------------------------------------------------

执行此清单创建一个会调度到所选节点上的 Pod:
$ kubectl apply -f pod-nginx-preferred-affinity.yaml

验证 Pod 是否在所选节点上运行:
$ kubectl get pods --output=wide

输出类似于此:
NAME     READY     STATUS    RESTARTS   AGE    IP           			NODE
nginx    	1/1       	Running   0          		  13s    10.200.0.4   worker0

## 配置 Pod 初始化
在应用容器运行前,怎样利用 Init 容器初始化 Pod.

* 创建一个包含 Init 容器的 Pod
本例中你将创建一个包含一个应用容器和一个 Init 容器的 Pod.Init 容器在应用容器启动前运行完成.

下面是 Pod 的配置文件:

init-containers.yaml
----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: workdir
      mountPath: /usr/share/nginx/html
  # 这些容器在 Pod 初始化期间运行
  initContainers:
  - name: install
    image: busybox:1.28
    command:
    - wget
    - "-O"
    - "/work-dir/index.html"
    - http://info.cern.ch
    volumeMounts:
    - name: workdir
      mountPath: "/work-dir"
  dnsPolicy: Default
  volumes:
  - name: workdir
    emptyDir: {}
--------------------------------------------------

检查 nginx 容器运行正常:
$ kubectl get pod init-demo

结果表明 nginx 容器运行正常:
NAME        READY     STATUS    RESTARTS   AGE
init-demo   1/1       Running   0          1m

通过 shell 进入 init-demo Pod 中的 nginx 容器:
$ kubectl exec -it init-demo -- /bin/bash

在 shell 中,发送个 GET 请求到 nginx 服务器:
root@nginx:~# apt-get update
root@nginx:~# apt-get install curl
root@nginx:~# curl localhost

结果表明 nginx 正在为 Init 容器编写的 web 页面服务:

<html><head></head><body><header>
<title>http://info.cern.ch</title>
</header>

<h1>http://info.cern.ch - home of the first website</h1>
  ...
<li><a href="http://info.cern.ch/hypertext/WWW/TheProject.html">Browse the first website</a></li>
  ...

## 为容器的生命周期事件设置处理函数
如何为容器的生命周期事件挂接处理函数.Kubernetes 支持 postStart 和 preStop 事件.当一个容器启动后,Kubernetes 将立即发送 postStart 事件;在容器被终结之前,Kubernetes 将发送一个 preStop 事件.容器可以为每个事件指定一个处理程序.

* 定义 postStart 和 preStop 处理函数
在本练习中,你将创建一个包含一个容器的 Pod,该容器为 postStart 和 preStop 事件提供对应的处理函数.

下面是对应 Pod 的配置文件:

lifecycle-events.yaml
-------------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh","-c","echo Hello from the postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
---------------------------------------------------------------------

在上述配置文件中,你可以看到 postStart 命令在容器的 /usr/share 目录下写入文件 message.命令 preStop 负责优雅地终止 nginx 服务.当因为失效而导致容器终止时,这一处理方式很有用.

创建 Pod:
$ kubectl apply -f lifecycle-events.yaml

验证 Pod 中的容器已经运行:
$ kubectl get pod lifecycle-demo

使用 shell 连接到你的 Pod 里的容器:
$ kubectl exec -it lifecycle-demo -- /bin/bash

在 shell 中,验证 postStart 处理函数创建了 message 文件:
$ root@lifecycle-demo:/# cat /usr/share/message

命令行输出的是 postStart 处理函数所写入的文本:
Hello from the postStart handler

* 讨论
Kubernetes 在容器创建后立即发送 postStart 事件.然而,postStart 处理函数的调用不保证早于容器的入口点(entrypoint) 的执行.postStart 处理函数与容器的代码是异步执行的,但 Kubernetes 的容器管理逻辑会一直阻塞等待 postStart 处理函数执行完毕.只有 postStart 处理函数执行完毕,容器的状态才会变成 RUNNING.

Kubernetes 在容器结束前立即发送 preStop 事件.除非 Pod 宽限期限超时,Kubernetes 的容器管理逻辑会一直阻塞等待 preStop 处理函数执行完毕.

说明:
Kubernetes 只有在一个 Pod 或该 Pod 中的容器结束(Terminated) 的时候才会发送 preStop 事件,这意味着在 Pod 完成(Completed) 时 preStop 的事件处理逻辑不会被触发.

## 配置 Pod 使用 ConfigMap
很多应用在其初始化或运行期间要依赖一些配置信息.大多数时候,存在要调整配置参数所设置的数值的需求.ConfigMap 是 Kubernetes 的一种机制,可让你将配置数据注入到应用的 Pod 内部.

ConfigMap 概念允许你将配置清单与镜像内容分离,以保持容器化的应用程序的可移植性.例如,你可以下载并运行相同的容器镜像来启动容器,用于本地开发、系统测试或运行实时终端用户工作负载.

本页提供了一系列使用示例,这些示例演示了如何创建 ConfigMap 以及配置 Pod 使用存储在 ConfigMap 中的数据.

* 创建 ConfigMap
你可以使用 kubectl create configmap 或者在 kustomization.yaml 中的 ConfigMap 生成器来创建 ConfigMap.

** 使用 kubectl create configmap 创建 ConfigMap
你可以使用 kubectl create configmap 命令基于目录、 文件或者字面值来创建 ConfigMap:

$ kubectl create configmap <映射名称> <数据源>

其中,<映射名称> 是为 ConfigMap 指定的名称,<数据源> 是要从中提取数据的目录、 文件或者字面值.ConfigMap 对象的名称必须是合法的 DNS 子域名.

在你基于文件来创建 ConfigMap 时,<数据源> 中的键名默认取自文件的基本名,而对应的值则默认为文件的内容.

你可以使用 kubectl describe 或者 kubectl get 获取有关 ConfigMap 的信息.

*** 基于一个目录来创建 ConfigMap
你可以使用 kubectl create configmap 基于同一目录中的多个文件创建 ConfigMap.当你基于目录来创建 ConfigMap 时,kubectl 识别目录下文件名可以作为合法键名的文件,并将这些文件打包到新的 ConfigMap 中.普通文件之外的所有目录项都会被忽略(例如: 子目录、符号链接、设备、管道等等).

说明:
用于创建 ConfigMap 的每个文件名必须由可接受的字符组成,即: 字母(A 到 Z 和 a 到 z)、数字(0 到 9)、'-'、'_' 或 '.'.如果在一个目录中使用 kubectl create configmap,而其中任一文件名包含不可接受的字符,则 kubectl 命令可能会失败.

kubectl 命令在遇到不合法的文件名时不会打印错误.

创建本地目录:
$ mkdir -p configure-pod-container/configmap/

现在,下载示例的配置并创建 ConfigMap:

# 将示例文件下载到 `configure-pod-container/configmap/` 目录
wget https://kubernetes.io/examples/configmap/game.properties -O configure-pod-container/configmap/game.properties
wget https://kubernetes.io/examples/configmap/ui.properties -O configure-pod-container/configmap/ui.properties

# 创建 ConfigMap
$ kubectl create configmap game-config --from-file=configure-pod-container/configmap/

输出类似以下内容:
Name:         game-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30

ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice


BinaryData
====

Events:  <none>
-----------------------

configure-pod-container/configmap/ 目录中的 game.properties 和 ui.properties 文件出现在 ConfigMap 的 data 部分.

$ kubectl get configmaps game-config -o yaml

输出类似以下内容:
---------------------------------
apiVersion: v1
data:
  game.properties: |
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  creationTimestamp: "2024-08-14T09:53:35Z"
  name: game-config
  namespace: default
  resourceVersion: "55887"
  uid: c66783bd-1024-484e-b1ed-20858733108b
----------------------------------------------------

*** 基于文件创建 ConfigMap
你可以使用 kubectl create configmap 基于单个文件或多个文件创建 ConfigMap.

例如: 
$ kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties

将产生以下 ConfigMap: 
$ kubectl describe configmaps game-config-2

输出类似以下内容: 
Name:         game-config-2
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30

你可以多次使用 --from-file 参数,从多个数据源创建 ConfigMap.
$ kubectl create configmap game-config-2 --from-file=configure-pod-container/configmap/game.properties --from-file=configure-pod-container/configmap/ui.properties

你可以使用以下命令显示 game-config-2 ConfigMap 的详细信息: 
$ kubectl describe configmaps game-config-2

输出类似以下内容: 
Name:         game-config-2
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
ui.properties:
----
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

*** 使用 --from-env-file 选项基于 env 文件创建 ConfigMap,例如: 
# Env 文件包含环境变量列表.其中适用以下语法规则:
# 这些语法规则适用: 
#   Env 文件中的每一行必须为 VAR=VAL 格式.
#   以＃开头的行(即注释)将被忽略.
#   空行将被忽略.
#   引号不会被特殊处理(即它们将成为 ConfigMap 值的一部分).

# 将示例文件下载到 `configure-pod-container/configmap/` 目录
wget https://kubernetes.io/examples/configmap/game-env-file.properties -O configure-pod-container/configmap/game-env-file.properties
wget https://kubernetes.io/examples/configmap/ui-env-file.properties -O configure-pod-container/configmap/ui-env-file.properties

# Env 文件 `game-env-file.properties` 如下所示
cat configure-pod-container/configmap/game-env-file.properties
enemies=aliens
lives=3
allowed="true"

# 此注释和上方的空行将被忽略

$ kubectl create configmap game-config-env-file \
       --from-env-file=configure-pod-container/configmap/game-env-file.properties

将产生以下 ConfigMap.查看 ConfigMap: 
$ kubectl get configmap game-config-env-file -o yaml

输出类似以下内容: 
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2019-12-27T18:36:28Z
  name: game-config-env-file
  namespace: default
  resourceVersion: "809965"
  uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8
data:
  allowed: '"true"'
  enemies: aliens
  lives: "3"

从 Kubernetes 1.23 版本开始,kubectl 支持多次指定 --from-env-file 参数来从多个数据源创建 ConfigMap.
$ kubectl create configmap config-multi-env-files \
        --from-env-file=configure-pod-container/configmap/game-env-file.properties \
        --from-env-file=configure-pod-container/configmap/ui-env-file.properties

将产生以下 ConfigMap: 
$ kubectl get configmap config-multi-env-files -o yaml

输出类似以下内容: 
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2019-12-27T18:38:34Z
  name: config-multi-env-files
  namespace: default
  resourceVersion: "810136"
  uid: 252c4572-eb35-11e7-887b-42010a8002b8
data:
  allowed: '"true"'
  color: purple
  enemies: aliens
  how: fairlyNice
  lives: "3"
  textmode: "true"

*** 定义从文件创建 ConfigMap 时要使用的键
在使用 --from-file 参数时,你可以定义在 ConfigMap 的 data 部分出现键名,而不是按默认行为使用文件名: 
$ kubectl create configmap game-config-3 --from-file=<我的键名>=<文件路径>
<我的键名> 是你要在 ConfigMap 中使用的键名,<文件路径> 是你想要键所表示的数据源文件的位置.

例如: 
$ kubectl create configmap game-config-3 --from-file=game-special-key=configure-pod-container/configmap/game.properties

将产生以下 ConfigMap: 
$ kubectl get configmaps game-config-3 -o yaml

输出类似以下内容: 
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2022-02-18T18:54:22Z
  name: game-config-3
  namespace: default
  resourceVersion: "530"
  uid: 05f8da22-d671-11e5-8cd0-68f728db1985
data:
  game-special-key: |   # 注意这里,不是默认的文件名称,而是在命令行时候自定义的
    enemies=aliens
    lives=3
    enemies.cheat=true
    enemies.cheat.level=noGoodRotten
    secret.code.passphrase=UUDDLRLRBABAS
    secret.code.allowed=true
    secret.code.lives=30

*** 根据字面值创建 ConfigMap
你可以将 kubectl create configmap 与 --from-literal 参数一起使用,通过命令行定义文字值: 
$ kubectl create configmap special-config --from-literal=special.how=very --from-literal=special.type=charm

你可以传入多个键值对.命令行中提供的每对键值在 ConfigMap 的 data 部分中均表示为单独的条目.
$ kubectl get configmaps special-config -o yaml

输出类似以下内容: 
apiVersion: v1
data:
  special.how: very
  special.type: charm
kind: ConfigMap
metadata:
  creationTimestamp: "2024-08-15T07:56:20Z"
  name: special-config
  namespace: default
  resourceVersion: "59136"
  uid: 1d1deef6-87ee-4dc5-ad42-355edc0c7b9f

** 基于生成器创建 ConfigMap
你还可以基于生成器(Generators)创建 ConfigMap,然后将其应用于集群的 API 服务器上创建对象.生成器应在目录内的 kustomization.yaml 中指定.

*** 基于文件生成 ConfigMap
例如,要基于 configure-pod-container/configmap/game.properties 文件生成一个 ConfigMap: 

# 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件
cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: game-config-4
  options:  # 这里不是必须的
    labels:
      game-config: config-4
  files:
  - configure-pod-container/configmap/game.properties
EOF

应用(Apply)kustomization 目录创建 ConfigMap 对象: 
$ kubectl apply -k .

你可以像这样检查 ConfigMap 已经被创建: 
$ kubectl get configmap

NAME                       DATA   AGE
game-config-4-92cb5f7c7h   1      37s

也可以这样: 
$ kubectl describe configmaps/game-config-4-92cb5f7c7h
Name:         game-config-4-92cb5f7c7h
Namespace:    default
Labels:       game-config=config-4
Annotations:  <none>

Data
====
game.properties:
----
enemies=aliens
lives=3
enemies.cheat=true
enemies.cheat.level=noGoodRotten
secret.code.passphrase=UUDDLRLRBABAS
secret.code.allowed=true
secret.code.lives=30
BinaryData
====
Events:  <none>
请注意,生成的 ConfigMap 名称具有通过对内容进行散列而附加的后缀,这样可以确保每次修改内容时都会生成新的 ConfigMap.

*** 定义从文件生成 ConfigMap 时要使用的键
在 ConfigMap 生成器中,你可以定义一个非文件名的键名.例如,从 configure-pod-container/configmap/game.properties 文件生成 ConfigMap,但使用 game-special-key 作为键名: 

# 创建包含 ConfigMapGenerator 的 kustomization.yaml 文件
cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: game-config-5
  options:  # 不是必须的
    labels:
      game-config: config-5
  files:
  - game-special-key=configure-pod-container/configmap/game.properties
EOF

应用 Kustomization 目录创建 ConfigMap 对象.
$ kubectl apply -k .
configmap/game-config-5-2ddm949dbt created

*** 基于字面值生成 ConfigMap
此示例向你展示如何使用 Kustomize 和 kubectl,基于两个字面键/值对 special.type=charm 和 special.how=very 创建一个 ConfigMap.为了实现这一点,你可以配置 ConfigMap 生成器.创建(或替换)kustomization.yaml,使其具有以下内容.

---
# 基于字面创建 ConfigMap 的 kustomization.yaml 内容
$ cat <<EOF >./kustomization.yaml
configMapGenerator:
- name: special-config-2
  literals:
  - special.how=very
  - special.type=charm
EOF

应用 Kustomization 目录创建 ConfigMap 对象.
$ kubectl apply -k .
configmap/special-config-2-2b86tk8fhm created

* 临时清理
在继续之前,清理你创建的一些 ConfigMap: 
$ kubectl delete configmap special-config
$ kubectl delete configmap env-config

# 注意这里,使用了 -l 参数,并且使用了标签的选择算符 in
# 如下,这 2 个  cm 在建立的时候,使用了标签,因此,可以使用 标签选择算符 in
# 参考: https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/labels/#%E5%9F%BA%E4%BA%8E%E9%9B%86%E5%90%88-%E7%9A%84%E9%9C%80%E6%B1%82
# kubectl get cm --show-labels
# game-config-4-92cb5f7c7h      1      71s    game-config=config-4
# game-config-5-2ddm949dbt      1      105s   game-config=config-5

$ kubectl delete configmap -l 'game-config in (config-4,config-5)' 

* 使用 ConfigMap 数据定义容器环境变量
** 使用单个 ConfigMap 中的数据定义容器环境变量

1. 在 ConfigMap 中将环境变量定义为键值对: 
$ kubectl create configmap special-config --from-literal=special.how=very

2. 将 ConfigMap 中定义的 special.how 赋值给 Pod 规约中的 SPECIAL_LEVEL_KEY 环境变量.

pod-single-configmap-env-variable.yaml
-----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        # 定义环境变量
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              # ConfigMap 包含你要赋给 SPECIAL_LEVEL_KEY 的值
              name: special-config    # 这里表示 kubectl get cm 中显示的 configmap 的名字
              # 指定与取值相关的键名
              key: special.how
  restartPolicy: Never
---------------------------------------------------------

创建 Pod: 
$ kubectl create -f pod-single-configmap-env-variable.yaml

现在,Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very.

** 使用来自多个 ConfigMap 的数据定义容器环境变量 
与前面的示例一样,首先创建 ConfigMap.这是你将使用的清单: 

configmaps.yaml
--------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: env-config
  namespace: default
data:
  log_level: INFO
----------------------------

. 创建 ConfigMap: 
kubectl create -f configmaps.yaml

. 在 Pod 规约中定义环境变量.

pod-multiple-configmap-env-variable.yaml
-----------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:                             # 关键字
            configMapKeyRef:
              name: special-config
              key: special.how
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: log_level
  restartPolicy: Never
------------------------------------

创建 Pod: 
$ kubectl create -f pod-multiple-configmap-env-variable.yaml

现在,Pod 的输出包含环境变量 SPECIAL_LEVEL_KEY=very 和 LOG_LEVEL=INFO.

* 将 ConfigMap 中的所有键值对配置为容器环境变量
创建一个包含多个键值对的 ConfigMap.

configmap-multikeys.yaml
--------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charm
---------------------------

创建 ConfigMap:
$ kubectl create -f configmap-multikeys.yaml

使用 envFrom 将所有 ConfigMap 的数据定义为容器环境变量,ConfigMap 中的键成为 Pod 中的环境变量名称.

pod-configmap-envFrom.yaml
--------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:                              # 关键,将多个 key/value 导入,区别于 valueFrom 的单个 key/value
      - configMapRef:
          name: special-config
  restartPolicy: Never
---------------------------------------------

创建 Pod: 
$ kubectl create -f pod-configmap-envFrom.yaml

现在,Pod 的输出包含环境变量 SPECIAL_LEVEL=very 和 SPECIAL_TYPE=charm

* 在 Pod 命令中使用 ConfigMap 定义的环境变量
你可以使用 $(VAR_NAME) Kubernetes 替换语法在容器的 command 和 args 属性中使用 ConfigMap 定义的环境变量.

例如,以下 Pod 清单: 
pod-configmap-env-var-valueFrom.yaml
----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/echo", "$(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)" ]
      env:
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: SPECIAL_LEVEL
        - name: SPECIAL_TYPE_KEY
          valueFrom:
            configMapKeyRef:
              name: special-config
              key: SPECIAL_TYPE
  restartPolicy: Never
-----------------------------------------

通过运行下面命令创建该 Pod: 
$ kubectl create -f pod-configmap-env-var-valueFrom.yaml

此 Pod 在 test-container 容器中产生以下输出: 

kubectl logs dapi-test-pod
very charm

* 将 ConfigMap 数据添加到一个卷中
如基于文件创建 ConfigMap 中所述,当你使用 --from-file 创建 ConfigMap 时,文件名成为存储在 ConfigMap 的 data 部分中的键,文件内容成为键对应的值.

本节中的示例引用了一个名为 special-config 的 ConfigMap: 

configmap-multikeys.yaml
--------------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  SPECIAL_LEVEL: very
  SPECIAL_TYPE: charm
---------------------------------------

创建 ConfigMap: 
$ kubectl create -f configmap-multikeys.yaml

** 使用存储在 ConfigMap 中的数据填充卷
在 Pod 规约的 volumes 部分下添加 ConfigMap 名称.这会将 ConfigMap 数据添加到 volumeMounts.mountPath 所指定的目录 (在本例中为 /etc/config).command 部分列出了名称与 ConfigMap 中的键匹配的目录文件.

pod-configmap-volume.yaml
----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh", "-c", "ls /etc/config/" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        # 提供包含要添加到容器中的文件的 ConfigMap 的名称
        name: special-config
  restartPolicy: Never
------------------------------------------------------

创建 Pod: 
$ kubectl create -f pod-configmap-volume.yaml
Pod 运行时,命令 ls /etc/config/ 产生下面的输出: 

SPECIAL_LEVEL
SPECIAL_TYPE

文本数据会展现为 UTF-8 字符编码的文件.如果使用其他字符编码,可以使用 binaryData(详情参阅 ConfigMap 对象).

说明: 
如果该容器镜像的 /etc/config 目录中有一些文件,卷挂载将使该镜像中的这些文件无法访问.

** 将 ConfigMap 数据添加到卷中的特定路径
使用 path 字段为特定的 ConfigMap 项目指定预期的文件路径.在这里,ConfigMap 中键 SPECIAL_LEVEL 的内容将挂载在 config-volume 卷中 /etc/config/keys 文件中.

pod-configmap-volume-specific-key.yaml
-----------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      command: [ "/bin/sh","-c","cat /etc/config/keys" ]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: special-config
        items:
        - key: SPECIAL_LEVEL
          path: keys
  restartPolicy: Never
--------------------------------------

创建 Pod: 
$ kubectl create -f pod-configmap-volume-specific-key.yaml

当 Pod 运行时,命令 cat /etc/config/keys 产生以下输出: 
very

注意: 
如前,/etc/config/ 目录中所有先前的文件都将被删除.

删除该 Pod: 
$ kubectl delete pod dapi-test-pod --now

你可以将密钥投射到特定路径,语法请参阅 Secret(https://kubernetes.io/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B) 指南中的相应部分.你可以设置密钥的 POSIX 权限,语法请参阅 Secret 指南中的相应部分.

** 映射键到指定路径并设置文件访问权限 
你可以将指定键名投射到特定目录,也可以逐个文件地设定访问权限.Secret(https://kubernetes.io/zh-cn/docs/tasks/inject-data-application/distribute-credentials-secure/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B) 指南中为这一语法提供了解释.

** 可选引用
ConfigMap 引用可以被标记为可选.如果 ConfigMap 不存在,则挂载的卷将为空.如果 ConfigMap 存在,但引用的键不存在,则挂载点下的路径将不存在.

** 挂载的 ConfigMap 会被自动更新
当已挂载的 ConfigMap 被更新时,所投射的内容最终也会被更新.这适用于 Pod 启动后可选引用的 ConfigMap 重新出现的情况.

Kubelet 在每次定期同步时都会检查所挂载的 ConfigMap 是否是最新的.然而,它使用其基于 TTL 机制的本地缓存来获取 ConfigMap 的当前值.因此,从 ConfigMap 更新到新键映射到 Pod 的总延迟可能与 kubelet 同步周期(默认为 1 分钟)+ kubelet 中 ConfigMap 缓存的 TTL(默认为 1 分钟)一样长.你可以通过更新 Pod 的一个注解来触发立即刷新.

说明: 
使用 ConfigMap 作为 subPath 卷(将单一的卷共享给多个挂载使用)的容器将不会收到 ConfigMap 更新.

* 了解 ConfigMap 和 Pod
ConfigMap API 资源将配置数据存储为键值对.数据可以在 Pod 中使用,也可以用来提供系统组件(如控制器)的配置.ConfigMap 与 Secret 类似,但是提供的是一种处理不含敏感信息的字符串的方法.用户和系统组件都可以在 ConfigMap 中存储配置数据.

说明: 
ConfigMap 应该引用属性文件,而不是替换它们.可以将 ConfigMap 理解为类似于 Linux /etc 目录及其内容的东西.例如,如果你基于 ConfigMap 创建 Kubernetes 卷,则 ConfigMap 中的每个数据项都由该数据卷中的某个独立的文件表示.

ConfigMap 的 data 字段包含配置数据.如下例所示,它可以简单(如用 --from-literal 的单个属性定义)或复杂(如用 --from-file 的配置文件或 JSON blob 定义).
----------------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  creationTimestamp: 2016-02-18T19:14:38Z
  name: example-config
  namespace: default
data:
  # 使用 --from-literal 定义的简单属性
  example.property.1: hello
  example.property.2: world
  # 使用 --from-file 定义复杂属性的例子
  example.property.file: |-
    property.1=value-1
    property.2=value-2
    property.3=value-3   
------------------------------------------ 
当 kubectl 从非 ASCII 或 UTF-8 编码的输入创建 ConfigMap 时,该工具将这些输入放入 ConfigMap 的 binaryData 字段,而不是 data 字段.文本和二进制数据源都可以组合在一个 ConfigMap 中.

如果你想查看 ConfigMap 中的 binaryData 键(及其值),可以运行 kubectl get configmap -o jsonpath='{.binaryData}' <name>.

Pod 可以从使用 data 或 binaryData 的 ConfigMap 中加载数据.

** 可选的 ConfigMap
你可以在 Pod 规约中将对 ConfigMap 的引用标记为可选(optional).如果 ConfigMap 不存在,那么它在 Pod 中为其提供数据的配置(例如: 环境变量、挂载的卷)将为空.如果 ConfigMap 存在,但引用的键不存在,那么数据也是空的.

例如,以下 Pod 规约将 ConfigMap 中的环境变量标记为可选: 
---------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: ["/bin/sh", "-c", "env"]
      env:                # 注意这里,是环境变量
        - name: SPECIAL_LEVEL_KEY
          valueFrom:
            configMapKeyRef:
              name: a-config
              key: akey
              optional: true # 将环境变量标记为可选
  restartPolicy: Never
-------------------------------------------

当你运行这个 Pod 并且名称为 a-config 的 ConfigMap 不存在时,输出空值.当你运行这个 Pod 并且名称为 a-config 的 ConfigMap 存在,但是在 ConfigMap 中没有名称为 akey 的键时,控制台输出也会为空.如果你确实在名为 a-config 的 ConfigMap 中为 akey 设置了键值,那么这个 Pod 会打印该值,然后终止.

你也可以在 Pod 规约中将 ConfigMap 提供的卷和文件标记为可选.此时 Kubernetes 将总是为卷创建挂载路径,即使引用的 ConfigMap 或键不存在.例如,以下 Pod 规约将所引用得 ConfigMap 的卷标记为可选: 
----------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: gcr.io/google_containers/busybox
      command: ["/bin/sh", "-c", "ls /etc/config"]
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:                      # 注意这里,是卷
    - name: config-volume
      configMap:
        name: no-config
        optional: true # 将引用的 ConfigMap 的卷标记为可选
  restartPolicy: Never
------------------------------------------------------

** 限制
在 Pod 规约中引用某个 ConfigMap 之前,必须先创建这个对象,或者在 Pod 规约中将 ConfigMap 标记为 optional(请参阅可选的 ConfigMaps).如果所引用的 ConfigMap 不存在,并且没有将应用标记为 optional 则 Pod 将无法启动.同样,引用 ConfigMap 中不存在的主键也会令 Pod 无法启动,除非你将 Configmap 标记为 optional.
如果你使用 envFrom 来基于 ConfigMap 定义环境变量,那么无效的键将被忽略.Pod 可以被启动,但无效名称将被记录在事件日志中(InvalidVariableNames).日志消息列出了每个被跳过的键.例如: 
$ kubectl get events

输出与此类似: 
LASTSEEN FIRSTSEEN COUNT NAME          KIND  SUBOBJECT  TYPE      REASON                            SOURCE                MESSAGE
0s       0s        1     dapi-test-pod Pod              Warning   InvalidEnvironmentVariableNames   {kubelet, 127.0.0.1}  Keys [1badkey, 2alsobad] from the EnvFrom configMap default/myconfig were skipped since they are considered invalid environment variable names.

. ConfigMap 位于确定的名字空间中.每个 ConfigMap 只能被同一名字空间中的 Pod 引用.
. 你不能将 ConfigMap 用于静态 Pod(后续章节介绍),因为 Kubernetes 不支持这种用法.

## 在 Pod 中的容器之间共享进程命名空间 -- 通过在其他容器中查看无法登陆容器
为 Pod 配置进程命名空间共享.当启用进程命名空间共享时,容器中的进程对同一 Pod 中的所有其他容器都是可见的.
你可以使用此功能来配置协作容器,比如日志处理 sidecar 容器,或者对那些不包含诸如 shell 等调试实用工具的镜像进行故障排查.

* 配置 Pod
使用 Pod .spec 中的 shareProcessNamespace 字段可以启用进程命名空间共享.例如: 

share-process-namespace.yaml
----------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  shareProcessNamespace: true
  containers:
  - name: nginx
    image: nginx
  - name: shell
    image: busybox:1.28
    securityContext:
      capabilities:
        add:
        - SYS_PTRACE
    stdin: true
    tty: true
----------------------------------

1. 在集群中创建 nginx Pod: 
$ kubectl apply -f share-process-namespace.yaml

2. 获取容器 shell,执行 ps: 
$ kubectl attach -it nginx -c shell

如果没有看到命令提示符,请按 enter 回车键.在容器 shell 中: 

# 在 “shell” 容器中运行以下命令
ps ax

输出类似于: 
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   14 101       0:00 nginx: worker process
   15 root      0:00 sh
   21 root      0:00 ps ax

你可以在其他容器中对进程发出信号.例如,发送 SIGHUP 到 nginx 以重启工作进程.此操作需要 SYS_PTRACE 权能.

# 在 “shell” 容器中运行以下命令
kill -HUP 8   # 如有必要,更改 “8” 以匹配 nginx master 进程的 PID
ps ax

输出类似于: 
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   15 root      0:00 sh
   22 101       0:00 nginx: worker process
   23 root      0:00 ps ax

甚至可以使用 /proc/$pid/root 链接访问另一个容器的文件系统.

# 在 “shell” 容器中运行以下命令
# 如有必要,更改 “8” 为 Nginx 进程的 PID
head /proc/8/root/etc/nginx/nginx.conf  # 注意这里,

输出类似于: 
user  nginx;
worker_processes  1;

error_log  /var/log/nginx/error.log warn;
pid        /var/run/nginx.pid;

events {
    worker_connections  1024;

* 理解进程命名空间共享 - 查看其他容器内容的关键
Pod 共享许多资源,因此它们共享进程命名空间是很有意义的.不过,有些容器可能希望与其他容器隔离,因此了解这些差异很重要:
1. 容器进程不再具有 PID 1.在没有 PID 1 的情况下,一些容器拒绝启动 (例如,使用 systemd 的容器),或者拒绝执行 kill -HUP 1 之类的命令来通知容器进程.在具有共享进程命名空间的 Pod 中,kill -HUP 1 将  通知 Pod 沙箱(在上面的例子中是 /pause).
2. 进程对 Pod 中的其他容器可见.这包括 /proc 中可见的所有信息,例如作为参数或环境变量传递的密码.这些仅受常规 Unix 权限的保护.
3. 容器文件系统通过 /proc/$pid/root 链接对 Pod 中的其他容器可见.这使调试更加容易,但也意味着文件系统安全性只受文件系统权限的保护.

## 为 Pod 配置 user 名字空间
特性状态:  Kubernetes v1.30 [beta]
为 Pod 配置 user 名字空间.可以将容器内的用户与主机上的用户隔离开来.

在容器中以 root 用户运行的进程可以以不同的(非 root)用户在宿主机上运行;换句话说,进程在 user 名字空间内部拥有执行操作的全部特权,但在 user 名字空间外部并没有执行操作的特权.

你可以使用这个特性来减少有害的容器对同一宿主机上其他容器的影响.有些安全脆弱性问题被评为 HIGH 或 CRITICAL,但当 user 名字空间被启用时,它们是无法被利用的.相信 user 名字空间也能减轻一些未来的漏洞影响.

在不使用 user 名字空间的情况下,对于以 root 用户运行的容器而言,发生容器逃逸时,容器将拥有在宿主机上的 root 特权.如果容器被赋予了某些权限,则这些权限在宿主机上同样有效.当使用 user 名字空间时这些都不可能发生.

* 准备开始
你的 Kubernetes 服务器版本必须不低于版本 v1.25.要获知版本信息,请输入 kubectl version.
. 节点的操作系统必须为 Linux
. 你需要在宿主机上执行命令
. 你需要能够通过 exec 操作进入 Pod
. 你需要启用 UserNamespacesSupport 特性门控

说明: 
在 user 名字空间原来仅支持无状态的 Pod 时,启用 user 名字空间的特性门控先前被命名为 UserNamespacesStatelessPodsSupport.只有 Kubernetes v1.25 到 v1.27 才能识别 UserNamespacesStatelessPodsSupport.

详细参考:
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/user-namespaces/

## 创建静态 Pod - 注意: Pod 名称将把以连字符开头的节点主机名作为后缀,比如: static-web(yaml 中定义)-k8s02(Node名称)
静态 Pod 在指定的节点上由 kubelet 守护进程直接管理,不需要 API 服务器监管.与由控制面管理的 Pod(例如,Deployment) 不同;kubelet 监视每个静态 Pod(在它失败之后重新启动).

静态 Pod 始终都会绑定到特定节点的 Kubelet 上.

kubelet 会尝试通过 Kubernetes API 服务器为每个静态 Pod 自动创建一个镜像 Pod.这意味着节点上运行的静态 Pod 对 API 服务来说是可见的,但是不能通过 API 服务器来控制.Pod 名称将把以连字符开头的节点主机名作为后缀.

说明: 
如果你在运行一个 Kubernetes 集群,并且在每个节点上都运行一个静态 Pod,就可能需要考虑使用 DaemonSet 替代这种方式.

说明: 
静态 Pod 的 spec 不能引用其他 API 对象 (如: ServiceAccount、ConfigMap、Secret 等).

说明: 
静态 Pod 不支持临时容器.

* 创建静态 Pod 
可以通过文件系统上的配置文件或者 Web 网络上的配置文件来配置静态 Pod.

** 文件系统上的静态 Pod 声明文件
声明文件是标准的 Pod 定义文件,以 JSON 或者 YAML 格式存储在指定目录.路径设置在 Kubelet 配置文件的 staticPodPath: <目录> 字段,kubelet 会定期的扫描这个文件夹下的 YAML/JSON 文件来创建/删除静态 Pod.注意 kubelet 扫描目录的时候会忽略以点开头的文件.

例如: 下面是如何以静态 Pod 的方式启动一个简单 web 服务: 
1. 选择一个要运行静态 Pod 的节点.在这个例子中选择 my-node1.
ssh my-node1

2. 选择一个目录,比如在 /etc/kubernetes/manifests 目录来保存 Web 服务 Pod 的定义文件,例如 /etc/kubernetes/manifests/static-web.yaml: 

# 在 kubelet 运行的节点上执行以下命令
mkdir -p /etc/kubernetes/manifests/
cat <<EOF >/etc/kubernetes/manifests/static-web.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-web
  labels:
    role: myrole
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
EOF

3. 在该节点上配置 kubelet,在 kubelet 配置文件中设定 staticPodPath 值.欲了解更多信息,请参考通过配置文件设定 kubelet 参数.
^^^^^^^^^^^^^^^^^^^^
# kubelet 配置文件中参数
使用 kubeadm 模式启动的 kubelet(systemd 管理) 配置文件路径为
/var/lib/kubelet/config.yaml

在此文件中添加一行
staticPodPath: /etc/kubernetes/manifests

当前使用的是 k8s v1.30.0 的版本,默认已经有此行参数
vvvvvvvvvvvvvvvvvvvv

另一个已弃用的方法是,在该节点上通过命令行参数配置 kubelet,以便从本地查找静态 Pod 清单.若使用这种弃用的方法,请启动 kubelet 时加上 --pod-manifest-path=/etc/kubernetes/manifests/ 参数.

4. 重启 kubelet.在 Fedora 上,你将使用下面的命令: 

# 在 kubelet 运行的节点上执行以下命令
systemctl restart kubelet

** Web 网上的静态 Pod 声明文件
Kubelet 根据 --manifest-url=<URL> 参数的配置定期的下载指定文件,并且转换成 JSON/YAML 格式的 Pod 定义文件.与文件系统上的清单文件使用方式类似,kubelet 调度获取清单文件.如果静态 Pod 的清单文件有改变,kubelet 会应用这些改变.

按照下面的方式来: 
1. 创建一个 YAML 文件,并保存在 Web 服务器上,这样你就可以将该文件的 URL 传递给 kubelet.
-----------------------
apiVersion: v1
kind: Pod
metadata:
  name: static-web
  labels:
    role: myrole
spec:
  containers:
    - name: web
      image: nginx
      ports:
        - name: web
          containerPort: 80
          protocol: TCP
-------------------------------

2. 通过在选择的节点上使用 --manifest-url=<manifest-url> 配置运行 kubelet.在 Fedora 添加下面这行到 /etc/kubernetes/kubelet: 
KUBELET_ARGS="--cluster-dns=10.254.0.10 --cluster-domain=kube.local --manifest-url=<manifest-url>"

3. 重启 kubelet.在 Fedora 上,你将运行如下命令: 

# 在 kubelet 运行的节点上执行以下命令
systemctl restart kubelet

* 观察静态 Pod 的行为
当 kubelet 启动时,会自动启动所有定义的静态 Pod.当定义了一个静态 Pod 并重新启动 kubelet 时,新的静态 Pod 就应该已经在运行了.

可以在节点上运行下面的命令来查看正在运行的容器(包括静态 Pod): 
# 在 kubelet 运行的节点上执行以下命令
crictl ps

输出可能会像这样: 
CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
129fd7d382018   docker.io/library/nginx@sha256:...    11 minutes ago    Running    web     0          34533c6729106

说明: 
crictl 会输出镜像 URI 和 SHA-256 校验和.NAME 看起来像:  docker.io/library/nginx@sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31.

可以在 API 服务上看到镜像 Pod: 
$ kubectl get pods
NAME                  READY   STATUS    RESTARTS        AGE
static-web-my-node1   1/1     Running   0               2m

说明: 
要确保 kubelet 在 API 服务上有创建镜像 Pod 的权限.如果没有,创建请求会被 API 服务拒绝.

静态 Pod 上的标签被传播到镜像 Pod.你可以通过选择算符使用这些标签.

如果你用 kubectl 从 API 服务上删除镜像 Pod,kubelet 不会移除静态 Pod: 
$ kubectl delete pod static-web-my-node1

pod "static-web-my-node1" deleted

可以看到 Pod 还在运行: 
$ kubectl get pods
NAME                  READY   STATUS    RESTARTS   AGE
static-web-my-node1   1/1     Running   0          4s

回到 kubelet 运行所在的节点上,你可以手动停止容器.可以看到过了一段时间后 kubelet 会发现容器停止了并且会自动重启 Pod: 
# 在 kubelet 运行的节点上执行以下命令
# 把 ID 换为你的容器的 ID
crictl stop 129fd7d382018
sleep 20
crictl ps

CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
89db4553e1eeb   docker.io/library/nginx@sha256:...    19 seconds ago    Running    web     1          34533c6729106

一旦你找到合适的容器,你就可以使用 crictl 获取该容器的日志.
# 在容器运行所在的节点上执行以下命令
crictl logs <container_id>
10.240.0.48 - - [16/Nov/2022:12:45:49 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
10.240.0.48 - - [16/Nov/2022:12:45:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"
10.240.0.48 - - [16/Nove/2022:12:45:51 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.47.0" "-"

若要找到如何使用 crictl 进行调试的更多信息,请访问使用 crictl 对 Kubernetes 节点进行调试.

* 动态增加和删除静态 Pod
运行中的 kubelet 会定期扫描配置的目录(比如例子中的 /etc/kubernetes/manifests 目录)中的变化,并且根据文件中出现/消失的 Pod 来添加/删除 Pod.

# 这里假定你在用主机文件系统上的静态 Pod 配置文件
# 在容器运行所在的节点上执行以下命令
mv /etc/kubernetes/manifests/static-web.yaml /tmp
sleep 20

crictl ps
# 可以看到没有 nginx 容器在运行
mv /tmp/static-web.yaml  /etc/kubernetes/manifests/
sleep 20

crictl ps
CONTAINER       IMAGE                                 CREATED           STATE      NAME    ATTEMPT    POD ID
f427638871c35   docker.io/library/nginx@sha256:...    19 seconds ago    Running    web     1          34533c6729106

* 附加内容 - 重要: 使用 kubeadm 管理的控制平面及 etcd 的静态 pod 生成详细信息,详细的官方参考很重要
https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/implementation-details/#save-the-kubeadm-clusterconfiguration-in-a-configmap-for-later-reference

https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/implementation-details/#generate-static-pod-manifests-for-control-plane-components

https://kubernetes.io/zh-cn/docs/reference/setup-tools/kubeadm/implementation-details/#generate-static-pod-manifest-for-local-etcd

## 将 Docker Compose 文件转换为 Kubernetes 资源 - kompose 工具用于将 docker compose 的配置内容转换成 k8s 中的 svc,deployment 等等配置文件
Kompose 是什么？它是一个转换工具，可将 Compose （即 Docker Compose）所组装的所有内容转换成容器编排器（Kubernetes 或 OpenShift）可识别的形式。

* 安装 Kompose
我们有很多种方式安装 Kompose。首选方式是从最新的 GitHub 发布页面下载二进制文件。

# Linux
curl -L https://github.com/kubernetes/kompose/releases/download/v1.34.0/kompose-linux-amd64 -o kompose

chmod +x kompose
sudo mv ./kompose /usr/local/bin/kompose

或者，你可以下载 tar 包。

* 使用 Kompose
只需几步，我们就把你从 Docker Compose 带到 Kubernetes。 你只需要一个现有的 docker-compose.yml 文件。

1. 进入 docker-compose.yml 文件所在的目录。如果没有，请使用下面这个进行测试。
--------------------------------------------
services:

  redis-leader:
    container_name: redis-leader
    image: redis
    ports:
      - "6379"

  redis-replica:
    container_name: redis-replica
    image: redis
    ports:
      - "6379"
    command: redis-server --replicaof redis-leader 6379 --dir /tmp

  web:
    container_name: web
    image: quay.io/kompose/web
    ports:
      - "8080:8080"
    environment:
      - GET_HOSTS_FROM=dns
    labels:
      kompose.service.type: LoadBalancer
------------------------------------------------------

2. 要将 docker-compose.yml 转换为 kubectl 可用的文件，请运行 kompose convert 命令进行转换，然后运行 kubectl apply -f <output file> 进行创建。
$ kompose convert
                           
输出类似于：
INFO Kubernetes file "redis-leader-service.yaml" created
INFO Kubernetes file "redis-replica-service.yaml" created
INFO Kubernetes file "web-tcp-service.yaml" created
INFO Kubernetes file "redis-leader-deployment.yaml" created
INFO Kubernetes file "redis-replica-deployment.yaml" created
INFO Kubernetes file "web-deployment.yaml" created

$ kubectl apply -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml

输出类似于：
deployment.apps/redis-leader created
deployment.apps/redis-replica created
deployment.apps/web created
service/redis-leader created
service/redis-replica created
service/web-tcp created

你部署的应用在 Kubernetes 中运行起来了。

3. 访问你的应用。
如果你在开发过程中使用 minikube，请执行：
$ minikube service web-tcp

否则，我们要查看一下你的服务使用了什么 IP！
$ kubectl describe svc web-tcp
Name:                     web-tcp
Namespace:                default
Labels:                   io.kompose.service=web-tcp
Annotations:              kompose.cmd: kompose convert
                          kompose.service.type: LoadBalancer
                          kompose.version: 1.33.0 (3ce457399)
Selector:                 io.kompose.service=web
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.102.30.3
IPs:                      10.102.30.3
Port:                     8080  8080/TCP
TargetPort:               8080/TCP
NodePort:                 8080  31624/TCP
Endpoints:                10.244.0.5:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>

如果你使用的是云驱动，你的 IP 将在 LoadBalancer Ingress 字段给出。
$ curl http://192.0.2.89

4. 清理。
你完成示例应用 Deployment 的测试之后，只需在 Shell 中运行以下命令，就能删除用过的资源。
$ kubectl delete -f web-tcp-service.yaml,redis-leader-service.yaml,redis-replica-service.yaml,web-deployment.yaml,redis-leader-deployment.yaml,redis-replica-deployment.yaml


* 用户指南 - 重要
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/translate-compose-kubernetes/#user-guide

## 通过配置内置准入控制器实施 Pod 安全标准 - 和下一章配合使用
Kubernetes 提供一种内置的准入控制器 用来强制实施 Pod 安全性标准。 你可以配置此准入控制器来设置集群范围的默认值和豁免选项。

Pod 安全性准入（Pod Security Admission）在 Kubernetes v1.22 作为 Alpha 特性发布， 在 Kubernetes v1.23 中作为 Beta 特性默认可用。从 1.25 版本起， 此特性进阶至正式发布（Generally Available）。

* 配置准入控制器 
说明：
pod-security.admission.config.k8s.io/v1 配置需要 v1.25+。 对于 v1.23 和 v1.24，使用 v1beta1。 对于 v1.22，使用 v1alpha1。

如果要开启,必须在启动 kube-apiserver 的参数中添加 --admission-control-config-file 

详细参考:
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/enforce-standards-admission-controller/

## 使用名字空间标签来实施 Pod 安全性标准 - 和上一章配合使用
名字空间可以打上标签以强制执行 Pod 安全性标准。 特权（privileged）、 基线（baseline）和 受限（restricted） 这三种策略涵盖了广泛安全范围，并由 Pod 安全准入控制器实现。

详细参考:
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/

Pod 安全性标准
参考:
https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-standards/

## 从 PodSecurityPolicy 迁移到内置的 PodSecurity 准入控制器 - 依赖于前 2 章的内容
详细参考:
https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/migrate-from-psp/

