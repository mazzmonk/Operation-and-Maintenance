# Taints 污点和 Tolerations 容忍概述
官方参考: https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/

节点和 Pod 亲和力,是将 Pod 吸引到一组节点【根据拓扑域】(作为优选或硬性要求).
污点(Taints)则相反,它们允许一个节点排斥一组 Pod.
容忍(Tolerations)应用于 pod,允许(但不强制要求) pod 调度到具有匹配污点的节点上.

污点(Taints)和容忍(Tolerations)共同作用,确保 pods 不会被调度到不适当的节点.一个或多个污点应用于节点;这标志着该节点不应该接受任何不容忍污点的 Pod.

说明：我们在平常使用中发现 pod 不会调度到 k8s 的 master 节点,就是因为 master 节点存在污点.


### Taints 污点
# Taints 污点的组成
使用 kubectl taint 命令可以给某个 Node 节点设置污点,Node 被设置污点之后就和 Pod 之间存在一种相斥的关系,可以让 Node 拒绝 Pod 的调度执行,甚至将 Node 上已经存在的 Pod 驱逐出去.

每个污点的组成如下：
  key=value:effect

每个污点有一个 key 和 value 作为污点的标签,effect 描述污点的作用.当前 taint effect 支持如下选项：
  * NoSchedule：除非具有匹配的容忍度规约,否则新的 Pod 不会被调度到带有污点的节点上.当前正在节点上运行的 Pod 不会被驱逐.
  * PreferNoSchedule：是“偏好”或“软性”的 NoSchedule.控制平面将尝试避免将不能容忍污点的 Pod 调度到的节点上,但不能保证完全避免.
  * NoExecute：这会影响已在节点上运行的 Pod,具体影响如下：
    1.如果 pod 不能容忍 effect 值为 NoExecute 的 taint,那么 pod 将马上被驱逐
    2.如果 pod 能够容忍 effect 值为 NoExecute 的 taint,且在 toleration 定义中没有指定 tolerationSeconds,则 pod 会一直在这个
                    节点上运行.
    3.如果 pod 能够容忍 effect 值为 NoExecute 的 taint,但是在 toleration 定义中指定了 tolerationSeconds,则表示  pod 还能在这
                    个节点上继续运行的时间长度.
 
# 污点(Taints)添加
$ kubectl taint nodes k8s01 check=zhang:NoSchedule
node/k8s01 tainted

$ kubectl describe node k8s01
...
Taints: check=zhang:NoSchedule   ### 可见已添加污点

在 k8s01 节点添加了一个污点(taint),污点的 key 为 check,value 为 zhang,污点 effect 为 NoSchedule.这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 k8s01 这个节点.

# 污点(Taints)删除
$ kubectl taint nodes k8s01 zhang:NoSchedule-

# 或者
$ kubectl taint nodes k8s01 check=zhang:NoSchedule-
node/k8s01 untainted

$ kubectl describe node k8s01
Taints: <none>   ### 可见已删除污点


### Tolerations容忍
设置了污点的 Node 将根据 taint 的 effect：NoSchedule、PreferNoSchedule、NoExecute 和 Pod 之间产生互斥的关系,Pod 将在一定程度上不会被调度到 Node 上.

但我们可以在 Pod 上设置容忍(Tolerations),意思是设置了容忍的 Pod 将可以容忍污点的存在,可以被调度到存在污点的 Node 上.

pod.spec.tolerations 示例

tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
---
tolerations:
- key: "key"
  operator: "Exists"
  effect: "NoSchedule"
---
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoExecute"
  tolerationSeconds: 3600

重要说明：
  * 其中 key、value、effect 要与 Node 上设置的 taint 保持一致
  * operator 的值为 Exists 时,将会忽略 value;只要有 key 和 effect 就行
  * tolerationSeconds：表示 pod 能够容忍 effect 值为 NoExecute 的 taint;当指定了 tolerationSeconds【容忍时间】,则表示 pod 还能在这个节点上继续运行的时间长度.


# 你可以在 Pod 规约中为 Pod 设置容忍度.
下面两个容忍度均与上面例子中使用 kubectl taint 命令创建的污点相匹配,因此如果一个 Pod 拥有其中的任何一个容忍度,都能够被调度到 k8s01：

tolerations:
- key: "check"
  operator: "Equal"
  value: "zhang"
  effect: "NoSchedule"


tolerations:
- key: "check"
  operator: "Exists"
  effect: "NoSchedule"

默认的 Kubernetes 调度器在选择一个节点来运行特定的 Pod 时会考虑污点和容忍度.然而,如果你手动为一个 Pod 指定了 .spec.nodeName,那么选节点操作会绕过调度器； 这个 Pod 将会绑定到你指定的节点上,即使你选择的节点上有 NoSchedule 的污点.如果这种情况发生,且节点上还设置了 NoExecute 的污点,kubelet 会将 Pod 驱逐出去,除非有适当的容忍度设置.


下面是一个定义了一些容忍度的 Pod 的例子：
-------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"

operator 的默认值是 Equal.
一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果,并且：
  * 如果 operator 是 Exists(此时容忍度不能指定 value),或者
  * 如果 operator 是 Equal,则它们的值应该相等.

说明：
存在两种特殊情况：
  1.如果一个容忍度的 key 为空且 operator 为 Exists,表示这个容忍度与任意的 key、value 和 effect 都匹配,即这
              个容忍度能容忍任何污点.
  2.如果 effect 为空,则可以与所有键名 key1 的效果相匹配.

-------------------------------------------


### 多个 Taints 污点和多个 Tolerations 容忍怎么判断
可以在同一个 node 节点上设置多个污点(Taints),在同一个 pod 上设置多个容忍(Tolerations).Kubernetes 处理多个污点和容忍的方式就像一个过滤器：从节点的所有污点开始,然后忽略可以被Pod容忍匹配的污点;保留其余不可忽略的污点,污点的 effect 对 Pod 具有显示效果：特别是：
  * 如果有至少一个不可忽略污点,effect 为 NoSchedule,那么 Kubernetes 将不调度 Pod 到该节点
  * 如果没有 effect 为 NoSchedule 的不可忽视污点,但有至少一个不可忽视污点,effect 为PreferNoSchedule,那么 Kubernetes 将尽量不调度 Pod 到该节点
  * 如果有至少一个不可忽视污点,effect 为 NoExecute,那么 Pod 将被从该节点驱逐(如果Pod已经在该节点运行),并且不会被调度到该节点(如果Pod还未在该节点运行)

# 当有多个 Master 存在时
当有多个 Master 存在时,为了防止资源浪费,可以进行如下设置：
$ kubectl taint nodes k8s01 node-role.kubernetes.io/master:PreferNoSchedule-


### 基于污点的驱逐

当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：
  * node.kubernetes.io/not-ready：节点未准备好。这相当于节点状况 Ready 的值为 "False"。
  * node.kubernetes.io/unreachable：节点控制器访问不到节点. 这相当于节点状况 Ready 的值为 "Unknown"。
  * node.kubernetes.io/memory-pressure：节点存在内存压力。
  * node.kubernetes.io/disk-pressure：节点存在磁盘压力。  # 比如磁盘空间不足
  * node.kubernetes.io/pid-pressure：节点的 PID 压力。
  * node.kubernetes.io/network-unavailable：节点网络不可用。
  * node.kubernetes.io/unschedulable：节点不可调度。
  * node.cloudprovider.kubernetes.io/uninitialized：如果 kubelet 启动时指定了一个“外部”云平台驱动， 它将给当前节点添加一个污
           点将其标志为不可用。在 cloud-controller-manager 的一个控制器初始化这个节点后，kubelet 将删除这个污点。


