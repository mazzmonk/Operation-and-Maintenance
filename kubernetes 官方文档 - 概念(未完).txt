索引
----------------------------------------------------------------------------------------------------------------

# 概述
## kubernetes 对象
* 理解 Kubernetes 对象
** 对象规约(Spec)与状态(Status)
** 必需字段 
### 名字空间
* 并非所有对象都在名字空间中
### 标签和选择算符
** 基于等值的 需求
** 基于集合 的需求
* API
** LIST 和 WATCH 过滤
** 支持基于集合需求的资源
### 注解
### Finalizers
^^^^^^^^^^^^^^^^^^^
## 在这个博客中介绍了使用终结器控制删除,以及强制删除等等的影响
## 强制删除命名空间
vvvvvvvvvvvvvvvvvvv
### 字段选择器
### 推荐使用的标签

------------------
# Kubernetes 架构
## 节点(node)
  ** 向 API 服务器添加节点的方式主要有两种:
  ** 容量与可分配
  ** Pod 优先级和抢占
  ** 心跳 
  ** 交换内存管理

## 控制面节点和 Kubernetes 集群之间的通信路径.
  ** 节点到控制面 --- node -> api server
^^^^^^^^^^^^^^^^^^^^
  ** TLS 启动引导
  ** 初始化过程 
  ** 启动引导初始化 
  ** 配置 
  ** 证书机构 
  ** kube-apiserver 配置 
    * 识别客户证书 
    * 初始启动引导认证 
    * 授权 kubelet 创建 CSR 
  ** kube-controller-manager 配置 
    * 访问密钥和证书 
    * 批复 
  ** kubelet 配置
  ** 客户和服务证书 
  ** 证书轮换 
  ** 其它身份认证组件 
  ** kubectl 批复 
vvvvvvvvvvvvvvvvvvvv
  ** 控制面到节点 --- api server -> node
    * API 服务器到 kubelet
^^^^^^^^^^^^^^^^^^^^
## Kubelet 认证/鉴权
  ** Kubelet 身份认证
  ** Kubelet 鉴权
vvvvvvvvvvvvvvvvvvvv
    * apiserver 到节点、Pod 和服务

## 控制器
  ** 控制器模式
    * 通过 API 服务器来控制
    * 直接控制
  ** 期望状态与当前状态
  ** 设计
  ** 运行控制器的方式

## Kubernetes 组件
^^^^^^^^^^^^^^^^^^^^
  ** 控制平面组件(Control Plane Components)
    * kube-apiserver  
    * kube-scheduler
    * kube-controller-manager
  ** Node 组件 
    * kubelet
    * kube-proxy
    * 容器运行时(Container Runtime) 
vvvvvvvvvvvvvvvvvvvv

## 垃圾收集
  ** 属主与依赖 
  ** 级联删除 
    * 前台级联删除
    * 后台级联删除
    * 被遗弃的依赖对象 
  ** 未使用容器和镜像的垃圾收集 
    * 容器镜像生命期 
    * 容器垃圾收集 
  ** 配置垃圾收集 

------------------
# 容器
## 镜像
** 镜像拉取策略 
** 默认镜像拉取策略 
** 必要的镜像拉取 
** ImagePullBackOff
* 带镜像索引的多架构镜像

------------------
# 工作负载
## Pod 
  ** Pod 模版
  ** Pod 更新与替换 
  ** 容器探针 
### Pod 的生命周期
  ** Pod 阶段 
  ** 容器状态 
  ** 容器重启策略
  ** 容器探针 
    * 检查机制 
    * 探测结果 
    *探测类型 
  ** 强制终止 Pod 
  ** 失效 Pod 的垃圾收集 
^^^^^^^^^^^^^^^^^^^^
## Pod 拓扑分布约束
vvvvvvvvvvvvvvvvvvvv
### 干扰(Disruptions)

## 工作负载管理
### Deployments
  ** 用例
  ** 创建 Deployment 
  ** 更新 Deployment 
    * 更改标签选择算符
  ** 回滚 Deployment
    * 检查 Deployment 上线历史
    * 回滚到之前的修订版本 
  ** 缩放 Deployment 
    * 比例缩放 
    * 比例缩放例子
  ** 暂停、恢复 Deployment 
    * 例如,对于一个刚刚创建的 Deployment:获取 Deployment 信息:
    * 使用如下指令暂停运行:
    * 接下来更新 Deployment 镜像:
    * 注意没有新的上线被触发:
    * 获取上线状态确保 Deployment 更新已经成功:
    * 你可以根据需要执行很多更新操作,例如,可以要使用的资源:
    * 最终,恢复 Deployment 执行并观察新的 ReplicaSet 的创建过程,其中包含了所应用的所有更新:
    * 观察上线的状态,直到完成.
    * 获取最近上线的状态:
  ** Deployment 状态 
    * 进行中的 Deployment 
    * 完成的 Deployment 
    * 失败的 Deployment 
    * 对失败 Deployment 的操作 
  ** 清理策略 
  ** 编写 Deployment 规约 
    * Pod 模板 
    * 选择算符 
    * 策略 
    * 重新创建 Deployment 
    * 滚动更新 Deployment 
    * 最大不可用 
    * 最大峰值 
    * 进度期限秒数 
    * 最短就绪时间 
    * 修订历史限制
    * paused(暂停的) 
### ReplicaSet
  ** 使用 ReplicaSets
    * 删除 ReplicaSet 和它的 Pod
    * 只删除 ReplicaSet
    * 将 Pod 从 ReplicaSet 中隔离
    * 缩放 RepliaSet
    * ReplicaSet 作为水平的 Pod 自动缩放器目标
  ** ReplicaSet 的替代方案## StatefulSets
### StatefulSets
  ** 使用 StatefulSets
  ** 限制 
  ** 组件 
  ** Pod 选择算符 
  ** Pod 标识 
    * 有序索引 
    * 稳定的网络 ID 
    * 稳定的存储 
    * Pod 名称标签 
  ** 部署和扩缩保证 
    * Pod 管理策略
  ** 更新策略 
  ** 滚动更新
    * 分区滚动更新 
    * 强制回滚
    * 最短就绪秒数 
### DaemonSet
  ** 编写 DaemonSet Spec 
    * 创建 DaemonSet 
    * 必需字段 
    * Pod 模板 
    * Pod 选择算符 
    * 仅在某些节点上运行 Pod 
  ** 与 Daemon Pods 通信 
  ** 更新 DaemonSet 
### Jobs
  ** 运行示例 Job 
  ** 编写 Job 规约
    * Pod 模版
    * Pod 选择算符 
    * Job 的并行执行
    * 完成模式 
  ** 处理 Pod 和容器失效
    * Pod 回退失效策略
  ** Job 终止与清理
  ** 自动清理完成的 Job 
    * 已完成 Job 的 TTL 机制 
  ** Job 模式 
  ** 高级用法 
    * 挂起 Job 
    * 指定你自己的 Pod 选择算符
    * 使用 Finalizer 追踪 Job 
### CronJob
  ** CronJob
    * 示例
    * Cron 时间表语法
  ** CronJob 限制 
  ** 控制器版本 
### ReplicationController
  ** ReplicationController 的替代方案

-------------------
# 服务、负载均衡和联网
* Kubernetes 网络模型 
## 服务
  ** 动机
  ** Service 资源
    * 云原生服务发现 
  ** 定义 Service
    * 没有选择算符的 Service 
    * 超出容量的 Endpoints 
    * EndpointSlices
    * 应用协议 
  ** 虚拟 IP 和 Service 代理
    * 为什么不使用 DNS 轮询?
    * userspace 代理模式
    * iptables 代理模式
    * IPVS 代理模式   --- 说明: 这种模式,本质是在服务器上启动了 LVS ,每一个 services 都会启动一个对应于 LVS 中的 VIP,pod 的 IP 则为 LVS 中的后端节点 IP 
  ** 多端口 Service 
  ** 选择自己的 IP 地址
  ** 流量策略 
    * 外部流量策略 
    * 内部流量策略 
  ** 服务发现 
    * 环境变量 
    * DNS
  ** 无头服务(Headless Services) 
    * 带选择算符的服务
    * 无选择算符的服务 
  ** 发布服务(服务类型) 
    * NodePort 类型 
    * LoadBalancer 类型 
    * 禁用负载均衡器节点端口分配 
    * AWS TLS 支持
    * ExternalName 类型 
    * 外部 IP 
  ** 不足之处
  ** 虚拟IP实施

## Service 与 Pod 的 DNS
  ** 介绍
    * Service 的名字空间
    * DNS 记录 
    * 服务 
  ** Pods
    * A/AAAA 记录
    * Pod 的 hostname 和 subdomain 字段
    * Pod 的 setHostnameAsFQDN 字段 
    * Pod 的 DNS 策略 
    * Pod 的 DNS 配置 
    * 功能的可用性

^^^^^^^^^^^^^^^^^^^^
## Kubernetes 连接容器的模型 
  ** 在集群中暴露 Pod
  ** 创建 Service
  ** 访问 Service
    * 环境变量
    * DNS
  ** 保护 Service
  ** 暴露 Service
vvvvvvvvvvvvvvvvvvvv

## Ingress
  ** 术语
  ** Ingress 是什么?
  ** 环境准备
  ** Ingress 资源 - ingress 定义的是一组规则,即转发规则,后续的 ingress controller 则是具体的程序,比如 nginx, 两者就像在 nginx 中定义的转发规则,和 nginx 这个软件本身,二者是分开的
    * Ingress 规则 
    * 默认后端 
    * 资源后端 
    * 路径类型 
    * 示例
  ** 主机名通配符 
  ** Ingress 类 - 不同类型的 Ingress Controller 对应的 Ingress 配置通常也是不同的,当集群中存在多于一个的 Ingress Controller 时,就需要为 Ingress 指定对应的 Controller 是哪个.
    * IngressClass 的作用域
    * 废弃的注解 
    * 默认 Ingress 类 
  ** Ingress 类型 
    * 由单个 Service 来完成的 Ingress 
    * 简单扇出 
    * 基于名称的虚拟托管 
    * TLS
    * 负载均衡 
  ** 更新 Ingress 

## Ingress 控制器
  ** 其他控制器 
  ** 使用多个 Ingress 控制器

## 拓扑感知提示
  ** 动机
  ** 使用拓扑感知提示
  ** 工作原理
    * EndpointSlice 控制器
    * kube-proxy
  ** 保护措施
  ** 限制

## 服务内部流量策略
  ** 使用服务内部流量策略
  ** 工作原理
  ** 限制

## EndpointSlices(端点切片) --- 未知工作场景,及原理
  ** 动机 
  ** Endpoint Slice 资源
  ** 地址类型
    * 状况
    * 拓扑信息 
    * 管理 
    * 属主关系 
    * EndpointSlice 镜像 
    * EndpointSlices 的分布问题 
    * 重复的端点 

## 网络策略
  ** 前置条件 
  ** 隔离和非隔离的 Pod 
  ** NetworkPolicy 资源 
  ** 选择器 to 和 from 的行为 
  ** 默认策略 
    * 默认拒绝所有入站流量
    * 默认允许所有入站流量
    * 默认拒绝所有出站流量 
    * 默认允许所有出站流量
    * 默认拒绝所有入口和所有出站流量
  **SCTP 支持 
  ** 针对某个端口范围 
  ** 基于名字指向某名字空间 
  ** 通过网络策略(至少目前还)无法完成的工作

## IPv4/IPv6 双协议栈

-------------------
# 存储
## 卷
  ** 背景 
  ** 卷类型 
    * awsElasticBlockStore
    * azureDisk
    * azureFile
    * cephfs
    * cinder
    * configMap
    * downwardAPI
    * emptyDir
    * fc (光纤通道)
    * flocker (已弃用)
    * gcePersistentDisk
    * gitRepo (已弃用) 
    * glusterfs
    * hostPath
    * iscsi
    * local
    * apiVersion: v1
    * nfs
    * persistentVolumeClaim
    * portworxVolume
    * projected (投射) 
    * quobyte (已弃用)
    * rbd
    * secret
    * storageOS (已弃用)
    * vsphereVolume
  ** 使用 subPath 
    * 使用带有扩展环境变量的 subPath 
  ** 资源 
  ** 树外(Out-of-Tree)卷插件
    * CSI
    * flexVolume 
  ** 挂载卷的传播 
    * 配置 

## 持久卷
  ** 介绍 
  ** 卷和申领的生命周期 
    * 供应 
    * 绑定 
    * 使用 
    * 保护使用中的存储对象 
    * 回收 
    * 预留 PersistentVolume 
    * 扩充 PVC 申领 
  ** 持久卷的类型 
  ** 持久卷 
    * 容量 
    * 卷模式 
    * 访问模式 
    * 类 
    *回收策略 
    * 挂载选项 
    * 节点亲和性 
    * 阶段 
  ** PersistentVolumeClaims 
    * 访问模式
    * 卷模式
    * 资源 
    * 选择算符 
    * 类 
  ** 使用申领作为卷 
    * 关于名字空间的说明 
    * 类型为 hostpath 的 PersistentVolume 
  ** 原始块卷支持 
    * 使用原始块卷的持久卷 
    * 申请原始块卷的 PVC 申领 
  ** 卷填充器(Populator)与数据源 
  ** 数据源引用 
  ** 使用卷填充器 --- 未知如何使用
    * 在容器中添加原始块设备路径的 Pod 规约
    * 绑定块卷 
  ** 对卷快照及从卷快照中恢复卷的支持 
    * 基于卷快照创建 PVC 申领
  ** 卷克隆 
    * 基于现有 PVC 创建新的 PVC 申领 
  ** 编写可移植的配置 
  
## 投射卷
  ** 介绍 
    * 带有 Secret、DownwardAPI 和 ConfigMap 的配置示例
    * 带有非默认权限模式设置的 Secret 的配置示例
  ** serviceAccountToken 投射卷 
  ** 与 SecurityContext 间的关系 

## 临时卷
  ** 临时卷的类型
  ** CSI 临时卷
  ** CSI 驱动程序限制
  ** 通用临时卷 
  ** 生命周期和 PersistentVolumeClaim
  ** PersistentVolumeClaim 的命名
  ** 安全

## 存储类
  ** 介绍
  ** StorageClass 资源
    * 存储制备器 
    * 回收策略
    * 允许卷扩展
    * 挂载选项
    * 卷绑定模式 
    * 允许的拓扑结构 
  ** 参数
    * AWS EBS 
    * GCE PD
    * Glusterfs
    * NFS
    * OpenStack Cinder
    * vSphere
    * Ceph RBD
    * Quobyte
    * Azure 磁盘
    * Azure 文件
    * Portworx 卷
    * ScaleIO
    * StorageOS
    * 本地

## 动态卷供应
  ** 背景
  ** 启用动态卷供应 
  ** 使用动态卷供应
  ** 设置默认值的行为
  ** 拓扑感知

## 卷快照
  ** 介绍
  ** 卷快照和卷快照内容的生命周期 
    * 供应卷快照
    * 绑定
    * 快照源的持久性卷声明保护

## 卷快照类
  ** 介绍
  ** VolumeSnapshotClass 资源 
    * 驱动程序 
    * 删除策略

## CSI 卷克隆

## 存储容量
  ** API
  ** 调度
  ** 重新调度
  ** 限制
  ** 开启存储容量跟踪

## 卷健康监测
## 特定于节点的卷数限制

-------------------
# 配置
## 配置最佳实践
  ** 一般配置提示 
  ** "Naked" Pods 与 ReplicaSet,Deployment 和 Jobs
  ** 服务 
  ** 使用标签 
  ** 使用 kubectl 

## ConfigMap
  ** 动机 
  ** ConfigMap 对象
  ** ConfigMaps 和 Pods
  ** 使用 ConfigMap 
    * 在 Pod 中将 ConfigMap 当做文件使用
  ** 不可变更的 ConfigMap 
 
## Secret
  ** Secret 的使用
    * Secret 的替代方案 
  ** 使用 Secret 
    * 创建 Secret 
    * 编辑 Secret 
    * 使用 Secret 
    * 在 Pod 中以文件形式使用 Secret 
    * 以环境变量的方式使用 Secret 
    * 容器镜像拉取 Secret 
  ** 使用场景 
    * 使用场景:作为容器环境变量
    * 使用场景:带 SSH 密钥的 Pod
    * 使用场景:带有生产、测试环境凭据的 Pod 
    * 使用场景:在 Secret 卷中带句点的文件 
    * 使用场景:仅对 Pod 中一个容器可见的 Secret
  ** Secret 的类型 
    * Opaque Secret
    * 服务账号令牌 Secret 
    * Docker 配置 Secret 
    * SSH 身份认证 Secret
    * TLS Secret
    * 启动引导令牌 Secret 
  ** 不可更改的 Secret
    * 将 Secret 标记为不可更改 
  ** Secret 的信息安全问题
    * 针对开发人员的安全性建议
    * 针对集群管理员的安全性建议

## 为 Pod 和容器管理资源
  ** 请求和约束 
  ** 资源类型 
  ** Pod 和 容器的资源请求和约束
  ** Kubernetes 中的资源单位 
    * CPU 资源单位 
  ** 内存资源单位 
  ** 容器资源示例 
  ** 带资源请求的 Pod 如何调度
  ** Kubernetes 应用资源请求与约束的方式
  ** 监控计算和内存资源用量 
  ** 本地临时存储 
    * 本地临时性存储的配置 
    * 为本地临时性存储设置请求和约束值
    * 带临时性存储的 Pods 的调度行为
    * 临时性存储消耗的管理
  ** 扩展资源(Extended Resources) 
  ** PID 限制 
  ** 疑难解答
    * 我的 Pod 处于悬决状态且事件信息显示 FailedScheduling
    * 我的容器被终止了

## 使用 kubeconfig 文件组织集群访问
  ** 支持多集群、用户和身份认证机制
  ** 上下文(Context)
  ** KUBECONFIG 环境变量
  ** 合并 kubeconfig 文件
  ** 文件引用
  ** 代理

-------------------
# 安全
## 云原生安全概述
  ** 云原生安全的 4C
  ** 云
    * 云提供商安全性
    * 基础设施安全
  ** 集群
    * 集群组件
    * 集群中的组件(您的应用) --- 重要

^^^^^^^^^^^^^^^^^^^
## 使用 RBAC 鉴权
  ** API 对象 
    * Role 和 ClusterRole 
    * ClusterRole 示例
    * RoleBinding 和 ClusterRoleBinding 
    * 对资源的引用 
    * 聚合的 ClusterRole 
    * 对主体的引用 
  ** 默认 Roles 和 Role Bindings
    * 自动协商 
    * API 发现角色 
    * 面向用户的角色 
    * 核心组件角色 
    * 其他组件角色 
    * 内置控制器的角色 
  ** 初始化与预防权限提升
    * 对角色创建或更新的限制
    * 对角色绑定创建或更新的限制
  ** 一些命令行工具
    * kubectl create role
    * kubectl create clusterrole
    * kubectl create rolebinding
    * kubectl create clusterrolebinding
    * kubectl auth reconcile
  ** 服务账户权限 
  ** Endpoints 写权限
  ** 从 ABAC 升级
    * 并行鉴权 
    * 宽松的 RBAC 权限 
vvvvvvvvvvvvvvvvvvv

## Kubernetes API 访问控制
  ** 传输安全
  ** 认证
  ** 鉴权
  ** 准入控制
  ** 审计
  ** API 服务器端口和 IP

^^^^^^^^^^^^^^^^^^^
## 静态加密 Secret 数据
  ** Before you begin
  ** 配置并确定是否已启用静态数据加密
  ** 理解静态数据加密
    * Providers:
    * 加密你的数据
    * 验证数据已被加密
    * 确保所有 Secret 都被加密
    * 轮换解密密钥
    * 解密所有数据
vvvvvvvvvvvvvvvvvvv

## Pod 安全性标准
## Pod 安全性准入

-------------------
# 策略
## 限制范围
  ** 启用 LimitRange
    * 限制范围总览

## 资源配额
  ** 启用资源配额 
  ** 计算资源配额
    * 扩展资源的资源配额
  ** 存储资源配额
  ** 对象数量配额
  ** 配额作用域 
    * 基于优先级类(PriorityClass)来设置资源配额
  ** 跨名字空间的 Pod 亲和性配额 
  ** 请求与限制的比较 
  ** 查看和设置配额
  ** 配额和集群容量 
  ** 默认情况下限制特定优先级的资源消耗

## 进程 ID 约束与预留
  ** 节点级别 PID 限制 
  ** Pod 级别 PID 限制 
  ** 基于 PID 的驱逐 

## 节点资源管理器

-------------------
# 调度、抢占和驱逐
## Kubernetes 调度器
  ** 调度概览
  ** kube-scheduler
  ** kube-scheduler 调度流程

## 将 Pod 指派给节点
  ** 节点标签 
  ** 节点隔离/限制 
  ** nodeSelector
  ** 亲和性与反亲和性 
    * 节点亲和性 
    * pod 间亲和性与反亲和性 
  ** nodeName

## Pod 开销
  ** 启用 Pod 开销
  ** 使用示例
  ** 验证 Pod cgroup 限制
    * 可观察性



特性门控
https://kubernetes.io/zh/docs/reference/command-line-tools-reference/feature-gates/
----------------------------------------------------------------------------------------------------------------

# 概述
## kubernetes 对象
* 理解 Kubernetes 对象
** 对象规约(Spec)与状态(Status)
几乎每个 Kubernetes 对象包含两个嵌套的对象字段,它们负责管理对象的配置:对象 spec(规约)和 对象 status(状态).对于具有 spec 的对象,你必须在创建对象时设置其内容,描述你希望对象所具有的特征:期望状态(Desired State).

status 描述了对象的 当前状态(Current State),它是由 Kubernetes 系统和组件 设置并更新的.在任何时刻,Kubernetes 控制平面 都一直积极地管理着对象的实际状态,以使之与期望状态相匹配.

例如,Kubernetes 中的 Deployment 对象能够表示运行在集群中的应用.当创建 Deployment 时,可能需要设置 Deployment 的 spec,以指定该应用需要有 3 个副本运行.Kubernetes 系统读取 Deployment 规约,并启动我们所期望的应用的 3 个实例 —— 更新状态以与规约相匹配.如果这些实例中有的失败了(一种状态变更),Kubernetes 系统通过执行修正操作 来响应规约和状态间的不一致 —— 在这里意味着它会启动一个新的实例来替换.

** 必需字段 
在想要创建的 Kubernetes 对象对应的 .yaml 文件中,需要配置如下的字段:

apiVersion - 创建该对象所使用的 Kubernetes API 的版本
kind - 想要创建的对象的类别
metadata - 帮助唯一性标识对象的一些数据,包括一个 name 字符串、UID 和可选的 namespace
spec - 你所期望的该对象的状态
对象 spec 的精确格式对每个 Kubernetes 对象来说是不同的,包含了特定于该对象的嵌套字段.Kubernetes API 参考 能够帮助我们找到任何我们想创建的对象的规约格式.

例如,Pod 参考文档详细说明了 API 中 Pod 的 spec 字段, Deployment 的参考文档则详细说明了 Deployment 的 spec 字段.在这些 API 参考页面中,你将看到提到的 PodSpec 和 DeploymentSpec.这些名字是 Kubernetes 用来实现其 API 的 Golang 代码的实现细节.

### 名字空间
* 并非所有对象都在名字空间中
大多数 kubernetes 资源(例如 Pod、Service、副本控制器等)都位于某些名字空间中.但是名字空间资源本身并不在名字空间中.而且底层资源,例如 节点 和持久化卷不属于任何名字空间.

查看哪些 Kubernetes 资源在名字空间中,哪些不在名字空间中:

# 位于名字空间中的资源
kubectl api-resources --namespaced=true

# 不在名字空间中的资源
kubectl api-resources --namespaced=false

### 标签和选择算符
** 基于等值的 需求
基于等值 或 基于不等值 的需求允许按标签键和值进行过滤.匹配对象必须满足所有指定的标签约束,尽管它们也可能具有其他标签.可接受的运算符有=、== 和 != 三种.前两个表示 相等(并且只是同义词),而后者表示 不相等.例如:

environment = production
tier != frontend

前者选择所有资源,其键名等于 environment,值等于 production.后者选择所有资源,其键名等于 tier,值不同于 frontend,所有资源都没有带有 tier 键的标签.可以使用逗号运算符来过滤 production 环境中的非 frontend 层资源:environment=production,tier!=frontend.


** 基于集合 的需求
基于集合 的标签需求允许你通过一组值来过滤键.支持三种操作符:in、notin 和 exists (只可以用在键标识符上).例如:

environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
第一个示例选择了所有键等于 environment 并且值等于 production 或者 qa 的资源.
第二个示例选择了所有键等于 tier 并且值不等于 frontend 或者 backend 的资源,以及所有没有 tier 键标签的资源.
第三个示例选择了所有包含了有 partition 标签的资源;没有校验它的值.
第四个示例选择了所有没有 partition 标签的资源;没有校验它的值.类似地,逗号分隔符充当 与 运算符.因此,使用 partition 键(无论为何值)和 environment 不同于 qa 来过滤资源可以使用 partition, environment notin(qa) 来实现.
基于集合 的标签选择算符是相等标签选择算符的一般形式,因为 environment=production 等同于 environment in(production);!= 和 notin 也是类似的.

基于集合 的要求可以与基于 相等 的要求混合使用.例如:partition in (customerA, customerB),environment!=qa.

\
* API
** LIST 和 WATCH 过滤
LIST 和 WATCH 操作可以使用查询参数指定标签选择算符过滤一组对象.两种需求都是允许的.(这里显示的是它们出现在 URL 查询字符串中)

基于等值 的需求: ?labelSelector=environment%3Dproduction,tier%3Dfrontend
基于集合 的需求: ?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29
两种标签选择算符都可以通过 REST 客户端用于 list 或者 watch 资源.例如,使用 kubectl 定位 apiserver,可以使用 基于等值 的标签选择算符可以这么写:

kubectl get pods -l environment=production,tier=frontend

或者使用 基于集合的 需求:

kubectl get pods -l 'environment in (production),tier in (frontend)'

正如刚才提到的,基于集合 的需求更具有表达力.例如,它们可以实现值的 或 操作:

kubectl get pods -l 'environment in (production, qa)'

或者通过 exists 运算符限制不匹配:

kubectl get pods -l 'environment,environment notin (frontend)'

** 支持基于集合需求的资源
比较新的资源,例如 Job、 Deployment、 Replica Set 和 DaemonSet ,也支持 基于集合的 需求.

selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}

matchLabels 是由 {key,value} 对组成的映射. matchLabels 映射中的单个 {key,value } 等同于 matchExpressions 的元素,其 key 字段为 "key",operator 为 "In",而 values 数组仅包含 "value". matchExpressions 是 Pod 选择算符需求的列表. 有效的运算符包括 In、NotIn、Exists 和 DoesNotExist. 在 In 和 NotIn 的情况下,设置的值必须是非空的. 来自 matchLabels 和 matchExpressions 的所有要求都按逻辑与的关系组合到一起 -- 它们必须都满足才能匹配.

### 注解
你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据.客户端程序(例如工具和库)能够获取这些元数据信息.

你可以使用标签或注解将元数据附加到 Kubernetes 对象.注解不用于标识和选择对象. 注解中的元数据,可以很小,也可以很大,可以是结构化的,也可以是非结构化的,能够包含标签不允许的字符.

注解和标签一样,是键/值对:

"metadata": {
  "annotations": {
    "key1" : "value1",
    "key2" : "value2"
  }
}

例如,下面是一个 Pod 的配置文件,其注解中包含 imageregistry: https://hub.docker.com/:

apiVersion: v1
kind: Pod
metadata:
  name: annotations-demo
  annotations:
    imageregistry: "https://hub.docker.com/"
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80


### Finalizers
Finalizer(终结器)是带有命名空间的键,告诉 Kubernetes 等到特定的条件被满足后, 再完全删除被标记为删除的资源.Finalizer 提醒控制器清理被删除的对象拥有的资源.

当你告诉 Kubernetes 删除一个指定了 Finalizer 的对象时, Kubernetes API 会将该对象标记为删除,使其进入只读状态.此时控制平面或其他组件会采取 Finalizer 所定义的行动, 而目标对象仍然处于终止中(Terminating)的状态.这些行动完成后,控制器会删除目标对象相关的 Finalizer.当 metadata.finalizers 字段为空时,Kubernetes 认为删除已完成.

你可以使用 Finalizer 控制资源的垃圾收集.例如,你可以定义一个 Finalizer,在删除目标资源前清理相关资源或基础设施.

你可以通过使用 Finalizers 提醒控制器 在删除目标资源前执行特定的清理任务, 来控制资源的垃圾收集.

Finalizers 通常不指定要执行的代码.相反,它们通常是特定资源上的键的列表,类似于注解.Kubernetes 自动指定了一些 Finalizers,但你也可以指定你自己的.

Finalizers 如何工作 
当你使用清单文件创建资源时,你可以在 metadata.finalizers 字段指定 Finalizers. 当你试图删除该资源时,管理该资源的控制器会注意到 finalizers 字段中的值,并进行以下操作:

修改对象,将你开始执行删除的时间添加到 metadata.deletionTimestamp 字段.
将该对象标记为只读,直到其 metadata.finalizers 字段为空.
然后,控制器试图满足资源的 Finalizers 的条件. 每当一个 Finalizer 的条件被满足时,控制器就会从资源的 finalizers 字段中删除该键. 当该字段为空时,垃圾收集继续进行. 你也可以使用 Finalizers 来阻止删除未被管理的资源.

一个常见的 Finalizer 的例子是 kubernetes.io/pv-protection,它用来防止意外删除 PersistentVolume 对象. 当一个 PersistentVolume 对象被 Pod 使用时,Kubernetes 会添加 pv-protection Finalizer. 如果你试图删除 PersistentVolume,它将进入 Terminating 状态,但是控制器因为该 Finalizer 存在而无法删除该资源. 当 Pod 停止使用 PersistentVolume 时,Kubernetes 清除 pv-protection Finalizer,控制器就会删除该卷.

^^^^^^^^^^^^^^^^^^^
## 在这个博客中介绍了使用终结器控制删除,以及强制删除等等的影响
https://kubernetes.io/blog/2021/05/14/using-finalizers-to-control-deletion/

## 强制删除命名空间
有一种情况可能需要强制完成命名空间.如果你删除了一个命名空间并且你已经清除了它下面的所有对象,但命名空间仍然存在,可以通过更新命名空间子资源来强制删除,finalize. 这会通知命名空间控制器它需要从命名空间中删除终结器并执行任何清理:

cat <<EOF | curl -X PUT \
  localhost:8080/api/v1/namespaces/test/finalize \
  -H "Content-Type: application/json" \
  --data-binary @-
{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "test"
  },
  "spec": {
    "finalizers": null
  }
}
EOF
应该谨慎执行此操作,因为它可能只删除命名空间并将孤立对象留在现在不退出的命名空间中——这对 Kubernetes 来说是一个令人困惑的状态.如果发生这种情况,可以手动重新创建命名空间,有时孤立对象将重新出现在刚刚创建的命名空间下,这将允许手动清理和恢复.

vvvvvvvvvvvvvvvvvvv

### 字段选择器
字段选择器(Field selectors)允许你根据一个或多个资源字段的值 筛选 Kubernetes 资源.
下面这个 kubectl 命令将筛选出 status.phase 字段值为 Running 的所有 Pod:

kubectl get pods --field-selector status.phase=Running
说明:
字段选择器本质上是资源过滤器(Filters).默认情况下,字段选择器/过滤器是未被应用的, 这意味着指定类型的所有资源都会被筛选出来.这使得以下的两个 kubectl 查询是等价的:

kubectl get pods
kubectl get pods --field-selector ""

不同的 Kubernetes 资源类型支持不同的字段选择器.所有资源类型都支持 metadata.name 和 metadata.namespace 字段.使用不被支持的字段选择器会产生错误.

支持的操作符 
你可在字段选择器中使用 =、==和 != (= 和 == 的意义是相同的)操作符.例如,下面这个 kubectl 命令将筛选所有不属于 default 命名空间的 Kubernetes 服务:

kubectl get services  --all-namespaces --field-selector metadata.namespace!=default

同标签和其他选择器一样, 字段选择器可以通过使用逗号分隔的列表组成一个选择链.下面这个 kubectl 命令将筛选 status.phase 字段不等于 Running 同时 spec.restartPolicy 字段等于 Always 的所有 Pod:

kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always

你能够跨多种资源类型来使用字段选择器.下面这个 kubectl 命令将筛选出所有不在 default 命名空间中的 StatefulSet 和 Service:

kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default

### 推荐使用的标签
除了 kubectl 和 dashboard 之外,您可以使用其他工具来可视化和管理 Kubernetes 对象.一组通用的标签可以让多个工具之间相互操作,用所有工具都能理解的通用方式描述对象.
为说明这些标签的实际使用情况,请看下面的 StatefulSet 对象:

# 这是一段节选
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: mysql-abcxzy
    app.kubernetes.io/version: "5.7.21"
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: wordpress
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/created-by: controller-manager

应用可以在 Kubernetes 集群中安装一次或多次.在某些情况下,可以安装在同一命名空间中.例如,可以不止一次地为不同的站点安装不同的 WordPress.

应用的名称和实例的名称是分别记录的.例如,WordPress 应用的 app.kubernetes.io/name 为 wordpress,而其实例名称 app.kubernetes.io/instance 为 wordpress-abcxzy. 这使得应用和应用的实例均可被识别,应用的每个实例都必须具有唯一的名称.

考虑一个稍微复杂的应用:一个使用 Helm 安装的 Web 应用(WordPress),其中 使用了数据库(MySQL).以下代码片段说明用于部署此应用程序的对象的开始.

以下 Deployment 的开头用于 WordPress:

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: wordpress
    app.kubernetes.io/instance: wordpress-abcxzy
    app.kubernetes.io/version: "4.9.4"
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: server
    app.kubernetes.io/part-of: wordpress
...
这个 Service 用于暴露 WordPress:

apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: wordpress
    app.kubernetes.io/instance: wordpress-abcxzy
    app.kubernetes.io/version: "4.9.4"
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: server
    app.kubernetes.io/part-of: wordpress
...
MySQL 作为一个 StatefulSet 暴露,包含它和它所属的较大应用程序的元数据:

apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: mysql-abcxzy
    app.kubernetes.io/version: "5.7.21"
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: wordpress
...
Service 用于将 MySQL 作为 WordPress 的一部分暴露:

apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/name: mysql
    app.kubernetes.io/instance: mysql-abcxzy
    app.kubernetes.io/version: "5.7.21"
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/component: database
    app.kubernetes.io/part-of: wordpress
...

------------------
# Kubernetes 架构
## 节点(node)
Kubernetes 通过将容器放入在节点(Node)上运行的 Pod 中来执行你的工作负载
节点上的组件包括 kubelet、 容器运行时以及 kube-proxy.


** 向 API 服务器添加节点的方式主要有两种:

1. 节点上的 kubelet 向控制面执行自注册;
2. 你,或者别的什么人,手动添加一个 Node 对象.

节点自注册
当 kubelet 标志 --register-node 为 true(默认)时,它会尝试向 API 服务注册自己. 这是首选模式,被绝大多数发行版选用.

对于自注册模式,kubelet 使用下列参数启动:

--kubeconfig - 用于向 API 服务器表明身份的凭据路径.
--cloud-provider - 与某云驱动 进行通信以读取与自身相关的元数据的方式.
--register-node - 自动向 API 服务注册.
--register-with-taints - 使用所给的污点列表(逗号分隔的 <key>=<value>:<effect>)注册节点. 当 register-node 为 false 时无效.
--node-ip - 节点 IP 地址.
--node-labels - 在集群中注册节点时要添加的 标签. (参见 NodeRestriction 准入控制插件所实施的标签限制).
--node-status-update-frequency - 指定 kubelet 向控制面发送状态的频率.
启用节点授权模式和 NodeRestriction 准入插件 时,仅授权 kubelet 创建或修改其自己的节点资源.

手动节点管理
你可以使用 kubectl 来创建和修改 Node 对象.

如果你希望手动创建节点对象时,请设置 kubelet 标志 --register-node=false.

你可以修改 Node 对象(忽略 --register-node 设置). 例如,修改节点上的标签或标记其为不可调度.

你可以结合使用节点上的标签和 Pod 上的选择算符来控制调度. 例如,你可以限制某 Pod 只能在符合要求的节点子集上运行.

如果标记节点为不可调度(unschedulable),将阻止新 Pod 调度到该节点之上,但不会 影响任何已经在其上的 Pod. 这是重启节点或者执行其他维护操作之前的一个有用的准备步骤.

要标记一个节点为不可调度,执行以下命令:

kubectl cordon $NODENAME

安全腾空节点
https://kubernetes.io/zh/docs/tasks/administer-cluster/safely-drain-node/


** 容量与可分配
描述节点上的可用资源:CPU、内存和可以调度到节点上的 Pod 的个数上限.

capacity 块中的字段标示节点拥有的资源总量. allocatable 块指示节点上可供普通 Pod 消耗的资源量.

预留计算资源
https://kubernetes.io/zh/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable

为系统守护进程预留计算资源
Kubernetes 的节点可以按照 Capacity 调度.默认情况下 pod 能够使用节点全部可用容量. 这是个问题,因为节点自己通常运行了不少驱动 OS 和 Kubernetes 的系统守护进程. 除非为这些系统守护进程留出资源,否则它们将与 pod 争夺资源并导致节点资源短缺问题.

kubelet 公开了一个名为 'Node Allocatable' 的特性,有助于为系统守护进程预留计算资源. Kubernetes 推荐集群管理员按照每个节点上的工作负载密度配置 Node Allocatable.

示例场景 
这是一个用于说明节点可分配(Node Allocatable)计算方式的示例:

节点拥有 32Gi memeory,16 CPU 和 100Gi Storage 资源
--kube-reserved 被设置为 cpu=1,memory=2Gi,ephemeral-storage=1Gi
--system-reserved 被设置为 cpu=500m,memory=1Gi,ephemeral-storage=1Gi
--eviction-hard 被设置为 memory.available<500Mi,nodefs.available<10%
在这个场景下,'Allocatable' 将会是 14.5 CPUs、28.5Gi 内存以及 88Gi 本地存储. 调度器保证这个节点上的所有 Pod 的内存 requests 总量不超过 28.5Gi, 存储不超过 '88Gi'. 当 Pod 的内存使用总量超过 28.5Gi 或者磁盘使用总量超过 88Gi 时, kubelet 将会驱逐它们. 如果节点上的所有进程都尽可能多地使用 CPU,则 Pod 加起来不能使用超过 14.5 CPUs 的资源.

当没有执行 kube-reserved 和/或 system-reserved 策略且系统守护进程 使用量超过其预留时,如果节点内存用量高于 31.5Gi 或存储大于 90Gi, kubelet 将会驱逐 Pod.

** Pod 优先级和抢占
https://kubernetes.io/zh/docs/concepts/scheduling-eviction/pod-priority-preemption/


** 心跳 
Kubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性,并在检测到故障时采取行动.

对于节点,有两种形式的心跳:

更新节点的 .status
Lease 对象 在 kube-node-lease 命名空间中. 每个节点都有一个关联的 Lease 对象.
与 Node 的 .status 更新相比,Lease 是一种轻量级资源. 使用 Leases 心跳在大型集群中可以减少这些更新对性能的影响.

kubelet 负责创建和更新节点的 .status,以及更新它们对应的 Lease.

当状态发生变化时,或者在配置的时间间隔内没有更新事件时,kubelet 会更新 .status. .status 更新的默认间隔为 5 分钟(比不可达节点的 40 秒默认超时时间长很多).
kubelet 会每 10 秒(默认更新间隔时间)创建并更新其 Lease 对象. Lease 更新独立于 NodeStatus 更新而发生. 如果 Lease 的更新操作失败,kubelet 会采用指数回退机制,从 200 毫秒开始 重试,最长重试间隔为 7 秒钟.


** 交换内存管理
FEATURE STATE: Kubernetes v1.22 [alpha]
在 Kubernetes 1.22 之前,节点不支持使用交换内存,并且 默认情况下,如果在节点上检测到交换内存配置,kubelet 将无法启动. 在 1.22 以后,可以在每个节点的基础上启用交换内存支持.

要在节点上启用交换内存,必须启用kubelet 的 NodeSwap 特性门控, 同时使用 --fail-swap-on 命令行参数或者将 failSwapOn 配置 设置为false.

用户还可以选择配置 memorySwap.swapBehavior 以指定节点使用交换内存的方式. 例如:

memorySwap:
  swapBehavior: LimitedSwap
已有的 swapBehavior 的配置选项有:

LimitedSwap:Kubernetes 工作负载的交换内存会受限制. 不受 Kubernetes 管理的节点上的工作负载仍然可以交换.
UnlimitedSwap:Kubernetes 工作负载可以使用尽可能多的交换内存 请求,一直到系统限制.
如果启用了特性门控但是未指定 memorySwap 的配置,默认情况下 kubelet 将使用 LimitedSwap 设置.

LimitedSwap 设置的行为还取决于节点运行的是 v1 还是 v2 的控制组(也就是 cgroups):

cgroupsv1: Kubernetes 工作负载可以使用内存和 交换,达到 pod 的内存限制(如果设置).
cgroupsv2: Kubernetes 工作负载不能使用交换内存.


## 控制面节点(确切说是 API 服务器)和 Kubernetes 集群之间的通信路径.

** 节点到控制面 --- node -> api server
Kubernetes 采用的是中心辐射型(Hub-and-Spoke)API 模式. 所有从集群(或所运行的 Pods)发出的 API 调用都终止于 apiserver. 其它控制面组件都没有被设计为可暴露远程服务. apiserver 被配置为在一个安全的 HTTPS 端口(通常为 443)上监听远程连接请求, 并启用一种或多种形式的客户端身份认证机制. 一种或多种客户端鉴权机制应该被启用, 特别是在允许使用匿名请求 或服务账号令牌的时候.

应该使用集群的公共根证书开通节点,这样它们就能够基于有效的客户端凭据安全地连接 apiserver. 一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet. 请查看 kubelet TLS 启动引导 以了解如何自动提供 kubelet 客户端证书.

TLS 启动引导
https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/

以下内容摘录之上述的链接
^^^^^^^^^^^^^^^^^^^^
重要
** TLS 启动引导
在一个 Kubernetes 集群中,工作节点上的组件(kubelet 和 kube-proxy)需要与 Kubernetes 主控组件通信,尤其是 kube-apiserver.为了确保通信本身是私密的、不被干扰,并且确保集群的每个组件都在与另一个 可信的组件通信,我们强烈建议使用节点上的客户端 TLS 证书.

启动引导这些组件的正常过程,尤其是需要证书来与 kube-apiserver 安全通信的 工作节点,可能会是一个具有挑战性的过程,因为这一过程通常不受 Kubernetes 控制, 需要不少额外工作.这也使得初始化或者扩缩一个集群的操作变得具有挑战性.

为了简化这一过程,从 1.4 版本开始,Kubernetes 引入了一个证书请求和签名 API 以便简化此过程.


** 初始化过程 
当工作节点启动时,kubelet 执行以下操作:

1. 寻找自己的 kubeconfig 文件
2. 检索 API 服务器的 URL 和凭据,通常是来自 kubeconfig 文件中的 TLS 密钥和已签名证书
3. 尝试使用这些凭据来与 API 服务器通信

假定 kube-apiserver 成功地认证了 kubelet 的凭据数据,它会将 kubelet 视为 一个合法的节点并开始将 Pods 分派给该节点.

注意,签名的过程依赖于:

  * kubeconfig 中包含密钥和本地主机的证书
  * 证书被 kube-apiserver 所信任的一个证书机构(CA)所签名

负责部署和管理集群的人有以下责任:

1. 创建 CA 密钥和证书
2. 将 CA 证书发布到 kube-apiserver 运行所在的主控节点上
3. 为每个 kubelet 创建密钥和证书;强烈建议为每个 kubelet 使用独一无二的、 CN 取值与众不同的密钥和证书
4. 使用 CA 密钥对 kubelet 证书签名
5. 将 kubelet 密钥和签名的证书发布到 kubelet 运行所在的特定节点上

本文中描述的 TLS 启动引导过程有意简化甚至完全自动化上述过程,尤其是 第三步之后的操作,因为这些步骤是初始化或者扩缩集群时最常见的操作.


** 启动引导初始化 
在启动引导初始化过程中,会发生以下事情:

1. kubelet 启动
2. kubelet 看到自己 没有 对应的 kubeconfig 文件
3. kubelet 搜索并发现 bootstrap-kubeconfig 文件
4. kubelet 读取该启动引导文件,从中获得 API 服务器的 URL 和用途有限的 一个"令牌(Token)"
5. kubelet 建立与 API 服务器的连接,使用上述令牌执行身份认证
6. kubelet 现在拥有受限制的凭据来创建和取回证书签名请求(CSR)
7. kubelet 为自己创建一个 CSR,并将其 signerName 设置为 kubernetes.io/kube-apiserver-client-kubelet
8 CSR 被以如下两种方式之一批复:
  *如果配置了,kube-controller-manager 会自动批复该 CSR
  *如果配置了,一个外部进程,或者是人,使用 Kubernetes API 或者使用 kubectl 来批复该 CSR
9. kubelet 所需要的证书被创建
10. 证书被发放给 kubelet
11. kubelet 取回该证书
12. kubelet 创建一个合适的 kubeconfig,其中包含密钥和已签名的证书
13. kubelet 开始正常操作
14. 可选地,如果配置了,kubelet 在证书接近于过期时自动请求更新证书
15. 更新的证书被批复并发放;取决于配置,这一过程可能是自动的或者手动完成


** 配置 
要配置 TLS 启动引导及可选的自动批复,你必须配置以下组件的选项:

  * kube-apiserver
  * kube-controller-manager
  * kubelet
  * 集群内的资源:ClusterRoleBinding 以及可能需要的 ClusterRole

此外,你需要有 Kubernetes 证书机构(Certificate Authority,CA).


** 证书机构 
就像在没有启动引导的情况下,你会需要证书机构(CA)密钥和证书.这些数据会被用来对 kubelet 证书进行签名.如前所述,将证书机构密钥和证书发布到主控节点是你的责任.

就本文而言,我们假定这些数据被发布到主控节点上的 /var/lib/kubernetes/ca.pem(证书)和 /var/lib/kubernetes/ca-key.pem(密钥)文件中.我们将这两个文件称作"Kubernetes CA 证书和密钥".所有 Kubernetes 组件(kubelet、kube-apiserver、kube-controller-manager)都使用 这些凭据,并假定这里的密钥和证书都是 PEM 编码的.


** kube-apiserver 配置 
启用 TLS 启动引导对 kube-apiserver 有若干需求:

  * 能够识别对客户端证书进行签名的 CA
  * 能够对启动引导的 kubelet 执行身份认证,并将其置入 system:bootstrappers 组
  * 能够对启动引导的 kubelet 执行鉴权操作,允许其创建证书签名请求(CSR)

* 识别客户证书 
对于所有客户端证书的认证操作而言,这是很常见的.如果还没有设置,要为 kube-apiserver 命令添加 --client-ca-file=FILENAME 标志来启用客户端证书认证,在标志中引用一个包含用来签名的证书的证书机构包, 例如:--client-ca-file=/var/lib/kubernetes/ca.pem.

* 初始启动引导认证 
为了让启动引导的 kubelet 能够连接到 kube-apiserver 并请求证书, 它必须首先在服务器上认证自身身份.你可以使用任何一种能够对 kubelet 执行身份认证的 身份认证组件.

尽管所有身份认证策略都可以用来对 kubelet 的初始启动凭据来执行认证, 出于容易准备的因素,建议使用如下两个身份认证组件:

  1. 启动引导令牌(Bootstrap Token)
  2. 令牌认证文件

启动引导令牌是一种对 kubelet 进行身份认证的方法,相对简单且容易管理, 且不需要在启动 kube-apiserver 时设置额外的标志.启动引导令牌从 Kubernetes 1.12 开始是一种 Beta 功能特性.

无论选择哪种方法,这里的需求是 kubelet 能够被身份认证为某个具有如下权限的用户:

  1. 创建和读取 CSR
  2. 在启用了自动批复时,能够在请求节点客户端证书时得到自动批复

使用启动引导令牌执行身份认证的 kubelet 会被认证为 system:bootstrappers 组中的用户.这是使用启动引导令牌的一种标准方法.

随着这个功能特性的逐渐成熟,你需要确保令牌绑定到某基于角色的的访问控制(RBAC) 策略上,从而严格限制请求(使用启动引导令牌) 仅限于客户端申请提供证书.当 RBAC 被配置启用时,可以将令牌限制到某个组,从而 提高灵活性.例如,你可以在准备节点期间禁止某特定启动引导组的访问.

 -* 启动引导令牌 
启动引导令牌的细节在这里 详述.启动引导令牌在 Kubernetes 集群中存储为 Secret 对象,被发放给各个 kubelet.你可以在整个集群中使用同一个令牌,也可以为每个节点发放单独的令牌.

这一过程有两个方面:

  1. 基于令牌 ID、机密数据和范畴信息创建 Kubernetes Secret
  2. 将令牌发放给 kubelet

从 kubelet 的角度,所有令牌看起来都很像,没有特别的含义.从 kube-apiserver 服务器的角度,启动引导令牌是很特殊的.根据其 type、namespace 和 name,kube-apiserver 能够将认作特殊的令牌, 并授予携带该令牌的任何人特殊的启动引导权限,换言之,将其视为 system:bootstrappers 组的成员.这就满足了 TLS 启动引导的基本需求.

关于创建 Secret 的进一步细节可访问这里.

如果你希望使用启动引导令牌,你必须在 kube-apiserver 上使用下面的标志启用之:

--enable-bootstrap-token-auth=true

-* 令牌认证文件 
kube-apiserver 能够将令牌视作身份认证依据.这些令牌可以是任意数据,但必须表示为基于某安全的随机数生成器而得到的 至少 128 位混沌数据.这里的随机数生成器可以是现代 Linux 系统上的 /dev/urandom.生成令牌的方式有很多种.例如:

head -c 16 /dev/urandom | od -An -t x | tr -d ' '
上面的命令会生成类似于 02b50b05283e98dd0fd71db496ef01e8 这样的令牌.

令牌文件看起来是下面的例子这样,其中前面三个值可以是任何值,用引号括起来 的组名称则只能用例子中给的值.

02b50b05283e98dd0fd71db496ef01e8,kubelet-bootstrap,10001,"system:bootstrappers"

向 kube-apiserver 添加 --token-auth-file=FILENAME 标志(或许这要对 systemd 的单元文件作修改)以启用令牌文件.参见 这里 的文档以了解进一步的细节.

* 授权 kubelet 创建 CSR 
现在启动引导节点被身份认证为 system:bootstrapping 组的成员,它需要被 授权 创建证书签名请求(CSR)并在证书被签名之后将其取回.幸运的是,Kubernetes 提供了一个 ClusterRole,其中精确地封装了这些许可, system:node-bootstrapper.

为了实现这一点,你只需要创建 ClusterRoleBinding,将 system:bootstrappers 组绑定到集群角色 system:node-bootstrapper.

# 允许启动引导节点创建 CSR
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: create-csrs-for-bootstrapping
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:node-bootstrapper
  apiGroup: rbac.authorization.k8s.io


** kube-controller-manager 配置 
API 服务器从 kubelet 收到证书请求并对这些请求执行身份认证,但真正负责发放 签名证书的是控制器管理器.

控制器管理器通过一个证书发放的控制回路来执行此操作.该操作的执行方式是使用磁盘上 的文件用 cfssl 本地签名组件 来完成.目前,所发放的所有证书都有一年的有效期,并设定了默认的一组密钥用法.

为了让控制器管理器对证书签名,它需要:

  * 能够访问你之前所创建并分发的"Kubernetes CA 密钥和证书"
  * 启用 CSR 签名

* 访问密钥和证书 
如前所述,你需要创建一个 Kubernetes CA 密钥和证书,并将其发布到主控节点.这些数据会被控制器管理器来对 kubelet 证书进行签名.

由于这些被签名的证书反过来会被 kubelet 用来在 kube-apiserver 执行普通的 kubelet 身份认证,很重要的一点是为控制器管理器所提供的 CA 也被 kube-apiserver 信任用来执行身份认证.CA 密钥和证书是通过 kube-apiserver 的标志 --client-ca-file=FILENAME(例如,--client-ca-file=/var/lib/kubernetes/ca.pem), 来设定的,正如 kube-apiserver 配置节所述.

要将 Kubernetes CA 密钥和证书提供给 kube-controller-manager,可使用以下标志:

--cluster-signing-cert-file="/etc/path/to/kubernetes/ca/ca.crt" --cluster-signing-key-file="/etc/path/to/kubernetes/ca/ca.key"

例如:

--cluster-signing-cert-file="/var/lib/kubernetes/ca.pem" --cluster-signing-key-file="/var/lib/kubernetes/ca-key.pem"

所签名的证书的合法期限可以通过下面的标志来配置:

--cluster-signing-duration

* 批复 
为了对 CSR 进行批复,你需要告诉控制器管理器批复这些 CSR 是可接受的.这是通过将 RBAC 访问权限授予正确的组来实现的.

许可权限有两组:

  * nodeclient:如果节点在为节点创建新的证书,则该节点还没有证书.该节点 使用前文所列的令牌之一来执行身份认证,因此是组 system:bootstrappers 组 的成员.
  * selfnodeclient:如果节点在对证书执行续期操作,则该节点已经拥有一个证书.节点持续使用现有的证书将自己认证为 system:nodes 组的成员.

要允许 kubelet 请求并接收新的证书,可以创建一个 ClusterRoleBinding 将启动 引导节点所处的组 system:bootstrappers 绑定到为其赋予访问权限的 ClusterRole system:certificates.k8s.io:certificatesigningrequests:nodeclient:

# 批复 "system:bootstrappers" 组的所有 CSR
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auto-approve-csrs-for-group
subjects:
- kind: Group
  name: system:bootstrappers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient
  apiGroup: rbac.authorization.k8s.io

要允许 kubelet 对其客户端证书执行续期操作,可以创建一个 ClusterRoleBinding 将正常工作的节点所处的组 system:nodes 绑定到为其授予访问许可的 ClusterRole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient:

# 批复 "system:nodes" 组的 CSR 续约请求
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: auto-approve-renewals-for-nodes
subjects:
- kind: Group
  name: system:nodes
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
  apiGroup: rbac.authorization.k8s.io

作为 kube-controller-manager 的一部分的 csrapproving 控制器是自动被启用的.该控制器使用 SubjectAccessReview API 来确定是否某给定用户被授权请求 CSR,之后基于鉴权结果执行批复操作.为了避免与其它批复组件发生冲突,内置的批复组件不会显式地拒绝任何 CSRs.该组件仅是忽略未被授权的请求.控制器也会作为垃圾收集的一部分清除已过期的证书.


** kubelet 配置
最后,当主控节点被正确配置并且所有必要的身份认证和鉴权机制都就绪时, 我们可以配置 kubelet.

kubelet 需要以下配置来执行启动引导:

  * 一个用来存储所生成的密钥和证书的路径(可选,可以使用默认配置)
  * 一个用来指向尚不存在的 kubeconfig 文件的路径;kubelet 会将启动引导 配置文件放到这个位置
  * 一个指向启动引导 kubeconfig 文件的路径,用来提供 API 服务器的 URL 和启动引导凭据,例如,启动引导令牌
  * 可选的:轮换证书的指令

启动引导 kubeconfig 文件应该放在一个 kubelet 可访问的路径下,例如 /var/lib/kubelet/bootstrap-kubeconfig.

其格式与普通的 kubeconfig 文件完全相同.实例文件可能看起来像这样:

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /var/lib/kubernetes/ca.pem
    server: https://my.server.example.com:6443
  name: bootstrap
contexts:
- context:
    cluster: bootstrap
    user: kubelet-bootstrap
  name: bootstrap
current-context: bootstrap
preferences: {}
users:
- name: kubelet-bootstrap
  user:
    token: 07401b.f395accd246ae52d

需要额外注意的一些因素有:

certificate-authority:指向 CA 文件的路径,用来对 kube-apiserver 所出示的服务器证书进行验证
server: 用来访问 kube-apiserver 的 URL
token:要使用的令牌

令牌的格式并不重要,只要它与 kube-apiserver 的期望匹配即可.在上面的例子中,我们使用的是启动引导令牌.如前所述,任何合法的身份认证方法都可以使用,不限于令牌.

因为启动引导 kubeconfig 文件是一个标准的 kubeconfig 文件,你可以使用 kubectl 来生成该文件.要生成上面的示例文件:

kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-cluster bootstrap --server='https://my.server.example.com:6443' --certificate-authority=/var/lib/kubernetes/ca.pem
kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-credentials kubelet-bootstrap --token=07401b.f395accd246ae52d
kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig set-context bootstrap --user=kubelet-bootstrap --cluster=bootstrap
kubectl config --kubeconfig=/var/lib/kubelet/bootstrap-kubeconfig use-context bootstrap

要指示 kubelet 使用启动引导 kubeconfig 文件,可以使用下面的 kubelet 标志:

--bootstrap-kubeconfig="/var/lib/kubelet/bootstrap-kubeconfig" --kubeconfig="/var/lib/kubelet/kubeconfig"

在启动 kubelet 时,如果 --kubeconfig 标志所指定的文件并不存在,会使用通过标志 --bootstrap-kubeconfig 所指定的启动引导 kubeconfig 配置来向 API 服务器请求 客户端证书.在证书请求被批复并被 kubelet 收回时,一个引用所生成的密钥和所获得 证书的 kubeconfig 文件会被写入到通过 --kubeconfig 所指定的文件路径下.证书和密钥文件会被放到 --cert-dir 所指定的目录中.


** 客户和服务证书 
前文所述的内容都与 kubelet 客户端 证书相关,尤其是 kubelet 用来向 kube-apiserver 认证自身身份的证书.

kubelet 也可以使用 服务(Serving) 证书.kubelet 自身向外提供一个 HTTPS 末端,包含若干功能特性.要保证这些末端的安全性,kubelet 可以执行以下操作 之一:

  * 使用通过 --tls-private-key-file 和 --tls-cert-file 所设置的密钥和证书
  * 如果没有提供密钥和证书,则创建自签名的密钥和证书
  * 通过 CSR API 从集群服务器请求服务证书

TLS 启动引导所提供的客户端证书默认被签名为仅用于 client auth(客户端认证), 因此不能作为提供服务的证书,或者 server auth.

不过,你可以启用服务器证书,至少可以部分地通过证书轮换来实现这点.


** 证书轮换 
Kubernetes v1.8 和更高版本的 kubelet 实现了对客户端证书与/或服务证书进行轮换 这一 Beta 特性.这一特性通过 kubelet 对应的 RotateKubeletClientCertificate 和 RotateKubeletServerCertificate 特性门控标志来控制,并且是默认启用的.

RotateKubeletClientCertificate 会导致 kubelet 在其现有凭据即将过期时通过 创建新的 CSR 来轮换其客户端证书.要启用此功能特性,可将下面的标志传递给 kubelet:

--rotate-certificates

RotateKubeletServerCertificate 会让 kubelet 在启动引导其客户端凭据之后请求 一个服务证书 且 对该服务证书执行轮换操作.要启用此功能特性,将下面的标志 传递给 kubelet:

--rotate-server-certificates

说明:
Kubernetes 核心中所实现的 CSR 批复控制器出于 安全原因 并不会自动批复节点的 服务 证书.要使用 RotateKubeletServerCertificate 功能特性,集群运维人员需要运行一个 定制的控制器或者手动批复服务证书的请求.

对 kubelet 服务证书的批复过程因集群部署而异,通常应该仅批复如下 CSR:

  1. 由节点发出的请求(确保 spec.username 字段形式为 system:node:<nodeName> 且 spec.groups 包含 system:nodes)
  2. 请求中包含服务证书用法(确保 spec.usages 中包含 server auth,可选地也可 包含 digital signature 和 key encipherment,且不包含其它用法)
  3. 仅包含隶属于请求节点的 IP 和 DNS 的 subjectAltNames,没有 URI 和 Email 形式的 subjectAltNames(解析 spec.request 中的 x509 证书签名请求可以 检查 subjectAltNames)


** 其它身份认证组件 
本文所描述的所有 TLS 启动引导内容都与 kubelet 相关.不过,其它组件也可能需要 直接与 kube-apiserver 直接通信.容易想到的是 kube-proxy,同样隶属于 Kubernetes 的控制面并且运行在所有节点之上,不过也可能包含一些其它负责 监控或者联网的组件.

与 kubelet 类似,这些其它组件也需要一种向 kube-apiserver 认证身份的方法.你可以用几种方法来生成这类凭据:

  * 较老的方式:和 kubelet 在 TLS 启动引导之前所做的一样,用类似的方式 创建和分发证书
  * DaemonSet:由于 kubelet 自身被加载到所有节点之上,并且有足够能力来启动基本服务, 你可以运行将 kube-proxy 和其它特定节点的服务作为 kube-system 名字空间中的 DaemonSet 来执行,而不是独立的进程.由于 DaemonSet 位于集群内部,你可以为其 指派一个合适的服务账户,使之具有适当的访问权限来完成其使命.这也许是配置此类 服务的最简单的方法.


** kubectl 批复 
CSRs 可以在控制器管理其内置的批复工作流之外被批复.

签名控制器并不会立即对所有证书请求执行签名操作.相反,它会等待这些请求被某 具有适当特权的用户标记为 "Approved(已批准)"状态.这一流程有意允许由外部批复控制器来自动执行的批复,或者由控制器管理器内置的 批复控制器来自动批复.不过,集群管理员也可以使用 kubectl 来手动批准证书请求.管理员可以通过 kubectl get csr 来列举所有的 CSR,使用 kubectl descsribe csr <name> 来描述某个 CSR 的细节.管理员可以使用 kubectl certificate approve <name 来批准某 CSR,或者 kubectl certificate deny <name> 来拒绝某 CSR.

vvvvvvvvvvvvvvvvvvvv


想要连接到 apiserver 的 Pod 可以使用服务账号安全地进行连接. 当 Pod 被实例化时,Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里. kubernetes 服务(位于 default 名字空间中)配置了一个虚拟 IP 地址,用于(通过 kube-proxy)转发请求到 apiserver 的 HTTPS 末端.


** 控制面到节点 --- api server -> node
从控制面(apiserver)到节点有两种主要的通信路径. 第一种是从 apiserver 到集群中每个节点上运行的 kubelet 进程. 第二种是从 apiserver 通过它的代理功能连接到任何节点、Pod 或者服务.

* API 服务器到 kubelet
从 apiserver 到 kubelet 的连接用于:

  * 获取 Pod 日志
  * 挂接(通过 kubectl)到运行中的 Pod
  * 提供 kubelet 的端口转发功能.

这些连接终止于 kubelet 的 HTTPS 末端. 默认情况下,apiserver 不检查 kubelet 的服务证书.这使得此类连接容易受到中间人攻击, 在非受信网络或公开网络上运行也是不安全的.

为了对这个连接进行认证,使用 --kubelet-certificate-authority 标志给 apiserver 提供一个根证书包,用于 kubelet 的服务证书.

最后,应该启用 kubelet 用户认证和/或鉴权 来保护 kubelet API.


Kubelet 认证/鉴权
https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/

以下内容摘录之上述的链接
^^^^^^^^^^^^^^^^^^^^
## Kubelet 认证/鉴权

概述
kubelet 的 HTTPS 端点公开了 API, 这些 API 可以访问敏感度不同的数据, 并允许你在节点上和容器内以不同级别的权限执行操作.

本文档介绍了如何对 kubelet 的 HTTPS 端点的访问进行认证和鉴权.


** Kubelet 身份认证
默认情况下,未被已配置的其他身份认证方法拒绝的对 kubelet 的 HTTPS 端点的请求会被视为匿名请求, 并被赋予 system:anonymous 用户名和 system:unauthenticated 组.

要禁用匿名访问并向未经身份认证的请求发送 401 Unauthorized 响应,请执行以下操作:

  * 带 --anonymous-auth=false 标志启动 kubelet

要对 kubelet 的 HTTPS 端点启用 X509 客户端证书认证:

  * 带 --client-ca-file 标志启动 kubelet,提供一个 CA 证书包以供验证客户端证书
  * 带 --kubelet-client-certificate 和 --kubelet-client-key 标志启动 apiserver

要启用 API 持有者令牌(包括服务帐户令牌)以对 kubelet 的 HTTPS 端点进行身份验证,请执行以下操作:

  * 确保在 API 服务器中启用了 authentication.k8s.io/v1beta1 API 组
  * 带 --authentication-token-webhook 和 --kubeconfig 标志启动 kubelet
  * kubelet 调用已配置的 API 服务器上的 TokenReview API,以根据持有者令牌确定用户信息


** Kubelet 鉴权
任何成功通过身份验证的请求(包括匿名请求)之后都会被鉴权. 默认的鉴权模式为 AlwaysAllow,它允许所有请求.

细分对 kubelet API 的访问权限可能有多种原因:

  * 启用了匿名身份验证,但是应限制匿名用户调用 kubelet API 的能力
  * 启用了持有者令牌认证,但应限制任意 API 用户(如服务帐户)调用 kubelet API 的能力
  * 启用了客户端证书身份验证,但仅应允许已配置的 CA 签名的某些客户端证书使用 kubelet API

要细分对 kubelet API 的访问权限,请将鉴权委派给 API 服务器:

  * 确保在 API 服务器中启用了 authorization.k8s.io/v1beta1 API 组
  * 带 --authorization-mode=Webhook 和 --kubeconfig 标志启动 kubelet
  * kubelet 调用已配置的 API 服务器上的 SubjectAccessReview API, 以确定每个请求是否得到鉴权
kubelet 使用与 apiserver 相同的 请求属性 方法对 API 请求执行鉴权.

请求的动词根据传入请求的 HTTP 动词确定:

HTTP 动词	请求动词
POST	          create
GET, HEAD	get
PUT	            update
PATCH         patch
DELETE	       delete

资源和子资源是根据传入请求的路径确定的:

Kubelet API	  资源	  子资源
/stats/*	        nodes	stats
/metrics/*	    nodes	metrics
/logs/*         	nodes	log
/spec/*	        nodes	spec
其它所有	    nodes	proxy

名字空间和 API 组属性始终是空字符串, 资源名称始终是 kubelet 的 Node API 对象的名称.

在此模式下运行时,请确保传递给 apiserver 的由 --kubelet-client-certificate 和 --kubelet-client-key 标志标识的用户具有以下属性的鉴权:

verb=*, resource=nodes, subresource=proxy
verb=*, resource=nodes, subresource=stats
verb=*, resource=nodes, subresource=log
verb=*, resource=nodes, subresource=spec
verb=*, resource=nodes, subresource=metrics

vvvvvvvvvvvvvvvvvvvv

* apiserver 到节点、Pod 和服务
从 apiserver 到节点、Pod 或服务的连接默认为纯 HTTP 方式,因此既没有认证,也没有加密. 
这些连接 目前还不能安全地 在非受信网络或公共网络上运行.

SSH 隧道 
Kubernetes 支持使用 SSH 隧道来保护从控制面到节点的通信路径.SSH 隧道目前已被废弃.除非你了解个中细节,否则不应使用. Konnectivity 服务是对此通信通道的替代品.



## 控制器
在机器人技术和自动化领域,控制回路(Control Loop)是一个非终止回路,用于调节系统状态.

这是一个控制环的例子:房间里的温度自动调节器.

当你设置了温度,告诉了温度自动调节器你的期望状态(Desired State). 房间的实际温度是当前状态(Current State). 通过对设备的开关控制,温度自动调节器让其当前状态接近期望状态.

在 Kubernetes 中,控制器通过监控集群 的公共状态,并致力于将当前状态转变为期望的状态.


** 控制器模式
一个控制器至少追踪一种类型的 Kubernetes 资源.这些对象有一个代表期望状态的 spec 字段. 该资源的控制器负责确保其当前状态接近期望状态.

控制器可能会自行执行操作;在 Kubernetes 中更常见的是一个控制器会发送信息给 API 服务器,这会有副作用. 具体可参看后文的例子.

* 通过 API 服务器来控制
Job 控制器是一个 Kubernetes 内置控制器的例子. 内置控制器通过和集群 API 服务器交互来管理状态.

Job 是一种 Kubernetes 资源,它运行一个或者多个 Pod, 来执行一个任务然后停止. (一旦被调度了,对 kubelet 来说 Pod 对象就会变成了期望状态的一部分).

在集群中,当 Job 控制器拿到新任务时,它会保证一组 Node 节点上的 kubelet 可以运行正确数量的 Pod 来完成工作. Job 控制器不会自己运行任何的 Pod 或者容器.Job 控制器是通知 API 服务器来创建或者移除 Pod. 控制面中的其它组件 根据新的消息作出反应(调度并运行新 Pod)并且最终完成工作.

创建新 Job 后,所期望的状态就是完成这个 Job.Job 控制器会让 Job 的当前状态不断接近期望状态:创建为 Job 要完成工作所需要的 Pod,使 Job 的状态接近完成.

控制器也会更新配置对象.例如:一旦 Job 的工作完成了,Job 控制器会更新 Job 对象的状态为 Finished.

(这有点像温度自动调节器关闭了一个灯,以此来告诉你房间的温度现在到你设定的值了).

* 直接控制
相比 Job 控制器,有些控制器需要对集群外的一些东西进行修改.

例如,如果你使用一个控制回路来保证集群中有足够的 节点,那么控制器就需要当前集群外的 一些服务在需要时创建新节点.

和外部状态交互的控制器从 API 服务器获取到它想要的状态,然后直接和外部系统进行通信 并使当前状态更接近期望状态.

(实际上有一个控制器 可以水平地扩展集群中的节点.)

这里,很重要的一点是,控制器做出了一些变更以使得事物更接近你的期望状态, 之后将当前状态报告给集群的 API 服务器. 其他控制回路可以观测到所汇报的数据的这种变化并采取其各自的行动.

在温度计的例子中,如果房间很冷,那么某个控制器可能还会启动一个防冻加热器. 就 Kubernetes 集群而言,控制面间接地与 IP 地址管理工具、存储服务、云驱动 APIs 以及其他服务协作,通过扩展 Kubernetes 来实现这点.


** 期望状态与当前状态
Kubernetes 采用了系统的云原生视图,并且可以处理持续的变化.

在任务执行时,集群随时都可能被修改,并且控制回路会自动修复故障.这意味着很可能集群永远不会达到稳定状态.

只要集群中的控制器在运行并且进行有效的修改,整体状态的稳定与否是无关紧要的.


** 设计
作为设计原则之一,Kubernetes 使用了很多控制器,每个控制器管理集群状态的一个特定方面.最常见的一个特定的控制器使用一种类型的资源作为它的期望状态, 控制器管理控制另外一种类型的资源向它的期望状态演化.

使用简单的控制器而不是一组相互连接的单体控制回路是很有用的.控制器会失败,所以 Kubernetes 的设计正是考虑到了这一点.

说明:
可以有多个控制器来创建或者更新相同类型的对象.在后台,Kubernetes 控制器确保它们只关心与其控制资源相关联的资源.

例如,你可以创建 Deployment 和 Job;它们都可以创建 Pod.Job 控制器不会删除 Deployment 所创建的 Pod,因为有信息 (标签)让控制器可以区分这些 Pod.


** 运行控制器的方式
Kubernetes 内置一组控制器,运行在 kube-controller-manager 内.这些内置的控制器提供了重要的核心功能.

Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子.Kubernetes 允许你运行一个稳定的控制平面,这样即使某些内置控制器失败了, 控制平面的其他部分会接替它们的工作.

你会遇到某些控制器运行在控制面之外,用以扩展 Kubernetes.或者,如果你愿意,你也可以自己编写新控制器.你可以以一组 Pod 来运行你的控制器,或者运行在 Kubernetes 之外.最合适的方案取决于控制器所要执行的功能是什么.


## Kubernetes 组件
https://kubernetes.io/zh/docs/concepts/overview/components/#control-plane-components

以下内容摘录之上述的链接
#################### start #############################
################################################
重要
** 控制平面组件(Control Plane Components)
控制平面的组件对集群做出全局决策(比如调度),以及检测和响应集群事件(例如,当不满足部署的 replicas 字段时,启动新的 pod).

控制平面组件可以在集群中的任何节点上运行. 然而,为了简单起见,设置脚本通常会在同一个计算机上启动所有控制平面组件,并且不会在此计算机上运行用户容器.

* kube-apiserver 
API 服务器是 Kubernetes 控制平面的组件,该组件公开了 Kubernetes API. API 服务器是 Kubernetes 控制面的前端.

* kube-scheduler
控制平面组件,负责监视新创建的、未指定运行节点(node)的 Pods,选择节点让 Pod 在上面运行.

调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限.

* kube-controller-manager
运行控制器进程的控制平面组件.

从逻辑上讲,每个控制器都是一个单独的进程,但是为了降低复杂性,它们都被编译到同一个可执行文件,并在一个进程中运行.

这些控制器包括:

  * 节点控制器(Node Controller): 负责在节点出现故障时进行通知和响应
  * 任务控制器(Job controller): 监测代表一次性任务的 Job 对象,然后创建 Pods 来运行这些任务直至完成
  * 端点控制器(Endpoints Controller): 填充端点(Endpoints)对象(即加入 Service 与 Pod)
  * 服务帐户和令牌控制器(Service Account & Token Controllers): 为新的命名空间创建默认帐户和 API 访问令牌


** Node 组件 
节点组件在每个节点上运行,维护运行的 Pod 并提供 Kubernetes 运行环境.

* kubelet
一个在集群中每个节点(node)上运行的代理. 它保证容器(containers)都运行在 Pod 中.

kubelet 接收一组通过各类机制提供给它的 PodSpecs,确保这些 PodSpecs 中描述的容器处于运行状态且健康. kubelet 不会管理不是由 Kubernetes 创建的容器.

* kube-proxy
kube-proxy 是集群中每个节点上运行的网络代理,实现 Kubernetes 服务(Service) 概念的一部分.

kube-proxy 维护节点上的网络规则.这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信.

如果操作系统提供了数据包过滤层并可用的话,kube-proxy 会通过它来实现网络规则.否则,kube-proxy 仅转发流量本身.

* 容器运行时(Container Runtime) 
容器运行环境是负责运行容器的软件.

################################################
#################### end #############################



## 垃圾收集
垃圾收集是 Kubernetes 用于清理集群资源的各种机制的统称. 垃圾收集允许系统清理如下资源:

  * 失败的 Pod
  * 已完成的 Job
  * 不再存在属主引用的对象
  * 未使用的容器和容器镜像
  * 动态制备的、StorageClass 回收策略为 Delete 的 PV 卷
  * 阻滞或者过期的 CertificateSigningRequest (CSRs)
  * 在以下情形中删除了的节点对象:
    *当集群使用云控制器管理器运行于云端时;
    *当集群使用类似于云控制器管理器的插件运行在本地环境中时.
  * 节点租约对象


** 属主与依赖 
Kubernetes 中很多对象通过属主引用 链接到彼此.属主引用(Owner Reference)可以告诉控制面哪些对象依赖于其他对象. Kubernetes 使用属主引用来为控制面以及其他 API 客户端在删除某对象时提供一个 清理关联资源的机会.在大多数场合,Kubernetes 都是自动管理属主引用的.

属主关系与某些资源所使用的的标签和选择算符 不同.例如,考虑一个创建 EndpointSlice 对象的 Service 对象.Service 对象使用标签来允许控制面确定哪些 EndpointSlice 对象被该 Service 使用.除了标签,每个被 Service 托管的 EndpointSlice 对象还有一个属主引用属性. 属主引用可以帮助 Kubernetes 中的不同组件避免干预并非由它们控制的对象.

说明:
根据设计,系统不允许出现跨名字空间的属主引用.名字空间作用域的依赖对象可以指定集群作用域或者名字空间作用域的属主. 名字空间作用域的属主必须存在于依赖对象所在的同一名字空间. 如果属主位于不同名字空间,则属主引用被视为不存在,而当检查发现所有属主都已不存在时, 依赖对象会被删除.

集群作用域的依赖对象只能指定集群作用域的属主. 在 1.20 及更高版本中,如果一个集群作用域的依赖对象指定了某个名字空间作用域的类别作为其属主, 则该对象被视为拥有一个无法解析的属主引用,因而无法被垃圾收集处理.

在 1.20 及更高版本中,如果垃圾收集器检测到非法的跨名字空间 ownerReference, 或者某集群作用域的依赖对象的 ownerReference 引用某名字空间作用域的类别, 系统会生成一个警告事件,其原因为 OwnerRefInvalidNamespace,involvedObject 设置为非法的依赖对象.你可以通过运行 kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace 来检查是否存在这类事件.


** 级联删除 
Kubernetes 会检查并删除那些不再拥有属主引用的对象,例如在你删除了 ReplicaSet 之后留下来的 Pod.当你删除某个对象时,你可以控制 Kubernetes 是否要通过一个称作 级联删除(Cascading Deletion)的过程自动删除该对象的依赖对象. 级联删除有两种类型,分别如下:

  * 前台级联删除
  * 后台级联删除
你也可以使用 Kubernetes Finalizers 来控制垃圾收集机制如何以及何时删除包含属主引用的资源.

级联删除的手动做法以及删除被遗弃对象的方法
https://kubernetes.io/zh/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion

* 前台级联删除
在前台级联删除中,正在被你删除的对象首先进入 deletion in progress 状态. 在这种状态下,针对属主对象会发生以下事情:

  * Kubernetes API 服务器将对象的 metadata.deletionTimestamp 字段设置为对象被标记为要删除的时间点.
  * Kubernetes API 服务器也会将 metadata.finalizers 字段设置为 foregroundDeletion.
  * 在删除过程完成之前,通过 Kubernetes API 仍然可以看到该对象.

当属主对象进入删除过程中状态后,控制器删除其依赖对象.控制器在删除完所有依赖对象之后, 删除属主对象.这时,通过 Kubernetes API 就无法再看到该对象.

在前台级联删除过程中,唯一的可能阻止属主对象被删除的依赖对象是那些带有 ownerReference.blockOwnerDeletion=true 字段的对象. 

* 后台级联删除
在后台级联删除过程中,Kubernetes 服务器立即删除属主对象,控制器在后台清理所有依赖对象. 默认情况下,Kubernetes 使用后台级联删除方案,除非你手动设置了要使用前台删除, 或者选择遗弃依赖对象.

* 被遗弃的依赖对象 
当 Kubernetes 删除某个属主对象时,被留下来的依赖对象被称作被遗弃的(Orphaned)对象.默认情况下,Kubernetes 会删除依赖对象.


** 未使用容器和镜像的垃圾收集 
kubelet 会每五分钟对未使用的镜像执行一次垃圾收集, 每分钟对未使用的容器执行一次垃圾收集.你应该避免使用外部的垃圾收集工具,因为外部工具可能会破坏 kubelet 的行为,移除应该保留的容器.

要配置对未使用容器和镜像的垃圾收集选项,可以使用一个 配置文件,基于 KubeletConfiguration 资源类型来调整与垃圾搜集相关的 kubelet 行为.

* 容器镜像生命期 
Kubernetes 通过其镜像管理器(Image Manager)来管理所有镜像的生命周期, 该管理器是 kubelet 的一部分,工作时与 cadvisor 协同.kubelet 在作出垃圾收集决定时会考虑如下磁盘用量约束:

  * HighThresholdPercent
  * LowThresholdPercent

磁盘用量超出所配置的 HighThresholdPercent 值时会触发垃圾收集, 垃圾收集器会基于镜像上次被使用的时间来按顺序删除它们,首先删除的是最老的镜像.kubelet 会持续删除镜像,直到磁盘用量到达 LowThresholdPercent 值为止.

* 容器垃圾收集 
kubelet 会基于如下变量对所有未使用的容器执行垃圾收集操作,这些变量都是你可以定义的:

  * MinAge:kubelet 可以垃圾回收某个容器时该容器的最小年龄.设置为 0 表示禁止使用此规则.
  * MaxPerPodContainer:每个 Pod 可以包含的已死亡的容器个数上限.设置为小于 0 的值表示禁止使用此规则.
  * MaxContainers:集群中可以存在的已死亡的容器个数上限.设置为小于 0 的值意味着禁止应用此规则.

除以上变量之外,kubelet 还会垃圾收集除无标识的以及已删除的容器,通常从最老的容器开始.

当保持每个 Pod 的最大数量的容器(MaxPerPodContainer)会使得全局的已死亡容器个数超出上限 (MaxContainers)时,MaxPerPodContainer 和 MaxContainers 之间可能会出现冲突.在这种情况下,kubelet 会调整 MaxPerPodContainer 来解决这一冲突.最坏的情形是将 MaxPerPodContainer 降格为 1,并驱逐最老的容器.此外,当隶属于某已被删除的 Pod 的容器的年龄超过 MinAge 时,它们也会被删除.


** 配置垃圾收集 
你可以通过配置特定于管理资源的控制器来调整资源的垃圾收集行为.

配置 Kubernetes 对象的级联删除--- 前述的链接描述此种情况
配置已完成 Job 的清理 --- https://kubernetes.io/zh/docs/concepts/workloads/controllers/ttlafterfinished/

-------------------
# 容器
## 镜像
* 更新镜像
** 镜像拉取策略 
容器的 imagePullPolicy 和镜像的标签会影响 kubelet 尝试拉取(下载)指定的镜像.

以下列表包含了 imagePullPolicy 可以设置的值,以及这些值的效果:

IfNotPresent
只有当镜像在本地不存在时才会拉取.
Always
每当 kubelet 启动一个容器时,kubelet 会查询容器的镜像仓库, 将名称解析为一个镜像摘要.如果 kubelet 有一个容器镜像,并且对应的摘要已在本地缓存,kubelet 就会使用其缓存的镜像; 否则,kubelet 就会使用解析后的摘要拉取镜像,并使用该镜像来启动容器.
Never
Kubelet 不会尝试获取镜像.如果镜像已经以某种方式存在本地, kubelet 会尝试启动容器;否则,会启动失败.更多细节见提前拉取镜像.
只要能够可靠地访问镜像仓库,底层镜像提供者的缓存语义甚至可以使 imagePullPolicy: Always 高效.你的容器运行时可以注意到节点上已经存在的镜像层,这样就不需要再次下载.

为了确保 Pod 总是使用相同版本的容器镜像,你可以指定镜像的摘要; 将 <image-name>:<tag> 替换为 <image-name>@<digest>,例如 image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2.

当使用镜像标签时,如果镜像仓库修改了代码所对应的镜像标签,可能会出现新旧代码混杂在 Pod 中运行的情况.镜像摘要唯一标识了镜像的特定版本,因此 Kubernetes 每次启动具有指定镜像名称和摘要的容器时,都会运行相同的代码.指定一个镜像可以固定你所运行的代码,这样镜像仓库的变化就不会导致版本的混杂.


** 默认镜像拉取策略 
当你(或控制器)向 API 服务器提交一个新的 Pod 时,你的集群会在满足特定条件时设置 imagePullPolicy 字段:

如果你省略了 imagePullPolicy 字段,并且容器镜像的标签是 :latest, imagePullPolicy 会自动设置为 Always.
如果你省略了 imagePullPolicy 字段,并且没有指定容器镜像的标签, imagePullPolicy 会自动设置为 Always.
如果你省略了 imagePullPolicy 字段,并且为容器镜像指定了非 :latest 的标签, imagePullPolicy 就会自动设置为 IfNotPresent.
说明:
容器的 imagePullPolicy 的值总是在对象初次 创建 时设置的,如果后来镜像的标签发生变化,则不会更新.

例如,如果你用一个 非 :latest 的镜像标签创建一个 Deployment, 并在随后更新该 Deployment 的镜像标签为 :latest,则 imagePullPolicy 字段 不会 变成 Always.你必须手动更改已经创建的资源的拉取策略.

** 必要的镜像拉取 
如果你想总是强制执行拉取,你可以使用下述的一中方式:

设置容器的 imagePullPolicy 为 Always.
省略 imagePullPolicy,并使用 :latest 作为镜像标签; 当你提交 Pod 时,Kubernetes 会将策略设置为 Always.
省略 imagePullPolicy 和镜像的标签; 当你提交 Pod 时,Kubernetes 会将策略设置为 Always.
启用准入控制器 AlwaysPullImages.

** ImagePullBackOff
当 kubelet 使用容器运行时创建 Pod 时,容器可能因为 ImagePullBackOff 导致状态为 Waiting.

ImagePullBackOff 状态意味着容器无法启动, 因为 Kubernetes 无法拉取容器镜像(原因包括无效的镜像名称,或从私有仓库拉取而没有 imagePullSecret).BackOff 部分表示 Kubernetes 将继续尝试拉取镜像,并增加回退延迟.

Kubernetes 会增加每次尝试之间的延迟,直到达到编译限制,即 300 秒(5 分钟).

* 带镜像索引的多架构镜像
Kubernetes 自身通常在命名容器镜像时添加后缀 -$(ARCH).为了向前兼容,请在生成较老的镜像时也提供后缀.这里的理念是为某镜像(如 pause)生成针对所有平台都适用的清单时, 生成 pause-amd64 这类镜像,以便较老的配置文件或者将镜像后缀影编码到其中的 YAML 文件也能兼容.


配置 Node 对私有仓库认证,在 Pod 上指定 ImagePullSecrets(仓库使用密钥)
https://kubernetes.io/zh/docs/concepts/containers/images/

容器运行时介绍
https://juejin.cn/post/6844904045295976461

容器运行时接口介绍
https://jimmysong.io/kubernetes-handbook/concepts/cri.html

容器运行时更像是一个接口程序,接口容器(有不同种类的容器)
OCI - 容器运行时的一个规范
runc - 具体的容器运行时的工具和库,而后出现了 rkt,containerd,cri-o 等等
cri - 容器运行时接口(Container Runtime Interface),简称 CRI

容器运行时及容器运行时接口关系,类似如下:

kubelet            <-- CRI protobuf -->      CRI shim          <-- container runtime -->    container               
  grpc client                                            grpc server 


------------------
# 工作负载
Kubernetes 提供若干种内置的工作负载资源:

  * Deployment 和 ReplicaSet (替换原来的资源 ReplicationController).Deployment 很适合用来管理你的集群上的无状态应用,Deployment 中的所有 Pod 都是相互等价的,并且在需要的时候被换掉.
  * StatefulSet 让你能够运行一个或者多个以某种方式跟踪应用状态的 Pods.例如,如果你的负载会将数据作持久存储,你可以运行一个 StatefulSet,将每个 Pod 与某个 PersistentVolume 对应起来.你在 StatefulSet 中各个 Pod 内运行的代码可以将数据复制到同一 StatefulSet 中的其它 Pod 中以提高整体的服务可靠性.
  * DaemonSet 定义提供节点本地支撑设施的 Pods.这些 Pods 可能对于你的集群的运维是 非常重要的,例如作为网络链接的辅助工具或者作为网络 插件 的一部分等等.每次你向集群中添加一个新节点时,如果该节点与某 DaemonSet 的规约匹配,则控制面会为该 DaemonSet 调度一个 Pod 到该新节点上运行.
  * Job 和 CronJob.定义一些一直运行到结束并停止的任务.Job 用来表达的是一次性的任务,而 CronJob 会根据其时间规划反复运行.


重要
运行一个单实例有状态应用
https://kubernetes.io/zh/docs/tasks/run-application/run-single-instance-stateful-application/

运行一个有状态的应用程序,多副本集合 
https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/

文中提到的 init 容器如下链接

Init 容器
https://kubernetes.io/zh/docs/concepts/workloads/pods/init-containers/

使用 CronJob 运行自动化任务
https://kubernetes.io/zh/docs/tasks/job/automated-tasks-with-cron-jobs/

## Pod
下面是一些管理一个或者多个 Pod 的工作负载资源的示例:

Deployment
StatefulSet
DaemonSet

** Pod 模版

工作负载的控制器会使用负载对象中的 PodTemplate 来生成实际的 Pod. PodTemplate 是你用来运行应用时指定的负载资源的目标状态的一部分.

控制器对象(如 Deployment 和 StatefulSet)包含 Pod 模板字段.Pod 模板包含 Pod 规范,该规范决定了每个 Pod 应如何运行,具体包括应在 Pod 中运行哪些容器以及 Pod 应装载哪些卷.

控制器对象使用 Pod 模板创建 Pod,并管理其在集群内的"所需状态".Pod 模板更改后,所有未来 Pod 都会反映新模板,但所有现有 Pod 则不会.

下面的示例是一个简单的 Job 的清单,其中的 template 指示启动一个容器. 该 Pod 中的容器会打印一条消息之后暂停.

apiVersion: batch/v1
kind: Job
metadata:
  name: hello
spec:
  template:
    # 这里是 Pod 模版
    spec:
      containers:
      - name: hello
        image: busybox
        command: ['sh', '-c', 'echo "Hello, Kubernetes!" && sleep 3600']
      restartPolicy: OnFailure
    # 以上为 Pod 模版

修改 Pod 模版或者切换到新的 Pod 模版都不会对已经存在的 Pod 起作用. Pod 不会直接收到模版的更新.相反, 新的 Pod 会被创建出来,与更改后的 Pod 模版匹配.

例如,Deployment 控制器针对每个 Deployment 对象确保运行中的 Pod 与当前的 Pod 模版匹配.如果模版被更新,则 Deployment 必须删除现有的 Pod,基于更新后的模版 创建新的 Pod.每个工作负载资源都实现了自己的规则,用来处理对 Pod 模版的更新.


** Pod 更新与替换 
正如前面章节所述,当某工作负载的 Pod 模板被改变时,控制器会基于更新的模板 创建新的 Pod 对象而不是对现有 Pod 执行更新或者修补操作.

Kubernetes 并不禁止你直接管理 Pod.对运行中的 Pod 的某些字段执行就地更新操作 还是可能的.不过,类似 patch 和 replace 这类更新操作有一些限制:

  * Pod 的绝大多数元数据都是不可变的.例如,你不可以改变其 namespace、name、 uid 或者 creationTimestamp 字段;generation 字段是比较特别的,如果更新 该字段,只能增加字段取值而不能减少.

  * 如果 metadata.deletionTimestamp 已经被设置,则不可以向 metadata.finalizers 列表中添加新的条目.

  * Pod 更新不可以改变除 spec.containers[*].image、spec.initContainers[*].image、 spec.activeDeadlineSeconds 或 spec.tolerations 之外的字段. 对于 spec.tolerations,你只被允许添加新的条目到其中.

  * 在更新spec.activeDeadlineSeconds 字段时,以下两种更新操作是被允许的:

    1. 如果该字段尚未设置,可以将其设置为一个正数;
    2. 如果该字段已经设置为一个正数,可以将其设置为一个更小的、非负的整数.


** 容器探针 
Probe 是由 kubelet 对容器执行的定期诊断.要执行诊断,kubelet 可以执行三种动作:

  * ExecAction(借助容器运行时执行)
  * TCPSocketAction(由 kubelet 直接检测)
  * HTTPGetAction(由 kubelet 直接检测)



## Pod 的生命周期

** Pod 阶段 
Pod 的 status 字段是一个 PodStatus 对象,其中包含一个 phase 字段.

Pod 的阶段(Phase)是 Pod 在其生命周期中所处位置的简单宏观概述. 该阶段并不是对容器或 Pod 状态的综合汇总,也不是为了成为完整的状态机.

Pod 阶段的数量和含义是严格定义的. 除了本文档中列举的内容外,不应该再假定 Pod 有其他的 phase 值.

下面是 phase 可能的值:

取值	                          描述
Pending(悬决)	      Pod 已被 Kubernetes 系统接受,但有一个或者多个容器尚未创建亦未运行.此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间.
Running(运行中)	  Pod 已经绑定到了某个节点,Pod 中所有的容器都已被创建.至少有一个容器仍在运行,或者正处于启动或重启状态.
Succeeded(成功)	  Pod 中的所有容器都已成功终止,并且不会再重启.
Failed(失败)	          Pod 中的所有容器都已终止,并且至少有一个容器是因为失败终止.也就是说,容器以非 0 状态退出或者被系统终止.
Unknown(未知)     因为某些原因无法取得 Pod 的状态.这种情况通常是因为与 Pod 所在主机通信失败.
如果某节点死掉或者与集群中其他节点失联,Kubernetes 会实施一种策略,将失去的节点上运行的所有 Pod 的 phase 设置为 Failed.


** 容器状态 
Kubernetes 会跟踪 Pod 中每个容器的状态,就像它跟踪 Pod 总体上的阶段一样. 你可以使用容器生命周期回调 来在容器生命周期中的特定时间点触发事件.

一旦调度器将 Pod 分派给某个节点,kubelet 就通过 容器运行时 开始为 Pod 创建容器. 容器的状态有三种:Waiting(等待)、Running(运行中)和 Terminated(已终止).

要检查 Pod 中容器的状态,你可以使用 kubectl describe pod <pod 名称>. 其输出中包含 Pod 中每个容器的状态.

每种状态都有特定的含义:

Waiting (等待) 
如果容器并不处在 Running 或 Terminated 状态之一,它就处在 Waiting 状态. 处于 Waiting 状态的容器仍在运行它完成启动所需要的操作:例如,从某个容器镜像 仓库拉取容器镜像,或者向容器应用 Secret 数据等等. 当你使用 kubectl 来查询包含 Waiting 状态的容器的 Pod 时,你也会看到一个 Reason 字段,其中给出了容器处于等待状态的原因.

Running(运行中) 
Running 状态表明容器正在执行状态并且没有问题发生. 如果配置了 postStart 回调,那么该回调已经执行且已完成. 如果你使用 kubectl 来查询包含 Running 状态的容器的 Pod 时,你也会看到 关于容器进入 Running 状态的信息.

Terminated(已终止) 
处于 Terminated 状态的容器已经开始执行并且或者正常结束或者因为某些原因失败. 如果你使用 kubectl 来查询包含 Terminated 状态的容器的 Pod 时,你会看到 容器进入此状态的原因、退出代码以及容器执行期间的起止时间.

如果容器配置了 preStop 回调,则该回调会在容器进入 Terminated 状态之前执行.


** 容器重启策略
Pod 的 spec 中包含一个 restartPolicy 字段,其可能取值包括 Always、OnFailure 和 Never.默认值是 Always.

restartPolicy 适用于 Pod 中的所有容器.restartPolicy 仅针对同一节点上 kubelet 的容器重启动作.当 Pod 中的容器退出时,kubelet 会按指数回退 方式计算重启的延迟(10s、20s、40s、...),其最长延迟为 5 分钟. 一旦某容器执行了 10 分钟并且没有出现问题,kubelet 对该容器的重启回退计时器执行 重置操作.


** 容器探针 
probe 是由 kubelet 对容器执行的定期诊断. 要执行诊断,kubelet 既可以在容器内执行代码,也可以发出一个网络请求.

* 检查机制 
使用探针来检查容器有四种不同的方法. 每个探针都必须准确定义为这四种机制中的一种:

  exec
  在容器内执行指定命令.如果命令退出时返回码为 0 则认为诊断成功.

  grpc
  使用 gRPC 执行一个远程过程调用. 目标应该实现 gRPC健康检查. 如果响应的状态是 "SERVING",则认为诊断成功. gRPC 探针是一个 alpha 特性,只有在你启用了 "GRPCContainerProbe" 特性门控时才能使用.

  httpGet
  对容器的 IP 地址上指定端口和路径执行 HTTP GET 请求.如果响应的状态码大于等于 200 且小于 400,则诊断被认为是成功的.

  tcpSocket
  对容器的 IP 地址上的指定端口执行 TCP 检查.如果端口打开,则诊断被认为是成功的. 如果远程系统(容器)在打开连接后立即将其关闭,这算作是健康的.

* 探测结果 
每次探测都将获得以下三种结果之一:

  Success(成功)
  容器通过了诊断.
  
  Failure(失败)
  容器未通过诊断.

  Unknown(未知)
  诊断失败,因此不会采取任何行动.

*探测类型 
针对运行中的容器,kubelet 可以选择是否执行以下三种探针,以及如何针对探测结果作出反应:

  livenessProbe
  指示容器是否正在运行.如果存活态探测失败,则 kubelet 会杀死容器,并且容器将根据其重启策略决定未来.如  果容器  不提供存活探针,则默认状态为 Success.

  readinessProbe
  指示容器是否准备好为请求提供服务.如果就绪态探测失败,端点控制器将从与 Pod 匹配的所有服务的端点列表   
  中删除该 Pod 的 IP 地址. 初始延迟之前的就绪态的状态值默认为 Failure. 如果容器不提供就绪态探针,则默认状  态为  Success.

  startupProbe
  指示容器中的应用是否已经启动.如果提供了启动探针,则所有其他探针都会被 禁用,直到此探针成功为止.如   
  果启动探测失败,kubelet 将杀死容器,而容器依其 重启策略进行重启. 如果容器没有提供启动探测,则默认状态  为 Su  ccess.

重要
配置存活态、就绪态和启动探针
https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/


何时该使用存活态探针? 
FEATURE STATE: Kubernetes v1.0 [stable]
如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃,则不一定需要存活态探针; kubelet 将根据 Pod 的restartPolicy 自动执行修复操作.

如果你希望容器在探测失败时被杀死并重新启动,那么请指定一个存活态探针, 并指定restartPolicy 为 "Always" 或 "OnFailure".

何时该使用就绪态探针? 
FEATURE STATE: Kubernetes v1.0 [stable]
如果要仅在探测成功时才开始向 Pod 发送请求流量,请指定就绪态探针. 在这种情况下,就绪态探针可能与存活态探针相同,但是规约中的就绪态探针的存在意味着 Pod 将在启动阶段不接收任何数据,并且只有在探针探测成功后才开始接收数据.

如果你希望容器能够自行进入维护状态,也可以指定一个就绪态探针,检查某个特定于 就绪态的因此不同于存活态探测的端点.

如果你的应用程序对后端服务有严格的依赖性,你可以同时实现存活态和就绪态探针. 当应用程序本身是健康的,存活态探针检测通过后,就绪态探针会额外检查每个所需的后端服务是否可用. 这可以帮助你避免将流量导向只能返回错误信息的 Pod.

如果你的容器需要在启动期间加载大型数据、配置文件或执行迁移,你可以使用 启动探针. 然而,如果你想区分已经失败的应用和仍在处理其启动数据的应用,你可能更倾向于使用就绪探针.

说明:
请注意,如果你只是想在 Pod 被删除时能够排空请求,则不一定需要使用就绪态探针; 在删除 Pod 时,Pod 会自动将自身置于未就绪状态,无论就绪态探针是否存在. 等待 Pod 中的容器停止期间,Pod 会一直处于未就绪状态.

何时该使用启动探针? 
FEATURE STATE: Kubernetes v1.18 [beta]
对于所包含的容器需要较长时间才能启动就绪的 Pod 而言,启动探针是有用的. 你不再需要配置一个较长的存活态探测时间间隔,只需要设置另一个独立的配置选定, 对启动期间的容器执行探测,从而允许使用远远超出存活态时间间隔所允许的时长.

如果你的容器启动时间通常超出 initialDelaySeconds + failureThreshold × periodSeconds 总值,你应该设置一个启动探测,对存活态探针所使用的同一端点执行检查. periodSeconds 的默认值是 10 秒.你应该将其 failureThreshold 设置得足够高, 以便容器有充足的时间完成启动,并且避免更改存活态探针所使用的默认值. 这一设置有助于减少死锁状况的发生.


** 强制终止 Pod 
注意: 对于某些工作负载及其 Pod 而言,强制删除很可能会带来某种破坏.
默认情况下,所有的删除操作都会附有 30 秒钟的宽限期限. kubectl delete 命令支持 --grace-period=<seconds> 选项,允许你重载默认值, 设定自己希望的期限值.

将宽限期限强制设置为 0 意味着立即从 API 服务器删除 Pod. 如果 Pod 仍然运行于某节点上,强制删除操作会触发 kubelet 立即执行清理操作.

说明: 你必须在设置 --grace-period=0 的同时额外设置 --force 参数才能发起强制删除请求.
执行强制删除操作时,API 服务器不再等待来自 kubelet 的、关于 Pod 已经在原来运行的节点上终止执行的确认消息. API 服务器直接删除 Pod 对象,这样新的与之同名的 Pod 即可以被创建. 在节点侧,被设置为立即终止的 Pod 仍然会在被强行杀死之前获得一点点的宽限时间.

如果你需要强制删除 StatefulSet 的 Pod,请参阅 从 StatefulSet 中删除 Pod 的任务文档.


** 失效 Pod 的垃圾收集 
对于已失败的 Pod 而言,对应的 API 对象仍然会保留在集群的 API 服务器上,直到 用户或者控制器进程显式地 将其删除.

控制面组件会在 Pod 个数超出所配置的阈值 (根据 kube-controller-manager 的 terminated-pod-gc-threshold 设置)时 删除已终止的 Pod(阶段值为 Succeeded 或 Failed). 这一行为会避免随着时间演进不断创建和终止 Pod 而引起的资源泄露问题.


^^^^^^^^^^^^^^^^^^^^
## Pod 拓扑分布约束
重要
https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-topology-spread-constraints/

FEATURE STATE: Kubernetes v1.19 [stable]
你可以使用 拓扑分布约束(Topology Spread Constraints) 来控制 Pods 在集群内故障域 之间的分布,例如区域(Region)、可用区(Zone)、节点和其他用户自定义拓扑域. 这样做有助于实现高可用并提升资源利用率.

说明:在 v1.18 之前的 Kubernetes 版本中,如果要使用 Pod 拓扑扩展约束,你必须在 API 服务器 和调度器 中启用 EvenPodsSpread 特性门控.
vvvvvvvvvvvvvvvvvvvv


## 干扰(Disruptions)
重要
https://kubernetes.io/zh/docs/concepts/workloads/pods/disruptions/


本指南针对的是希望构建高可用性应用程序的应用所有者,他们有必要了解可能发生在 Pod 上的干扰类型.

文档同样适用于想要执行自动化集群操作(例如升级和自动扩展集群)的集群管理员.

自愿干扰和非自愿干扰 
Pod 不会消失,除非有人(用户或控制器)将其销毁,或者出现了不可避免的硬件或软件系统错误.

我们把这些不可避免的情况称为应用的非自愿干扰(Involuntary Disruptions).例如:

  * 节点下层物理机的硬件故障
  * 集群管理员错误地删除虚拟机(实例)
  * 云提供商或虚拟机管理程序中的故障导致的虚拟机消失
  * 内核错误
  * 节点由于集群网络隔离从集群中消失
  * 由于节点资源不足导致 pod 被驱逐.

除了资源不足的情况,大多数用户应该都熟悉这些情况;它们不是特定于 Kubernetes 的.

我们称其他情况为自愿干扰(Voluntary Disruptions). 包括由应用程序所有者发起的操作和由集群管理员发起的操作.典型的应用程序所有者的操 作包括:

  * 删除 Deployment 或其他管理 Pod 的控制器
  * 更新了 Deployment 的 Pod 模板导致 Pod 重启
  * 直接删除 Pod(例如,因为误操作)

集群管理员操作包括:

  * 排空(drain)节点进行修复或升级.
  * 从集群中排空节点以缩小集群(了解集群自动扩缩).
  * 从节点中移除一个 Pod,以允许其他 Pod 使用该节点.

这些操作可能由集群管理员直接执行,也可能由集群管理员所使用的自动化工具执行,或者由集群托管提供商自动执行.

咨询集群管理员或联系云提供商,或者查询发布文档,以确定是否为集群启用了任何资源干扰源. 如果没有启用,可以不用创建 Pod Disruption Budgets(Pod 干扰预算)

注意: 并非所有的自愿干扰都会受到 Pod 干扰预算的限制. 例如,删除 Deployment 或 Pod 的删除操作就会跳过 Pod 干扰预算检查.


## 工作负载管理
### Deployments
一个 Deployment 为 Pods 和 ReplicaSets 提供声明式的更新能力.

你负责描述 Deployment 中的 目标状态,而 Deployment 控制器(Controller) 以受控速率更改实际状态, 使其变为期望状态.你可以定义 Deployment 以创建新的 ReplicaSet,或删除现有 Deployment, 并通过新的 Deployment 收养其资源.


** 用例
以下是 Deployments 的典型用例:

  * 创建 Deployment 以将 ReplicaSet 上线. ReplicaSet 在后台创建 Pods. 检查 ReplicaSet 的上线状态,查看其是否成功.
  * 通过更新 Deployment 的 PodTemplateSpec,声明 Pod 的新状态 . 新的 ReplicaSet 会被创建,Deployment 以受控速率将 Pod 从旧 ReplicaSet 迁移到新 ReplicaSet. 每个新的 ReplicaSet 都会更新 Deployment 的修订版本.
  * 如果 Deployment 的当前状态不稳定,回滚到较早的 Deployment 版本. 每次回滚都会更新 Deployment 的修订版本.
  * 扩大 Deployment 规模以承担更多负载.
  * 暂停 Deployment 以应用对 PodTemplateSpec 所作的多项修改, 然后恢复其执行以启动新的上线版本.
  * 使用 Deployment 状态 来判定上线过程是否出现停滞.
  * 清理较旧的不再需要的 ReplicaSet .


** 创建 Deployment 
下面是 Deployment 示例.其中创建了一个 ReplicaSet,负责启动三个 nginx Pods:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80

在该例中:

  * 创建名为 nginx-deployment(由 .metadata.name 字段标明)的 Deployment.
  * 该 Deployment 创建三个(由 replicas 字段标明)Pod 副本.
  *selector 字段定义 Deployment 如何查找要管理的 Pods. 在这里,你选择在 Pod 模板中定义的标签(app: nginx). 不过,更复杂的选择规则是也可能的,只要 Pod 模板本身满足所给规则即可.

说明:
spec.selector.matchLabels 字段是 {key,value} 键值对映射. 在 matchLabels 映射中的每个 {key,value} 映射等效于 matchExpressions 中的一个元素, 即其 key 字段是 "key",operator 为 "In",values 数组仅包含 "value". 在 matchLabels 和 matchExpressions 中给出的所有条件都必须满足才能匹配.

  * template 字段包含以下子字段:
    * Pod 被使用 labels 字段打上 app: nginx 标签.
    * Pod 模板规约(即 .template.spec 字段)指示 Pods 运行一个 nginx 容器, 该容器运行版本为 1.14.2 的 nginx Docker Hub镜像.
    * 创建一个容器并使用 name 字段将其命名为 nginx.

开始之前,请确保的 Kubernetes 集群已启动并运行. 按照以下步骤创建上述 Deployment :

  1. 通过运行以下命令创建 Deployment :

  kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml

  说明: 你可以设置 --record 标志将所执行的命令写入资源注解 kubernetes.io/change-cause 中. 这对于以后的检查是有用的.例如,要查看针对每个 Deployment 修订版本所执行过的命令.

  2. 运行 kubectl get deployments 检查 Deployment 是否已创建.如果仍在创建 Deployment, 则输出类似于:

  NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx-deployment   3         0         0            0           1s

  在检查集群中的 Deployment 时,所显示的字段有:

  * NAME 列出了集群中 Deployment 的名称.
  * READY 显示应用程序的可用的 副本 数.显示的模式是"就绪个数/期望个数".
  * UP-TO-DATE 显示为了达到期望状态已经更新的副本数.
  * AVAILABLE 显示应用可供用户使用的副本数.
  * AGE 显示应用程序运行的时间.

  请注意期望副本数是根据 .spec.replicas 字段设置 3.

  3. 要查看 Deployment 上线状态,运行 kubectl rollout status deployment/nginx-deployment.

  输出类似于:

  Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
  deployment "nginx-deployment" successfully rolled out

  4. 几秒钟后再次运行 kubectl get deployments.输出类似于:

  NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
  nginx-deployment   3         3         3            3           18s

  注意 Deployment 已创建全部三个副本,并且所有副本都是最新的(它们包含最新的 Pod 模板) 并且可用.

  5. 要查看 Deployment 创建的 ReplicaSet(rs),运行 kubectl get rs. 输出类似于:

  NAME                          DESIRED   CURRENT   READY   AGE
  nginx-deployment-75675f5897   3         3         3       18s

  ReplicaSet 输出中包含以下字段:

  * NAME 列出名字空间中 ReplicaSet 的名称;
  * DESIRED 显示应用的期望副本个数,即在创建 Deployment 时所定义的值. 此为期望状态;
  * CURRENT 显示当前运行状态中的副本个数;
  * READY 显示应用中有多少副本可以为用户提供服务;
  * AGE 显示应用已经运行的时间长度.
  
  注意 ReplicaSet 的名称始终被格式化为[Deployment名称]-[随机字符串]. 其中的随机字符串是使用 pod-template-hash 作为种子随机生成的.

  6. 要查看每个 Pod 自动生成的标签,运行 kubectl get pods --show-labels.返回以下输出:

  NAME                                READY     STATUS    RESTARTS   AGE       LABELS
  nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
  nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
  nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
  所创建的 ReplicaSet 确保总是存在三个 nginx Pod.

  说明: 你必须在 Deployment 中指定适当的选择算符和 Pod 模板标签(在本例中为 app: nginx). 标签或者选择算 符不要与其他控制器(包括其他 Deployment 和 StatefulSet)重叠. Kubernetes 不会阻止你这样做,但是如果多个控制器具有重叠的选择算符,它们可能会发生冲突 执行难以预料的操作.


** 更新 Deployment 
说明: 仅当 Deployment Pod 模板(即 .spec.template)发生改变时,例如模板的标签或容器镜像被更新, 才会触发 Deployment 上线. 其他更新(如对 Deployment 执行扩缩容的操作)不会触发上线动作.
按照以下步骤更新 Deployment:

  1. 先来更新 nginx Pod 以使用 nginx:1.16.1 镜像,而不是 nginx:1.14.2 镜像.

  kubectl --record deployment.apps/nginx-deployment set image \
   deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1

  或者使用下面的命令:

  kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1 --record
  
  输出类似于:

  deployment.apps/nginx-deployment image updated
  
  或者,可以 edit Deployment 并将 .spec.template.spec.containers[0].image 从 nginx:1.14.2 更改至 nginx:1.16.1.

  kubectl edit deployment.v1.apps/nginx-deployment
  
  输出类似于:

  deployment.apps/nginx-deployment edited

2.  要查看上线状态,运行:

  kubectl rollout status deployment/nginx-deployment

  输出类似于:

  Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
  
  或者

 deployment "nginx-deployment" successfully rolled out

下次要更新这些 Pods 时,只需再次更新 Deployment Pod 模板即可.

Deployment 可确保在更新时仅关闭一定数量的 Pod.默认情况下,它确保至少所需 Pods 75% 处于运行状态(最大不可用比例为 25%).

Deployment 还确保仅所创建 Pod 数量只可能比期望 Pods 数高一点点. 默认情况下,它可确保启动的 Pod 个数比期望个数最多多出 25%(最大峰值 25%).

* 更改标签选择算符
通常不鼓励更新标签选择算符.建议你提前规划选择算符. 在任何情况下,如果需要更新标签选择算符,请格外小心,并确保自己了解 这背后可能发生的所有事情.

说明: 在 API 版本 apps/v1 中,Deployment 标签选择算符在创建后是不可变的.
添加选择算符时要求使用新标签更新 Deployment 规约中的 Pod 模板标签,否则将返回验证错误. 此更改是非重叠的,也就是说新的选择算符不会选择使用旧选择算符所创建的 ReplicaSet 和 Pod, 这会导致创建新的 ReplicaSet 时所有旧 ReplicaSet 都会被孤立.
选择算符的更新如果更改了某个算符的键名,这会导致与添加算符时相同的行为.
删除选择算符的操作会删除从 Deployment 选择算符中删除现有算符. 此操作不需要更改 Pod 模板标签.现有 ReplicaSet 不会被孤立,也不会因此创建新的 ReplicaSet, 但请注意已删除的标签仍然存在于现有的 Pod 和 ReplicaSet 中.


** 回滚 Deployment
有时,你可能想要回滚 Deployment;例如,当 Deployment 不稳定时(例如进入反复崩溃状态). 默认情况下,Deployment 的所有上线记录都保留在系统中,以便可以随时回滚 (你可以通过修改修订历史记录限制来更改这一约束).

说明: Deployment 被触发上线时,系统就会创建 Deployment 的新的修订版本. 这意味着仅当 Deployment 的 Pod 模板(.spec.template)发生更改时,才会创建新修订版本 -- 例如,模板的标签或容器镜像发生变化. 其他更新,如 Deployment 的扩缩容操作不会创建 Deployment 修订版本. 这是为了方便同时执行手动缩放或自动缩放. 换言之,当你回滚到较早的修订版本时,只有 Deployment 的 Pod 模板部分会被回滚.

* 检查 Deployment 上线历史
按照如下步骤检查回滚历史:

1. 首先,检查 Deployment 修订历史:

kubectl rollout history deployment.v1.apps/nginx-deployment

输出类似于:

deployments "nginx-deployment"

  REVISION    CHANGE-CAUSE
  1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
  2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
  3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true

CHANGE-CAUSE 的内容是从 Deployment 的 kubernetes.io/change-cause 注解复制过来的. 复制动作发生在修订版本创建时.你可以通过以下方式设置 CHANGE-CAUSE 消息:

  * 使用 kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1" 为 Deployment 添加注解.
  * 追加 --record 命令行标志以保存正在更改资源的 kubectl 命令.
  * 手动编辑资源的清单.

2. 要查看修订历史的详细信息,运行:

kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2

输出类似于:

deployments "nginx-deployment" revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1 --record=true
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      <none>
  No volumes.

* 回滚到之前的修订版本 
按照下面给出的步骤将 Deployment 从当前版本回滚到以前的版本(即版本 2)

1. 假定现在你已决定撤消当前上线并回滚到以前的修订版本:

  kubectl rollout undo deployment.v1.apps/nginx-deployment

输出类似于:

deployment.apps/nginx-deployment

或者,你也可以通过使用 --to-revision 来回滚到特定修订版本:

kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2

输出类似于:

deployment.apps/nginx-deployment

与回滚相关的指令的更详细信息,请参考 kubectl rollout.
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout

现在,Deployment 正在回滚到以前的稳定版本.正如你所看到的,Deployment 控制器生成了 回滚到修订版本 2 的 DeploymentRollback 事件.


** 缩放 Deployment 
你可以使用如下指令缩放 Deployment:

kubectl scale deployment.v1.apps/nginx-deployment --replicas=10

输出类似于:

deployment.apps/nginx-deployment scaled

假设集群启用了Pod 的水平自动缩放, 你可以为 Deployment 设置自动缩放器,并基于现有 Pods 的 CPU 利用率选择 要运行的 Pods 个数下限和上限.

kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80

输出类似于:

deployment.apps/nginx-deployment scaled


* 比例缩放 

RollingUpdate(滚动升级) 的 Deployment 支持同时运行应用程序的多个版本. 当自动缩放器缩放处于上线进程(仍在进行中或暂停)中的 RollingUpdate Deployment 时,Deployment 控制器会平衡现有的活跃状态的 ReplicaSets(含 Pods 的 ReplicaSets)中的额外副本,以降低风险.这称为 比例缩放(Proportional Scaling).

在官方的例子中,描述一种情况,运行一个 10 个副本的 Deployment,其 maxSurge=3,maxUnavailable=2

使用新的镜像更新 pod,在未更新完毕的情况下,又做了扩容的操作,将 10 个副本扩到 15个, Deployment 控制器需要决定在何处添加 5 个新副本.如果未使用比例缩放,所有 5 个副本 都将添加到新的 ReplicaSet 中.使用比例缩放时,可以将额外的副本(这里是 5)分布到所有 ReplicaSet. 较大比例的副本会被添加到拥有最多副本的 ReplicaSet,而较低比例的副本会进入到 副本较少的 ReplicaSet.所有剩下的副本都会添加到副本最多的 ReplicaSet. 具有零副本的 ReplicaSets 不会被扩容.

官方的例子不好理解,使用如下的例子

* 比例缩放例子:
#################### start #############################
################################################* 

比例缩放指的是在上线 Deployment 时,临时运行着应用程序的多个版本(共存),比例缩放是控制上线时多个 Pod 服务可用数量的方式.

水平缩放只关心最终的期望 Pod 数量,直接修改副本数和水平缩放,决定最终 Pod 数量有多少个.

而比例缩放是控制对象上线过程中,新的 Pod 创建速度和 旧的 Pod 销毁速度、 Pod 的可用程度,跟上线过程中新旧版本的 Pod 替换数量有关.

创建的 Deployment 的部分 YAML 如下:

spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate

strategy 可以配置 Pod 是怎么更新的.

当我们设置.spec.strategy.type==RollingUpdate时,便会采取滚动更新的方式更新 Pods,此时可以指定 maxUnavailable 和 maxSurge 来控制滚动更新 过程.这个我们之前提到过,就是 Deployment 默认会保证一直有 75% 的 pod处于可用状态,在完成更新前可能有多个版本的 pod 共存.

 * maxUnavailable
    最大不可用数量或比例,旧的 Pod 会以这个数量或比例逐渐减少.

 * maxSurge
    最大峰值,新的 Pod 会按照这个数量或比例逐渐创建.


我们查看之前的 Deployment,执行命令 kubectl get deployment nginx -o yaml:

... ...
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
... ...
 配置表示,每次只有 1/4 的 Pod 被更新、替换.

这个是所有 Deployment 的默认配置,在更新镜像版本时,旧的 Pod 会被新的 Pod 替换,但是不是一下子完成的,每次处理 25% 的 Pod,在更新过程中,我们必须保证我们的服务依然可用,即还有旧版本的 Pod 在运行.这个配置设定了更新过程中至少保证 75% 的 Pod 还可以使用,这个就是比例缩放.

下面我们来进行实验.

首先创建新的 Deployment ,设置副本数量为 10:

kubectl create deployment nginx --image=nginx:1.19.0 --replicas=10
# kubectl scale deployment nginx --replicas=10

我们执行 kubectl edit deployment nginx 修改缩放个数:

  strategy:
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 2
    type: RollingUpdate
 
  除了可用百分比表示,也可以使用个数表示.
 旧的 Pod 按照最大 2 个的速度不断减少;新的 Pod 按照最大 3 个的速度不断增加;

比例缩放的配置处理好了,它会在我们上线新版本的时候生效,我们可以观察到这个过程,但是需要快一点执行命令查看状态.

快速执行以下命令:

kubectl set image deployment nginx nginx=nginx:1.20.0
kubectl get replicaset
root@instance-1:~# kubectl set image deployment nginx nginx=nginx:1.20.0
deployment.apps/nginx image updated
root@instance-1:~# kubectl get replicaset
NAME               DESIRED   CURRENT   READY   AGE
nginx-7b87485749   5         5         0       93m
nginx-85b45874d9   0         0         0       93m
nginx-bb957bbb5    8         8         8       35m
 
因为允许新的 Pod 创建较快(3个),所以最终可能新的 Pod 数量达到 10 个了,旧的 Pod 还有很多,总数量大于 10.

最终:

NAME               DESIRED   CURRENT   READY   AGE
nginx-7b87485749   10        10        10      99m
nginx-85b45874d9   0         0         0       99m
nginx-bb957bbb5    0         0         0       41m

如果想新版本的 Pod 上线速度更快,则可以把 maxSurge 数量或比例设置大一些;为了保证上线过程稳定、服务可用程度高,可以把 maxUnavailable 设置小一些.

################################################*
#################### end ##############################


** 暂停、恢复 Deployment 

你可以在触发一个或多个更新之前暂停 Deployment,然后再恢复其执行. 这样做使得你能够在暂停和恢复执行之间应用多个修补程序,而不会触发不必要的上线操作.

* 例如,对于一个刚刚创建的 Deployment:获取 Deployment 信息:

kubectl get deploy

输出类似于:

NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m

获取上线状态:

kubectl get rs

输出类似于:

NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m

* 使用如下指令暂停运行:

kubectl rollout pause deployment.v1.apps/nginx-deployment

输出类似于:

deployment.apps/nginx-deployment paused

* 接下来更新 Deployment 镜像:

kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.16.1

输出类似于:

deployment.apps/nginx-deployment image updated

* 注意没有新的上线被触发:

kubectl rollout history deployment.v1.apps/nginx-deployment

输出类似于:

deployments "nginx"
REVISION  CHANGE-CAUSE
1   <none>

* 获取上线状态确保 Deployment 更新已经成功:

kubectl get rs

输出类似于:

NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m

* 你可以根据需要执行很多更新操作,例如,可以要使用的资源:

kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi

输出类似于:

deployment.apps/nginx-deployment resource requirements updated

暂停 Deployment 之前的初始状态将继续发挥作用,但新的更新在 Deployment 被 暂停期间不会产生任何效果.

* 最终,恢复 Deployment 执行并观察新的 ReplicaSet 的创建过程,其中包含了所应用的所有更新:

kubectl rollout resume deployment.v1.apps/nginx-deployment

输出:

deployment.apps/nginx-deployment resumed

* 观察上线的状态,直到完成.

kubectl get rs -w

输出类似于:

NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s

* 获取最近上线的状态:

kubectl get rs

输出类似于:

NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s

说明:你不可以回滚处于暂停状态的 Deployment,除非先恢复其执行状态.


** Deployment 状态 
Deployment 的生命周期中会有许多状态.上线新的 ReplicaSet 期间可能处于 Progressing(进行中),可能是 Complete(已完成),也可能是 Failed(失败)以至于无法继续进行.

* 进行中的 Deployment 
执行下面的任务期间,Kubernetes 标记 Deployment 为 进行中(Progressing):

  * Deployment 创建新的 ReplicaSet
  * Deployment 正在为其最新的 ReplicaSet 扩容
  * Deployment 正在为其旧有的 ReplicaSet(s) 缩容
  * 新的 Pods 已经就绪或者可用(就绪至少持续了 MinReadySeconds 秒).

你可以使用 kubectl rollout status 监视 Deployment 的进度.

* 完成的 Deployment 
当 Deployment 具有以下特征时,Kubernetes 将其标记为 完成(Complete):

  * 与 Deployment 关联的所有副本都已更新到指定的最新版本,这意味着之前请求的所有更新都已完成.
  * 与 Deployment 关联的所有副本都可用.
  * 未运行 Deployment 的旧副本.

你可以使用 kubectl rollout status 检查 Deployment 是否已完成. 如果上线成功完成,kubectl rollout status 返回退出代码 0.

kubectl rollout status deployment/nginx-deployment
输出类似于:

Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment "nginx-deployment" successfully rolled out
$ echo $?
0

* 失败的 Deployment 
你的 Deployment 可能会在尝试部署其最新的 ReplicaSet 受挫,一直处于未完成状态. 造成此情况一些可能因素如下:

  * 配额(Quota)不足
  * 就绪探测(Readiness Probe)失败
  * 镜像拉取错误
  * 权限不足
  * 限制范围(Limit Ranges)问题
  * 应用程序运行时的配置错误

检测此状况的一种方法是在 Deployment 规约中指定截止时间参数:([.spec.progressDeadlineSeconds](#progress-deadline-seconds)). .spec.progressDeadlineSeconds 给出的是一个秒数值,Deployment 控制器在(通过 Deployment 状态) 标示 Deployment 进展停滞之前,需要等待所给的时长.

以下 kubectl 命令设置规约中的 progressDeadlineSeconds,从而告知控制器 在 10 分钟后报告 Deployment 没有进展:

kubectl patch deployment.v1.apps/nginx-deployment -p '{"spec":{"progressDeadlineSeconds":600}}'

输出类似于:

deployment.apps/nginx-deployment patched

超过截止时间后,Deployment 控制器将添加具有以下属性的 DeploymentCondition 到 Deployment 的 .status.conditions 中:

  * Type=Progressing
  * Status=False
  * Reason=ProgressDeadlineExceeded

Deployment 可能会出现瞬时性的错误,可能因为设置的超时时间过短, 也可能因为其他可认为是临时性的问题.例如,假定所遇到的问题是配额不足. 如果描述 Deployment,你将会注意到以下部分:

https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/#proportional-scaling

* 对失败 Deployment 的操作 
可应用于已完成的 Deployment 的所有操作也适用于失败的 Deployment. 你可以对其执行扩缩容、回滚到以前的修订版本等操作,或者在需要对 Deployment 的 Pod 模板应用多项调整时,将 Deployment 暂停.


** 清理策略 
你可以在 Deployment 中设置 .spec.revisionHistoryLimit 字段以指定保留此 Deployment 的多少个旧有 ReplicaSet.其余的 ReplicaSet 将在后台被垃圾回收. 默认情况下,此值为 10.

说明:显式将此字段设置为 0 将导致 Deployment 的所有历史记录被清空,因此 Deployment 将无法回滚.


** 编写 Deployment 规约 
同其他 Kubernetes 配置一样, Deployment 需要 apiVersion,kind 和 metadata 字段.

 Deployment 还需要 .spec 部分.

* Pod 模板 
.spec 中只有 .spec.template 和 .spec.selector 是必需的字段.

.spec.template 是一个 Pod 模板. 它和 Pod 的语法规则完全相同. 只是这里它是嵌套的,因此不需要 apiVersion 或 kind.

除了 Pod 的必填字段外,Deployment 中的 Pod 模板必须指定适当的标签和适当的重新启动策略. 对于标签,请确保不要与其他控制器重叠.请参考选择算符.

只有 .spec.template.spec.restartPolicy 等于 Always 才是被允许的,这也是在没有指定时的默认设置.

* 选择算符 
.spec.selector 是指定本 Deployment 的 Pod 标签选择算符的必需字段.

.spec.selector 必须匹配 .spec.template.metadata.labels,否则请求会被 API 拒绝.

在 API apps/v1版本中,.spec.selector 和 .metadata.labels 如果没有设置的话, 不会被默认设置为 .spec.template.metadata.labels,所以需要明确进行设置. 同时在 apps/v1版本中,Deployment 创建后 .spec.selector 是不可变的.

当 Pod 的标签和选择算符匹配,但其模板和 .spec.template 不同时,或者此类 Pod 的总数超过 .spec.replicas 的设置时,Deployment 会终结之. 如果 Pods 总数未达到期望值,Deployment 会基于 .spec.template 创建新的 Pod.

说明: 你不应直接创建、或者通过创建另一个 Deployment,或者创建类似 ReplicaSet 或 ReplicationController 这类控制器来创建标签与此选择算符匹配的 Pod. 如果这样做,第一个 Deployment 会认为它创建了这些 Pod. Kubernetes 不会阻止你这么做.
如果有多个控制器的选择算符发生重叠,则控制器之间会因冲突而无法正常工作.

* 策略 
.spec.strategy 策略指定用于用新 Pods 替换旧 Pods 的策略. .spec.strategy.type 可以是 "Recreate" 或 "RollingUpdate"."RollingUpdate" 是默认值.

* 重新创建 Deployment 
如果 .spec.strategy.type==Recreate,在创建新 Pods 之前,所有现有的 Pods 会被杀死.

* 滚动更新 Deployment 
Deployment 会在 .spec.strategy.type==RollingUpdate时,采取 滚动更新的方式更新 Pods.你可以指定 maxUnavailable 和 maxSurge 来控制滚动更新 过程.

* 最大不可用 
.spec.strategy.rollingUpdate.maxUnavailable 是一个可选字段,用来指定 更新过程中不可用的 Pod 的个数上限.该值可以是绝对数字(例如,5),也可以是 所需 Pods 的百分比(例如,10%).百分比值会转换成绝对数并去除小数部分. 如果 .spec.strategy.rollingUpdate.maxSurge 为 0,则此值不能为 0. 默认值为 25%.

例如,当此值设置为 30% 时,滚动更新开始时会立即将旧 ReplicaSet 缩容到期望 Pod 个数的70%. 新 Pod 准备就绪后,可以继续缩容旧有的 ReplicaSet,然后对新的 ReplicaSet 扩容,确保在更新期间 可用的 Pods 总数在任何时候都至少为所需的 Pod 个数的 70%.

* 最大峰值 
.spec.strategy.rollingUpdate.maxSurge 是一个可选字段,用来指定可以创建的超出 期望 Pod 个数的 Pod 数量.此值可以是绝对数(例如,5)或所需 Pods 的百分比(例如,10%). 如果 MaxUnavailable 为 0,则此值不能为 0.百分比值会通过向上取整转换为绝对数. 此字段的默认值为 25%.

例如,当此值为 30% 时,启动滚动更新后,会立即对新的 ReplicaSet 扩容,同时保证新旧 Pod 的总数不超过所需 Pod 总数的 130%.一旦旧 Pods 被杀死,新的 ReplicaSet 可以进一步扩容, 同时确保更新期间的任何时候运行中的 Pods 总数最多为所需 Pods 总数的 130%.

* 进度期限秒数 
.spec.progressDeadlineSeconds 是一个可选字段,用于指定系统在报告 Deployment 进展失败 之前等待 Deployment 取得进展的秒数. 这类报告会在资源状态中体现为 Type=Progressing、Status=False、 Reason=ProgressDeadlineExceeded.Deployment 控制器将持续重试 Deployment. 将来,一旦实现了自动回滚,Deployment 控制器将在探测到这样的条件时立即回滚 Deployment.

如果指定,则此字段值需要大于 .spec.minReadySeconds 取值

* 最短就绪时间 
.spec.minReadySeconds 是一个可选字段,用于指定新创建的 Pod 在没有任意容器崩溃情况下的最小就绪时间, 只有超出这个时间 Pod 才被视为可用.默认值为 0(Pod 在准备就绪后立即将被视为可用). 要了解何时 Pod 被视为就绪,可参考容器探针.

* 修订历史限制
Deployment 的修订历史记录存储在它所控制的 ReplicaSets 中.

.spec.revisionHistoryLimit 是一个可选字段,用来设定出于会滚目的所要保留的旧 ReplicaSet 数量. 这些旧 ReplicaSet 会消耗 etcd 中的资源,并占用 kubectl get rs 的输出. 每个 Deployment 修订版本的配置都存储在其 ReplicaSets 中;因此,一旦删除了旧的 ReplicaSet, 将失去回滚到 Deployment 的对应修订版本的能力. 默认情况下,系统保留 10 个旧 ReplicaSet,但其理想值取决于新 Deployment 的频率和稳定性.

更具体地说,将此字段设置为 0 意味着将清理所有具有 0 个副本的旧 ReplicaSet. 在这种情况下,无法撤消新的 Deployment 上线,因为它的修订历史被清除了.

* paused(暂停的) 
.spec.paused 是用于暂停和恢复 Deployment 的可选布尔字段. 暂停的 Deployment 和未暂停的 Deployment 的唯一区别是,Deployment 处于暂停状态时, PodTemplateSpec 的任何修改都不会触发新的上线. Deployment 在创建时是默认不会处于暂停状态.



### ReplicaSet
ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合. 因此,它通常用来保证给定数量的、完全相同的 Pod 的可用性.

ReplicaSet 的工作原理
ReplicaSet 是通过一组字段来定义的,包括一个用来识别可获得的 Pod 的集合的选择算符、一个用来标明应该维护的副本个数的数值、一个用来指定应该创建新 Pod 以满足副本个数条件时要使用的 Pod 模板等等. 每个 ReplicaSet 都通过根据需要创建和 删除 Pod 以使得副本个数达到期望值, 进而实现其存在价值.当 ReplicaSet 需要创建新的 Pod 时,会使用所提供的 Pod 模板.

ReplicaSet 通过 Pod 上的 metadata.ownerReferences 字段连接到附属 Pod,该字段给出当前对象的属主资源. ReplicaSet 所获得的 Pod 都在其 ownerReferences 字段中包含了属主 ReplicaSet 的标识信息.正是通过这一连接,ReplicaSet 知道它所维护的 Pod 集合的状态, 并据此计划其操作行为.

ReplicaSet 使用其选择算符来辨识要获得的 Pod 集合.如果某个 Pod 没有 OwnerReference 或者其 OwnerReference 不是一个 控制器,且其匹配到 某 ReplicaSet 的选择算符,则该 Pod 立即被此 ReplicaSet 获得.

何时使用 ReplicaSet
ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行. 然而,Deployment 是一个更高级的概念,它管理 ReplicaSet,并向 Pod 提供声明式的更新以及许多其他有用的功能. 因此,我们建议使用 Deployment 而不是直接使用 ReplicaSet,除非 你需要自定义更新业务流程或根本不需要更新.

这实际上意味着,你可能永远不需要操作 ReplicaSet 对象:而是使用 Deployment,并在 spec 部分定义你的应用.

在一个 pods 的集合中,包含不同的 pod, 如下: 但并非正规的用法
https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicaset/

效果如下:

kubectl get pods

将会生成下面的输出:

NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s

一个 ReplicaSet 中可以包含异质的 Pods 集合.


** 使用 ReplicaSets

* 删除 ReplicaSet 和它的 Pod
要删除 ReplicaSet 和它的所有 Pod,使用 kubectl delete 命令. 默认情况下,垃圾收集器 自动删除所有依赖的 Pod.

当使用 REST API 或 client-go 库时,你必须在删除选项中将 propagationPolicy 设置为 Background 或 Foreground.例如:

kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
   -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
   -H "Content-Type: application/json"

* 只删除 ReplicaSet
你可以只删除 ReplicaSet 而不影响它的 Pods,方法是使用 kubectl delete 命令并设置 --cascade=orphan 选项.

当使用 REST API 或 client-go 库时,你必须将 propagationPolicy 设置为 Orphan. 例如:

kubectl proxy --port=8080
curl -X DELETE  'localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend' \
  -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \
  -H "Content-Type: application/json"

一旦删除了原来的 ReplicaSet,就可以创建一个新的来替换它. 由于新旧 ReplicaSet 的 .spec.selector 是相同的,新的 ReplicaSet 将接管老的 Pod. 但是,它不会努力使现有的 Pod 与新的、不同的 Pod 模板匹配. 若想要以可控的方式更新 Pod 的规约,可以使用 Deployment 资源,因为 ReplicaSet 并不直接支持滚动更新.

* 将 Pod 从 ReplicaSet 中隔离
可以通过改变标签来从 ReplicaSet 的目标集中移除 Pod. 这种技术可以用来从服务中去除 Pod,以便进行排错、数据恢复等. 以这种方式移除的 Pod 将被自动替换(假设副本的数量没有改变).

* 缩放 RepliaSet
通过更新 .spec.replicas 字段,ReplicaSet 可以被轻松的进行缩放.ReplicaSet 控制器能确保匹配标签选择器的数量的 Pod 是可用的和可操作的.

在降低集合规模时,ReplicaSet 控制器通过对可用的 Pods 进行排序来优先选择 要被删除的 Pods.其一般性算法如下:

  1. 首先选择剔除悬决(Pending,且不可调度)的 Pods
  2. 如果设置了 controller.kubernetes.io/pod-deletion-cost 注解,则注解值 较小的优先被裁减掉
  3. 所处节点上副本个数较多的 Pod 优先于所处节点上副本较少者
  4. 如果 Pod 的创建时间不同,最近创建的 Pod 优先于早前创建的 Pod 被裁减. (当 LogarithmicScaleDown 这一 特性门控 被启用时,创建时间是按整数幂级来分组的).

如果以上比较结果都相同,则随机选择.

* ReplicaSet 作为水平的 Pod 自动缩放器目标
ReplicaSet 也可以作为 水平的 Pod 缩放器 (HPA) 的目标.也就是说,ReplicaSet 可以被 HPA 自动缩放. 以下是 HPA 以我们在前一个示例中创建的副本集为目标的示例.

controllers/hpa-rs.yaml 

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50

将这个列表保存到 hpa-rs.yaml 并提交到 Kubernetes 集群,就能创建它所定义的 HPA,进而就能根据复制的 Pod 的 CPU 利用率对目标 ReplicaSet进行自动缩放.

kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml

或者,可以使用 kubectl autoscale 命令完成相同的操作. (而且它更简单！)

kubectl autoscale rs frontend --max=10 --min=3 --cpu-percent=50


** ReplicaSet 的替代方案
Deployment (推荐)
Deployment 是一个 可以拥有 ReplicaSet 并使用声明式方式在服务器端完成对 Pods 滚动更新的对象. 尽管 ReplicaSet 可以独立使用,目前它们的主要用途是提供给 Deployment 作为 编排 Pod 创建、删除和更新的一种机制.当使用 Deployment 时,你不必关心 如何管理它所创建的 ReplicaSet,Deployment 拥有并管理其 ReplicaSet. 因此,建议你在需要 ReplicaSet 时使用 Deployment.

裸 Pod
与用户直接创建 Pod 的情况不同,ReplicaSet 会替换那些由于某些原因被删除或被终止的 Pod,例如在节点故障或破坏性的节点维护(如内核升级)的情况下. 因为这个原因,我们建议你使用 ReplicaSet,即使应用程序只需要一个 Pod. 想像一下,ReplicaSet 类似于进程监视器,只不过它在多个节点上监视多个 Pod, 而不是在单个节点上监视单个进程. ReplicaSet 将本地容器重启的任务委托给了节点上的某个代理(例如,Kubelet 或 Docker)去完成.

Job
使用Job 代替ReplicaSet, 可以用于那些期望自行终止的 Pod.

DaemonSet
对于管理那些提供主机级别功能(如主机监控和主机日志)的容器, 就要用 DaemonSet 而不用 ReplicaSet. 这些 Pod 的寿命与主机寿命有关:这些 Pod 需要先于主机上的其他 Pod 运行, 并且在机器准备重新启动/关闭时安全地终止.

ReplicationController
ReplicaSet 是 ReplicationController 的后继者.二者目的相同且行为类似,只是 ReplicationController 不支持 标签用户指南 中讨论的基于集合的选择算符需求. 因此,相比于 ReplicationController,应优先考虑 ReplicaSet.

### StatefulSets
StatefulSet 是用来管理有状态应用的工作负载 API 对象.

StatefulSet 用来管理某 Pod 集合的部署和扩缩, 并为这些 Pod 提供持久存储和持久标识符.

和 Deployment 类似, StatefulSet 管理基于相同容器规约的一组 Pod.但和 Deployment 不同的是, StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID.这些 Pod 是基于相同的规约来创建的, 但是不能相互替换:无论怎么调度,每个 Pod 都有一个永久不变的 ID.

如果希望使用存储卷为工作负载提供持久存储,可以使用 StatefulSet 作为解决方案的一部分.尽管 StatefulSet 中的单个 Pod 仍可能出现故障, 但持久的 Pod 标识符使得将现有卷与替换已失败 Pod 的新 Pod 相匹配变得更加容易.


** 使用 StatefulSets
StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值:

  * 稳定的、唯一的网络标识符.
  * 稳定的、持久的存储.
  * 有序的、优雅的部署和缩放.
  * 有序的、自动的滚动更新.

在上面描述中,"稳定的"意味着 Pod 调度或重调度的整个过程是有持久性的.如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩,则应该使用 由一组无状态的副本控制器提供的工作负载来部署应用程序,比如 Deployment 或者 ReplicaSet 可能更适用于你的无状态应用部署需要.


** 限制 
  * 给定 Pod 的存储必须由 PersistentVolume 驱动 基于所请求的 storage class 来提供,或者由管理员预先提供.
  * 删除或者收缩 StatefulSet 并不会删除它关联的存储卷.这样做是为了保证数据安全,它通常比自动清除 StatefulSet 所有相关的资源更有价值.
  * StatefulSet 当前需要无头服务 来负责 Pod 的网络标识.你需要负责创建此服务.
  * 当删除 StatefulSets 时,StatefulSet 不提供任何终止 Pod 的保证.为了实现 StatefulSet 中的 Pod 可以有序地且体面地终止,可以在删除之前将 StatefulSet 缩放为 0.
  * 在默认 Pod 管理策略(OrderedReady) 时使用 滚动更新,可能进入需要人工干预 才能修复的损坏状态.


** 组件 
下面的示例演示了 StatefulSet 的组件.

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi

上述例子中:

  * 名为 nginx 的 Headless Service 用来控制网络域名.
  * 名为 web 的 StatefulSet 有一个 Spec,它表明将在独立的 3 个 Pod 副本中启动 nginx 容器.
  * volumeClaimTemplates 将通过 PersistentVolumes 驱动提供的 PersistentVolumes 来提供稳定的存储.

StatefulSet 的命名需要遵循DNS 子域名规范.


** Pod 选择算符 
你必须设置 StatefulSet 的 .spec.selector 字段,使之匹配其在 .spec.template.metadata.labels 中设置的标签.在 Kubernetes 1.8 版本之前, 被忽略 .spec.selector 字段会获得默认设置值.在 1.8 和以后的版本中,未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误.


** Pod 标识 
StatefulSet Pod 具有唯一的标识,该标识包括顺序标识、稳定的网络标识和稳定的存储.该标识和 Pod 是绑定的,不管它被调度在哪个节点上.

* 有序索引 
对于具有 N 个副本的 StatefulSet,StatefulSet 中的每个 Pod 将被分配一个整数序号, 从 0 到 N-1,该序号在 StatefulSet 上是唯一的.

* 稳定的网络 ID 
StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名.组合主机名的格式为$(StatefulSet 名称)-$(序号).上例将会创建三个名称分别为 web-0、web-1、web-2 的 Pod.StatefulSet 可以使用 无头服务 控制它的 Pod 的网络域.管理域的这个服务的格式为: $(服务名称).$(命名空间).svc.cluster.local,其中 cluster.local 是集群域.一旦每个 Pod 创建成功,就会得到一个匹配的 DNS 子域,格式为: $(pod 名称).$(所属服务的 DNS 域名),其中所属服务由 StatefulSet 的 serviceName 域来设定.

正如限制中所述,你需要负责创建无头服务 以便为 Pod 提供网络标识.

下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例:

集群域名	    服务(名字空间/名字)	StatefulSet(名字空间/名字)	StatefulSet 域名	                        Pod DNS	                                                          Pod 主机名
cluster.local	default/nginx	                  default/web	                                nginx.default.svc.cluster.local	    web-{0..N-1}.nginx.default.svc.cluster.local	  web-{0..N-1}
cluster.local	foo/nginx	                        foo/web	                                      nginx.foo.svc.cluster.local	          web-{0..N-1}.nginx.foo.svc.cluster.local	        web-{0..N-1}
kube.local	    foo/nginx	                        foo/web	                                      nginx.foo.svc.kube.local	              web-{0..N-1}.nginx.foo.svc.kube.local	            web-{0..N-1}

说明: 集群域会被设置为 cluster.local,除非有其他配置.

* 稳定的存储 
对于 StatefulSet 中定义的每个 VolumeClaimTemplate,每个 Pod 接收到一个 PersistentVolumeClaim.在上面的 nginx 示例中,每个 Pod 将会得到基于 StorageClass my-storage-class 提供的 1 Gib 的 PersistentVolume.如果没有声明 StorageClass,就会使用默认的 StorageClass.当一个 Pod 被调度(重新调度)到节点上时,它的 volumeMounts 会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume.请注意,当 Pod 或者 StatefulSet 被删除时,与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除.要删除它必须通过手动方式来完成.

* Pod 名称标签 
当 StatefulSet 控制器(Controller) 创建 Pod 时, 它会添加一个标签 statefulset.kubernetes.io/pod-name,该标签值设置为 Pod 名称.这个标签允许你给 StatefulSet 中的特定 Pod 绑定一个 Service.


** 部署和扩缩保证 
对于包含 N 个 副本的 StatefulSet,当部署 Pod 时,它们是依次创建的,顺序为 0..N-1.
当删除 Pod 时,它们是逆序终止的,顺序为 N-1..0.
在将缩放操作应用到 Pod 之前,它前面的所有 Pod 必须是 Running 和 Ready 状态.
在 Pod 终止之前,所有的继任者必须完全关闭.
StatefulSet 不应将 pod.Spec.TerminationGracePeriodSeconds 设置为 0.这种做法是不安全的,要强烈阻止.更多的解释请参考 强制删除 StatefulSet Pod.

* Pod 管理策略
在 Kubernetes 1.7 及以后的版本中,StatefulSet 允许你放宽其排序保证, 同时通过它的 .spec.podManagementPolicy 域保持其唯一性和身份保证.


** 更新策略 
StatefulSet 的 .spec.updateStrategy 字段让 你可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解.有两个允许的值:

  OnDelete
  当 StatefulSet 的 .spec.updateStrategy.type 设置为 OnDelete 时, 它的控制器将不会自动更新 StatefulSet 中的 Pod.用户必须手动删除 Pod 以便让控制器创建新的 Pod,以此来对 StatefulSet 的 .spec.template 的变动作出反应.
  RollingUpdate
  RollingUpdate 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新.这是默认的更新策略.


** 滚动更新
当 StatefulSet 的 .spec.updateStrategy.type 被设置为 RollingUpdate 时, StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod.它将按照与 Pod 终止相同的顺序(从最大序号到最小序号)进行,每次更新一个 Pod.

Kubernetes 控制面会等到被更新的 Pod 进入 Running 和 Ready 状态,然后再更新其前身.如果你设置了 .spec.minReadySeconds(查看最短就绪秒数),控制面在 Pod 就绪后会额外等待一定的时间再执行下一步.

* 分区滚动更新 
通过声明 .spec.updateStrategy.rollingUpdate.partition 的方式,RollingUpdate 更新策略可以实现分区.如果声明了一个分区,当 StatefulSet 的 .spec.template 被更新时, 所有序号大于等于该分区序号的 Pod 都会被更新.所有序号小于该分区序号的 Pod 都不会被更新,并且,即使他们被删除也会依据之前的版本进行重建.如果 StatefulSet 的 .spec.updateStrategy.rollingUpdate.partition 大于它的 .spec.replicas,对它的 .spec.template 的更新将不会传递到它的 Pod.在大多数情况下,你不需要使用分区,但如果你希望进行阶段更新、执行金丝雀或执行 分阶段上线,则这些分区会非常有用.

* 强制回滚
在默认 Pod 管理策略(OrderedReady) 下使用 滚动更新 ,可能进入需要人工干预才能修复的损坏状态.

如果更新后 Pod 模板配置进入无法运行或就绪的状态(例如,由于错误的二进制文件 或应用程序级配置错误),StatefulSet 将停止回滚并等待.

在这种状态下,仅将 Pod 模板还原为正确的配置是不够的.由于 已知问题,StatefulSet 将继续等待损坏状态的 Pod 准备就绪(永远不会发生),然后再尝试将其恢复为正常工作配置.

恢复模板后,还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod.这样, StatefulSet 才会开始使用被还原的模板来重新创建 Pod.

* 最短就绪秒数 
FEATURE STATE: Kubernetes v1.22 [alpha]
.spec.minReadySeconds 是一个可选字段,用于指定新创建的 Pod 就绪(没有任何容器崩溃)后被认为可用的最小秒数.默认值是 0(Pod 就绪时就被认为可用).要了解 Pod 何时被认为已就绪,请参阅容器探针.

请注意只有当你启用 StatefulSetMinReadySeconds 特性门控时,该字段才会生效.


### DaemonSet
DaemonSet 确保全部(或者某些)节点上运行一个 Pod 的副本.当有节点加入集群时, 也会为他们新增一个 Pod .当有节点从集群移除时,这些 Pod 也会被回收.删除 DaemonSet 将会删除它创建的所有 Pod.

DaemonSet 的一些典型用法:

在每个节点上运行集群守护进程
在每个节点上运行日志收集守护进程
在每个节点上运行监控守护进程
一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet.一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet;每个具有不同的标志, 并且对不同硬件类型具有不同的内存、CPU 要求.


** 编写 DaemonSet Spec 
* 创建 DaemonSet 
你可以在 YAML 文件中描述 DaemonSet.例如,下面的 daemonset.yaml 文件描述了一个运行 fluentd-elasticsearch Docker 镜像的 DaemonSet:

controllers/daemonset.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # this toleration is to have the daemonset runnable on master nodes
      # remove it if your masters can't run pods
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

基于 YAML 文件创建 DaemonSet:

kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml

* 必需字段 
和所有其他 Kubernetes 配置一样,DaemonSet 需要 apiVersion、kind 和 metadata 字段.有关配置文件的基本信息,参见 部署应用、 配置容器和 使用 kubectl 进行对象管理 文档.

DaemonSet 对象的名称必须是一个合法的 DNS 子域名.

DaemonSet 也需要一个 .spec 配置段.

* Pod 模板 
.spec 中唯一必需的字段是 .spec.template.

.spec.template 是一个 Pod 模板.除了它是嵌套的,因而不具有 apiVersion 或 kind 字段之外,它与 Pod 具有相同的 schema.

除了 Pod 必需字段外,在 DaemonSet 中的 Pod 模板必须指定合理的标签(查看 Pod 选择算符).

在 DaemonSet 中的 Pod 模板必须具有一个值为 Always 的 RestartPolicy.当该值未指定时,默认是 Always.

* Pod 选择算符 
.spec.selector 字段表示 Pod 选择算符,它与 Job 的 .spec.selector 的作用是相同的.

从 Kubernetes 1.8 开始,您必须指定与 .spec.template 的标签匹配的 Pod 选择算符.用户不指定 Pod 选择算符时,该字段不再有默认值.选择算符的默认值生成结果与 kubectl apply 不兼容.此外,一旦创建了 DaemonSet,它的 .spec.selector 就不能修改.修改 Pod 选择算符可能导致 Pod 意外悬浮,并且这对用户来说是费解的.

spec.selector 是一个对象,如下两个字段组成:

  * matchLabels - 与 ReplicationController 的 .spec.selector 的作用相同.
  * matchExpressions - 允许构建更加复杂的选择器,可以通过指定 key、value 列表以及将 key 和 value 列表关联起来的 operator.

当上述两个字段都指定时,结果会按逻辑与(AND)操作处理.

如果指定了 .spec.selector,必须与 .spec.template.metadata.labels 相匹配.如果与后者不匹配,则 DeamonSet 会被 API 拒绝.

* 仅在某些节点上运行 Pod 
如果指定了 .spec.template.spec.nodeSelector,DaemonSet 控制器将在能够与 Node 选择算符 匹配的节点上创建 Pod.类似这种情况,可以指定 .spec.template.spec.affinity,之后 DaemonSet 控制器 将在能够与节点亲和性 匹配的节点上创建 Pod.如果根本就没有指定,则 DaemonSet Controller 将在所有节点上创建 Pod.


** 与 Daemon Pods 通信 
与 DaemonSet 中的 Pod 进行通信的几种可能模式如下:

推送(Push):配置 DaemonSet 中的 Pod,将更新发送到另一个服务,例如统计数据库.这些服务没有客户端.

NodeIP 和已知端口:DaemonSet 中的 Pod 可以使用 hostPort,从而可以通过节点 IP 访问到 Pod.客户端能通过某种方法获取节点 IP 列表,并且基于此也可以获取到相应的端口.

DNS:创建具有相同 Pod 选择算符的 无头服务, 通过使用 endpoints 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet.

Service:创建具有相同 Pod 选择算符的服务,并使用该服务随机访问到某个节点上的 守护进程(没有办法访问到特定节点).


** 更新 DaemonSet 
如果节点的标签被修改,DaemonSet 将立刻向新匹配上的节点添加 Pod, 同时删除不匹配的节点上的 Pod.

你可以修改 DaemonSet 创建的 Pod.不过并非 Pod 的所有字段都可更新.下次当某节点(即使具有相同的名称)被创建时,DaemonSet 控制器还会使用最初的模板.

您可以删除一个 DaemonSet.如果使用 kubectl 并指定 --cascade=orphan 选项, 则 Pod 将被保留在节点上.接下来如果创建使用相同选择算符的新 DaemonSet, 新的 DaemonSet 会收养已有的 Pod.如果有 Pod 需要被替换,DaemonSet 会根据其 updateStrategy 来替换.

你可以对 DaemonSet 执行滚动更新操作.


### Jobs
Job 会创建一个或者多个 Pods,并将继续重试 Pods 的执行,直到指定数量的 Pods 成功终止. 随着 Pods 成功结束,Job 跟踪记录成功完成的 Pods 个数. 当数量达到指定的成功个数阈值时,任务(即 Job)结束. 删除 Job 的操作会清除所创建的全部 Pods. 挂起 Job 的操作会删除 Job 的所有活跃 Pod,直到 Job 被再次恢复执行.

一种简单的使用场景下,你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成. 当第一个 Pod 失败或者被删除(比如因为节点硬件失效或者重启)时,Job 对象会启动一个新的 Pod.

你也可以使用 Job 以并行的方式运行多个 Pod.


** 运行示例 Job 
下面是一个 Job 配置示例.它负责计算 π 到小数点后 2000 位,并将结果打印出来. 此计算大约需要 10 秒钟完成.

controllers/job.yaml 

apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4

你可以使用下面的命令来运行此示例:

kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml

输出类似于:

job.batch/pi created

使用 kubectl 来检查 Job 的状态:

kubectl describe jobs/pi
输出类似于:

Name:           pi
Namespace:      default
Selector:       controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                job-name=pi
Annotations:    kubectl.kubernetes.io/last-applied-configuration:
                  {"apiVersion":"batch/v1","kind":"Job","metadata":{"annotations":{},"name":"pi","namespace":"default"},"spec":{"backoffLimit":4,"template":...
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7

要查看 Job 对应的已完成的 Pods,可以执行 kubectl get pods.

要以机器可读的方式列举隶属于某 Job 的全部 Pods,你可以使用类似下面这条命令:

pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}')

echo $pods

输出类似于:

pi-5rwd7

这里,选择算符与 Job 的选择算符相同.--output=jsonpath 选项给出了一个表达式,用来从返回的列表中提取每个 Pod 的 name 字段.

查看其中一个 Pod 的标准输出:

kubectl logs $pods

输出类似于:

3.141 ......


** 编写 Job 规约
与 Kubernetes 中其他资源的配置类似,Job 也需要 apiVersion、kind 和 metadata 字段. Job 的名字必须是合法的 DNS 子域名.

Job 配置还需要一个.spec 节.

* Pod 模版
Job 的 .spec 中只有 .spec.template 是必需的字段.

字段 .spec.template 的值是一个 Pod 模版. 其定义规范与 Pod 完全相同,只是其中不再需要 apiVersion 或 kind 字段.

除了作为 Pod 所必需的字段之外,Job 中的 Pod 模版必需设置合适的标签 (参见Pod 选择算符)和合适的重启策略.

Job 中 Pod 的 RestartPolicy 只能设置为 Never 或 OnFailure 之一.

* Pod 选择算符 
字段 .spec.selector 是可选的.在绝大多数场合,你都不需要为其赋值. 参阅设置自己的 Pod 选择算符.

* Job 的并行执行
适合以 Job 形式来运行的任务主要有三种:

  1. 非并行 Job:
    * 通常只启动一个 Pod,除非该 Pod 失败.
    * 当 Pod 成功终止时,立即视 Job 为完成状态.
  2. 具有 确定完成计数 的并行 Job:
    * .spec.completions 字段设置为非 0 的正数值.
    * Job 用来代表整个任务,当成功的 Pod 个数达到 .spec.completions 时,Job 被视为完成.
    * 当使用 .spec.completionMode="Indexed" 时,每个 Pod 都会获得一个不同的 索引值,介于 0 和 .spec.completions-1 之间.
  3. 带 工作队列 的并行 Job:
    * 不设置 spec.completions,默认值为 .spec.parallelism.
    * 多个 Pod 之间必须相互协调,或者借助外部服务确定每个 Pod 要处理哪个工作条目. 例如,任一 Pod 都可以从工作队列中取走最多 N 个工作条目.
    * 每个 Pod 都可以独立确定是否其它 Pod 都已完成,进而确定 Job 是否完成.
    * 当 Job 中 任何 Pod 成功终止,不再创建新 Pod.
    * 一旦至少 1 个 Pod 成功完成,并且所有 Pod 都已终止,即可宣告 Job 成功完成.
    * 一旦任何 Pod 成功退出,任何其它 Pod 都不应再对此任务执行任何操作或生成任何输出. 所有 Pod 都应启动退出过程.

对于 非并行 的 Job,你可以不设置 spec.completions 和 spec.parallelism. 这两个属性都不设置时,均取默认值 1.

对于 确定完成计数 类型的 Job,你应该设置 .spec.completions 为所需要的完成个数. 你可以设置 .spec.parallelism,也可以不设置.其默认值为 1.

对于一个 工作队列 Job,你不可以设置 .spec.completions,但要将.spec.parallelism 设置为一个非负整数.

控制并行性 
并行性请求(.spec.parallelism)可以设置为任何非负整数. 如果未设置,则默认为 1. 如果设置为 0,则 Job 相当于启动之后便被暂停,直到此值被增加.

实际并行性(在任意时刻运行状态的 Pods 个数)可能比并行性请求略大或略小, 原因如下:

  * 对于 确定完成计数 Job,实际上并行执行的 Pods 个数不会超出剩余的完成数. 如果 .spec.parallelism 值较高,会被忽略.
  * 对于 工作队列 Job,有任何 Job 成功结束之后,不会有新的 Pod 启动. 不过,剩下的 Pods 允许执行完毕.
  * 如果 Job 控制器 没有来得及作出响应,或者
  * 如果 Job 控制器因为任何原因(例如,缺少 ResourceQuota 或者没有权限)无法创建 Pods. Pods 个数可能比请求的数目小.
  * Job 控制器可能会因为之前同一 Job 中 Pod 失效次数过多而压制新 Pod 的创建.
  * 当 Pod 处于体面终止进程中,需要一定时间才能停止.

* 完成模式 
FEATURE STATE: Kubernetes v1.22 [beta]
带有 确定完成计数 的 Job,即 .spec.completions 不为 null 的 Job, 都可以在其 .spec.completionMode 中设置完成模式:

  * NonIndexed(默认值):当成功完成的 Pod 个数达到 .spec.completions 所 设值时认为 Job 已经完成.换言之,每个 Job 完成事件都是独立无关且同质的. 要注意的是,当 .spec.completions 取值为 null 时,Job 被隐式处理为 NonIndexed.

  * Indexed:Job 的 Pod 会获得对应的完成索引,取值为 0 到 .spec.completions-1. 该索引可以通过三种方式获取:

    * Pod 注解 batch.kubernetes.io/job-completion-index.
    * 作为 Pod 主机名的一部分,遵循模式 $(job-name)-$(index). 当你同时使用带索引的 Job(Indexed Job)与 服务(Service), Job 中的 Pods 可以通过 DNS 使用确切的主机名互相寻址.
    * 对于容器化的任务,在环境变量 JOB_COMPLETION_INDEX 中.

当每个索引都对应一个完成完成的 Pod 时,Job 被认为是已完成的. 关于如何使用这种模式的更多信息,可参阅 用带索引的 Job 执行基于静态任务分配的并行处理. 需要注意的是,对同一索引值可能被启动的 Pod 不止一个,尽管这种情况很少发生. 这时,只有一个会被记入完成计数中.


** 处理 Pod 和容器失效
Pod 中的容器可能因为多种不同原因失效,例如因为其中的进程退出时返回值非零, 或者容器因为超出内存约束而被杀死等等. 如果发生这类事件,并且 .spec.template.spec.restartPolicy = "OnFailure", Pod 则继续留在当前节点,但容器会被重新运行. 因此,你的程序需要能够处理在本地被重启的情况,或者要设置 .spec.template.spec.restartPolicy = "Never"

整个 Pod 也可能会失败,且原因各不相同. 例如,当 Pod 启动时,节点失效(被升级、被重启、被删除等)或者其中的容器失败而 .spec.template.spec.restartPolicy = "Never". 当 Pod 失败时,Job 控制器会启动一个新的 Pod. 这意味着,你的应用需要处理在一个新 Pod 中被重启的情况. 尤其是应用需要处理之前运行所产生的临时文件、锁、不完整的输出等问题.

注意,即使你将 .spec.parallelism 设置为 1,且将 .spec.completions 设置为 1,并且 .spec.template.spec.restartPolicy 设置为 "Never",同一程序仍然有可能被启动两次.

如果你确实将 .spec.parallelism 和 .spec.completions 都设置为比 1 大的值, 那就有可能同时出现多个 Pod 运行的情况. 为此,你的 Pod 也必须能够处理并发性问题.

* Pod 回退失效策略
在有些情形下,你可能希望 Job 在经历若干次重试之后直接进入失败状态,因为这很 可能意味着遇到了配置错误. 为了实现这点,可以将 .spec.backoffLimit 设置为视 Job 为失败之前的重试次数. 失效回退的限制值默认为 6. 与 Job 相关的失效的 Pod 会被 Job 控制器重建,回退重试时间将会按指数增长 (从 10 秒、20 秒到 40 秒)最多至 6 分钟. 当 Job 的 Pod 被删除时,或者 Pod 成功时没有其它 Pod 处于失败状态,失效回退的次数也会被重置(为 0).

说明: 如果你的 Job 的 restartPolicy 被设置为 "OnFailure",就要注意运行该 Job 的 Pod 会在 Job 到达失效回退次数上限时自动被终止. 这会使得调试 Job 中可执行文件的工作变得非常棘手. 我们建议在调试 Job 时将 restartPolicy 设置为 "Never", 或者使用日志系统来确保失效 Jobs 的输出不会意外遗失.


** Job 终止与清理
Job 完成时不会再创建新的 Pod,不过已有的 Pod 通常也不会被删除. 保留这些 Pod 使得你可以查看已完成的 Pod 的日志输出,以便检查错误、警告 或者其它诊断性输出. Job 完成时 Job 对象也一样被保留下来,这样你就可以查看它的状态. 在查看了 Job 状态之后删除老的 Job 的操作留给了用户自己. 你可以使用 kubectl 来删除 Job(例如,kubectl delete jobs/pi 或者 kubectl delete -f ./job.yaml). 当使用 kubectl 来删除 Job 时,该 Job 所创建的 Pods 也会被删除.

默认情况下,Job 会持续运行,除非某个 Pod 失败(restartPolicy=Never) 或者某个容器出错退出(restartPolicy=OnFailure). 这时,Job 基于前述的 spec.backoffLimit 来决定是否以及如何重试. 一旦重试次数到达 .spec.backoffLimit 所设的上限,Job 会被标记为失败, 其中运行的 Pods 都会被终止.

终止 Job 的另一种方式是设置一个活跃期限. 你可以为 Job 的 .spec.activeDeadlineSeconds 设置一个秒数值. 该值适用于 Job 的整个生命期,无论 Job 创建了多少个 Pod. 一旦 Job 运行时间达到 activeDeadlineSeconds 秒,其所有运行中的 Pod 都会被终止,并且 Job 的状态更新为 type: Failed 及 reason: DeadlineExceeded.

注意 Job 的 .spec.activeDeadlineSeconds 优先级高于其 .spec.backoffLimit 设置. 因此,如果一个 Job 正在重试一个或多个失效的 Pod,该 Job 一旦到达 activeDeadlineSeconds 所设的时限即不再部署额外的 Pod,即使其重试次数还未 达到 backoffLimit 所设的限制.

例如:

apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
注意 Job 规约和 Job 中的 Pod 模版规约 都有 activeDeadlineSeconds 字段. 请确保你在合适的层次设置正确的字段.

还要注意的是,restartPolicy 对应的是 Pod,而不是 Job 本身: 一旦 Job 状态变为 type: Failed,就不会再发生 Job 重启的动作. 换言之,由 .spec.activeDeadlineSeconds 和 .spec.backoffLimit 所触发的 Job 终结机制 都会导致 Job 永久性的失败,而这类状态都需要手工干预才能解决.


** 自动清理完成的 Job 
完成的 Job 通常不需要留存在系统中.在系统中一直保留它们会给 API 服务器带来额外的压力. 如果 Job 由某种更高级别的控制器来管理,例如 CronJobs, 则 Job 可以被 CronJob 基于特定的根据容量裁定的清理策略清理掉.

* 已完成 Job 的 TTL 机制 
FEATURE STATE: Kubernetes v1.21 [beta]
自动清理已完成 Job (状态为 Complete 或 Failed)的另一种方式是使用由 TTL 控制器所提供 的 TTL 机制. 通过设置 Job 的 .spec.ttlSecondsAfterFinished 字段,可以让该控制器清理掉 已结束的资源.

TTL 控制器清理 Job 时,会级联式地删除 Job 对象. 换言之,它会删除所有依赖的对象,包括 Pod 及 Job 本身. 注意,当 Job 被删除时,系统会考虑其生命周期保障,例如其 Finalizers.

例如:

apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never

Job pi-with-ttl 在结束 100 秒之后,可以成为被自动删除的对象.

如果该字段设置为 0,Job 在结束之后立即成为可被自动删除的对象. 如果该字段没有设置,Job 不会在结束之后被 TTL 控制器自动清除.


** Job 模式 
Job 对象可以用来支持多个 Pod 的可靠的并发执行. Job 对象不是设计用来支持相互通信的并行进程的,后者一般在科学计算中应用较多. Job 的确能够支持对一组相互独立而又有所关联的 工作条目 的并行处理. 这类工作条目可能是要发送的电子邮件、要渲染的视频帧、要编解码的文件、NoSQL 数据库中要扫描的主键范围等等.

在一个复杂系统中,可能存在多个不同的工作条目集合.这里我们仅考虑用户希望一起管理的 工作条目集合之一 — 批处理作业.

并行计算的模式有好多种,每种都有自己的强项和弱点.这里要权衡的因素有:

  * 每个工作条目对应一个 Job 或者所有工作条目对应同一 Job 对象. 后者更适合处理大量工作条目的场景; 前者会给用户带来一些额外的负担,而且需要系统管理大量的 Job 对象.
  * 创建与工作条目相等的 Pod 或者令每个 Pod 可以处理多个工作条目. 前者通常不需要对现有代码和容器做较大改动; 后者则更适合工作条目数量较大的场合,原因同上.
  * 有几种技术都会用到工作队列.这意味着需要运行一个队列服务,并修改现有程序或容器 使之能够利用该工作队列. 与之比较,其他方案在修改现有容器化应用以适应需求方面可能更容易一些.

下面是对这些权衡的汇总,列 2 到 4 对应上面的权衡比较. 模式的名称对应了相关示例和更详细描述的链接.

模式	                                          单个 Job 对象	    Pods 数少于工作条目数?	直接使用应用无需修改?
每工作条目一 Pod 的队列	      ✓		                                                                有时
Pod 数量可变的队列	                ✓	                        ✓	
静态任务分派的带索引的 Job	✓		                                                                 ✓
Job 模版扩展			                                                                                               ✓

当你使用 .spec.completions 来设置完成数时,Job 控制器所创建的每个 Pod 使用完全相同的 spec. 这意味着任务的所有 Pod 都有相同的命令行,都使用相同的镜像和数据卷,甚至连 环境变量都(几乎)相同. 这些模式是让每个 Pod 执行不同工作的几种不同形式.

下表显示的是每种模式下 .spec.parallelism 和 .spec.completions 所需要的设置. 其中,W 表示的是工作条目的个数.

模式	                                        .spec.completions     .spec.parallelism
每工作条目一 Pod 的队列	      W	                                任意值
Pod 个数可变的队列	                1	                                任意值
静态任务分派的带索引的 Job	W	
Job 模版扩展	                            1	                                 应该为 1


** 高级用法 
详细的设置如下:
https://kubernetes.io/zh/docs/concepts/workloads/controllers/job/

*挂起 Job 
FEATURE STATE: Kubernetes v1.21 [alpha]

说明:
该特性在 Kubernetes 1.21 版本中是 Alpha 阶段,启用该特性需要额外的步骤; 请确保你正在阅读与集群版本一致的文档.

* 指定你自己的 Pod 选择算符
通常,当你创建一个 Job 对象时,你不会设置 .spec.selector. 系统的默认值填充逻辑会在创建 Job 时添加此字段. 它会选择一个不会与任何其他 Job 重叠的选择算符设置.

不过,有些场合下,你可能需要重载这个自动设置的选择算符. 为了实现这点,你可以手动设置 Job 的 spec.selector 字段.

* 使用 Finalizer 追踪 Job 
FEATURE STATE: Kubernetes v1.22 [alpha]
说明:
要使用该行为,你必须为 API 服务器 和控制器管理器 启用 JobTrackingWithFinalizers 特性门控. 默认是禁用的.

启用后,控制面基于下述行为追踪新的 Job.现有 Job 不受影响. 作为用户,你会看到的唯一区别是控制面对 Job 完成情况的跟踪更加准确.

### CronJob
FEATURE STATE: Kubernetes v1.21 [stable]
CronJob 创建基于时隔重复调度的 Jobs.

一个 CronJob 对象就像 crontab (cron table) 文件中的一行. 它用 Cron 格式进行编写, 并周期性地在给定的调度时间执行 Job.

注意:
所有 CronJob 的 schedule: 时间都是基于 kube-controller-manager. 的时区.

如果你的控制平面在 Pod 或是裸容器中运行了 kube-controller-manager, 那么为该容器所设置的时区将会决定 Cron Job 的控制器所使用的时区.

注意:
如 v1 CronJob API 所述,官方并不支持设置时区.

Kubernetes 项目官方并不支持设置如 CRON_TZ 或者 TZ 等变量. CRON_TZ 或者 TZ 是用于解析和计算下一个 Job 创建时间所使用的内部库中一个实现细节. 不建议在生产集群中使用它.

为 CronJob 资源创建清单时,请确保所提供的名称是一个合法的 DNS 子域名. 名称不能超过 52 个字符. 这是因为 CronJob 控制器将自动在提供的 Job 名称后附加 11 个字符,并且存在一个限制, 即 Job 名称的最大长度不能超过 63 个字符.


** CronJob
CronJob 用于执行周期性的动作,例如备份、报告生成等. 这些任务中的每一个都应该配置为周期性重复的(例如:每天/每周/每月一次); 你可以定义任务开始执行的时间间隔.

* 示例
下面的 CronJob 示例清单会在每分钟打印出当前时间和问候消息:

application/job/cronjob.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure

使用 CronJob 运行自动化任务 一文会为你详细讲解此例.

* Cron 时间表语法
# ┌───────────── 分钟 (0 - 59)
# │ ┌───────────── 小时 (0 - 23)
# │ │ ┌───────────── 月的某天 (1 - 31)
# │ │ │ ┌───────────── 月份 (1 - 12)
# │ │ │ │ ┌───────────── 周的某天 (0 - 6)(周日到周一;在某些系统上,7 也是星期日)
# │ │ │ │ │                          或者是 sun,mon,tue,web,thu,fri,sat
# │ │ │ │ │
# │ │ │ │ │
# * * * * *

输入	                              描述	                                              相当于
@yearly (or @annually)	  每年 1 月 1 日的午夜运行一次	  0 0 1 1 *
@monthly	                      每月第一天的午夜运行一次	      0 0 1 * *
@weekly	                        每周的周日午夜运行一次	          0 0 * * 0
@daily (or @midnight)	  每天午夜运行一次	                      0 0 * * *
@hourly	                          每小时的开始一次	                      0 * * * *

例如,下面这行指出必须在每个星期五的午夜以及每个月 13 号的午夜开始任务:

0 0 13 * 5

要生成 CronJob 时间表表达式,你还可以使用 crontab.guru 之类的 Web 工具.


** CronJob 限制 
CronJob 根据其计划编排,在每次该执行任务的时候大约会创建一个 Job. 我们之所以说 "大约",是因为在某些情况下,可能会创建两个 Job,或者不会创建任何 Job. 我们试图使这些情况尽量少发生,但不能完全杜绝.因此,Job 应该是 幂等的.

如果 startingDeadlineSeconds 设置为很大的数值或未设置(默认),并且 concurrencyPolicy 设置为 Allow,则作业将始终至少运行一次.

注意:
如果 startingDeadlineSeconds 的设置值低于 10 秒钟,CronJob 可能无法被调度. 这是因为 CronJob 控制器每 10 秒钟执行一次检查.

对于每个 CronJob,CronJob 控制器(Controller) 检查从上一次调度的时间点到现在所错过了调度次数.如果错过的调度次数超过 100 次, 那么它就不会启动这个任务,并记录这个错误:

Cannot determine if job needs to be started. Too many missed start time (> 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
需要注意的是,如果 startingDeadlineSeconds 字段非空,则控制器会统计从 startingDeadlineSeconds 设置的值到现在而不是从上一个计划时间到现在错过了多少次 Job. 例如,如果 startingDeadlineSeconds 是 200,则控制器会统计在过去 200 秒中错过了多少次 Job.

如果未能在调度时间内创建 CronJob,则计为错过. 例如,如果 concurrencyPolicy 被设置为 Forbid,并且当前有一个调度仍在运行的情况下, 试图调度的 CronJob 将被计算为错过.

例如,假设一个 CronJob 被设置为从 08:30:00 开始每隔一分钟创建一个新的 Job, 并且它的 startingDeadlineSeconds 字段未被设置.如果 CronJob 控制器从 08:29:00 到 10:21:00 终止运行,则该 Job 将不会启动,因为其错过的调度 次数超过了 100.

为了进一步阐述这个概念,假设将 CronJob 设置为从 08:30:00 开始每隔一分钟创建一个新的 Job, 并将其 startingDeadlineSeconds 字段设置为 200 秒. 如果 CronJob 控制器恰好在与上一个示例相同的时间段(08:29:00 到 10:21:00)终止运行, 则 Job 仍将从 10:22:00 开始. 造成这种情况的原因是控制器现在检查在最近 200 秒(即 3 个错过的调度)中发生了多少次错过的 Job 调度,而不是从现在为止的最后一个调度时间开始.

CronJob 仅负责创建与其调度时间相匹配的 Job,而 Job 又负责管理其代表的 Pod.


** 控制器版本 
从 Kubernetes v1.21 版本开始,CronJob 控制器的第二个版本被用作默认实现. 要禁用此默认 CronJob 控制器而使用原来的 CronJob 控制器,请在 kube-controller-manager 中设置特性门控 CronJobControllerV2,将此标志设置为 false.例如:

--feature-gates="CronJobControllerV2=false"

### ReplicationController
链接地址:
https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicationcontroller/

说明: 现在推荐使用配置 ReplicaSet 的 Deployment 来建立副本管理机制.
ReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态. 换句话说,ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的.

** ReplicationController 的替代方案
ReplicaSet
ReplicaSet 是下一代 ReplicationController, 支持新的基于集合的标签选择算符. 它主要被 Deployment 用来作为一种编排 Pod 创建、删除及更新的机制. 请注意,我们推荐使用 Deployment 而不是直接使用 ReplicaSet,除非 你需要自定义更新编排或根本不需要更新.

Deployment(推荐)
Deployment 是一种更高级别的 API 对象,用于更新其底层 ReplicaSet 及其 Pod. 如果你想要这种滚动更新功能,那么推荐使用 Deployment,因为它们是声明式的、服务端的,并且具有其它特性.

裸 Pod
与用户直接创建 Pod 的情况不同,ReplicationController 能够替换因某些原因 被删除或被终止的 Pod ,例如在节点故障或中断节点维护的情况下,例如内核升级. 因此,我们建议你使用 ReplicationController,即使你的应用程序只需要一个 Pod. 可以将其看作类似于进程管理器,它只管理跨多个节点的多个 Pod ,而不是单个节点上的单个进程. ReplicationController 将本地容器重启委托给节点上的某个代理(例如,Kubelet 或 Docker).

Job
对于预期会自行终止的 Pod (即批处理任务),使用 Job 而不是 ReplicationController.

DaemonSet
对于提供机器级功能(例如机器监控或机器日志记录)的 Pod, 使用 DaemonSet 而不是 ReplicationController. 这些 Pod 的生命期与机器的生命期绑定:它们需要在其他 Pod 启动之前在机器上运行, 并且在机器准备重新启动或者关闭时安全地终止.

-------------------
# 服务、负载均衡和联网
* Kubernetes 网络模型 
每一个 Pod 都有它自己的IP地址, 这就意味着你不需要显式地在 Pod 之间创建链接, 你几乎不需要处理容器端口到主机端口之间的映射. 这将形成一个干净的、向后兼容的模型;在这个模型里,从端口分配、命名、服务发现、 负载均衡、应用配置和迁移的角度来看, Pod 可以被视作虚拟机或者物理主机.

Kubernetes 强制要求所有网络设施都满足以下基本要求(从而排除了有意隔离网络的策略):

  * 节点上的 Pod 可以不通过 NAT 和其他任何节点上的 Pod 通信
  * 节点上的代理(比如:系统守护进程、kubelet)可以和节点上的所有 Pod 通信

备注:对于支持在主机网络中运行 Pod 的平台(比如:Linux):

  * 运行在节点主机网络里的 Pod 可以不通过 NAT 和所有节点上的 Pod 通信

这个模型不仅不复杂,而且还和 Kubernetes 的实现从虚拟机向容器平滑迁移的初衷相符, 如果你的任务开始是在虚拟机中运行的,你的虚拟机有一个 IP, 可以和项目中其他虚拟机通信.这里的模型是基本相同的.

Kubernetes 的 IP 地址存在于 Pod 范围内 - 容器共享它们的网络命名空间 - 包括它们的 IP 地址和 MAC 地址. 这就意味着 Pod 内的容器都可以通过 localhost 到达对方端口. 这也意味着 Pod 内的容器需要相互协调端口的使用,但是这和虚拟机中的进程似乎没有什么不同, 这也被称为"一个 Pod 一个 IP"模型.

如何实现以上需求是所使用的特定容器运行时的细节.

也可以在 Node 本身请求端口,并用这类端口转发到你的 Pod(称之为主机端口), 但这是一个很特殊的操作.转发方式如何实现也是容器运行时的细节. Pod 自己并不知道这些主机端口的存在.

Kubernetes 网络解决四方面的问题:

  * 一个 Pod 中的容器之间通过本地回路(loopback)通信.
  * 集群网络在不同 pod 之间提供通信.
  * Service 资源允许你 对外暴露 Pods 中运行的应用程序, 以支持来自于集群外部的访问.
  * 可以使用 Services 来发布仅供集群内部使用的服务.

## 服务
重要
将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法.
使用 Kubernetes,你无需修改应用程序即可使用不熟悉的服务发现机制. Kubernetes 为 Pods 提供自己的 IP 地址,并为一组 Pod 提供相同的 DNS 名, 并且可以在它们之间进行负载均衡.


** 动机
创建和销毁 Kubernetes Pod 以匹配集群状态. Pod 是非永久性资源. 如果你使用 Deployment 来运行你的应用程序,则它可以动态创建和销毁 Pod.

每个 Pod 都有自己的 IP 地址,但是在 Deployment 中,在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同.

这导致了一个问题: 如果一组 Pod(称为"后端")为集群内的其他 Pod(称为"前端")提供功能, 那么前端如何找出并跟踪要连接的 IP 地址,以便前端可以使用提供工作负载的后端部分?

进入 Services.

** Service 资源
Kubernetes Service 定义了这样一种抽象:逻辑上的一组 Pod,一种可以访问它们的策略 —— 通常称为微服务. Service 所针对的 Pods 集合通常是通过选择算符来确定的. 要了解定义服务端点的其他方法,请参阅不带选择算符的服务.

举个例子,考虑一个图片处理后端,它运行了 3 个副本.这些副本是可互换的 —— 前端不需要关心它们调用了哪个后端副本. 然而组成这一组后端程序的 Pod 实际上可能会发生变化, 前端客户端不应该也没必要知道,而且也不需要跟踪这一组后端的状态.

Service 定义的抽象能够解耦这种关联.

* 云原生服务发现 
如果你想要在应用程序中使用 Kubernetes API 进行服务发现,则可以查询 API 服务器 的 Endpoints 资源,只要服务中的 Pod 集合发生更改,Endpoints 就会被更新.

对于非本机应用程序,Kubernetes 提供了在应用程序和后端 Pod 之间放置网络端口或负载均衡器的方法.


** 定义 Service
Service 在 Kubernetes 中是一个 REST 对象,和 Pod 类似. 像所有的 REST 对象一样,Service 定义可以基于 POST 方式,请求 API server 创建新的实例. Service 对象的名称必须是合法的 RFC 1035 标签名称..

例如,假定有一组 Pod,它们对外暴露了 9376 端口,同时还被打上 app=MyApp 标签:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376

上述配置创建一个名称为 "my-service" 的 Service 对象,它会将请求代理到使用 TCP 端口 9376,并且具有标签 "app=MyApp" 的 Pod 上.

Kubernetes 为该服务分配一个 IP 地址(有时称为 "集群IP"),该 IP 地址由服务代理使用. (请参见下面的 VIP 和 Service 代理).

服务选择算符的控制器不断扫描与其选择器匹配的 Pod,然后将所有更新发布到也称为 "my-service" 的 Endpoint 对象.

说明: 需要注意的是,Service 能够将一个接收 port 映射到任意的 targetPort. 默认情况下,targetPort 将被设置为与 port 字段相同的值.
Pod 中的端口定义是有名字的,你可以在服务的 targetPort 属性中引用这些名称. 即使服务中使用单个配置的名称混合使用 Pod,并且通过不同的端口号提供相同的网络协议,此功能也可以使用. 这为部署和发展服务提供了很大的灵活性. 例如,你可以更改 Pods 在新版本的后端软件中公开的端口号,而不会破坏客户端.

服务的默认协议是 TCP;你还可以使用任何其他受支持的协议.

由于许多服务需要公开多个端口,因此 Kubernetes 在服务对象上支持多个端口定义. 每个端口定义可以具有相同的 protocol,也可以具有不同的协议.

* 没有选择算符的 Service 
服务最常见的是抽象化对 Kubernetes Pod 的访问,但是它们也可以抽象化其他种类的后端. 实例:

  * 希望在生产环境中使用外部的数据库集群,但测试环境使用自己的数据库.
  * 希望服务指向另一个 名字空间(Namespace) 中或其它集群中的服务.
  * 你正在将工作负载迁移到 Kubernetes. 在评估该方法时,你仅在 Kubernetes 中运行一部分后端.

在任何这些场景中,都能够定义没有选择算符的 Service. 实例:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376

由于此服务没有选择算符,因此不会自动创建相应的 Endpoint 对象. 你可以通过手动添加 Endpoint 对象,将服务手动映射到运行该服务的网络地址和端口:

apiVersion: v1
kind: Endpoints
metadata:
  name: my-service
subsets:
  - addresses:
      - ip: 192.0.2.42
    ports:
      - port: 9376

Endpoints 对象的名称必须是合法的 DNS 子域名.

说明:
端点 IPs 必须不可以 是:本地回路(IPv4 的 127.0.0.0/8, IPv6 的 ::1/128)或 本地链接(IPv4 的 169.254.0.0/16 和 224.0.0.0/24,IPv6 的 fe80::/64).

端点 IP 地址不能是其他 Kubernetes 服务的集群 IP,因为 kube-proxy 不支持将虚拟 IP 作为目标.

访问没有选择算符的 Service,与有选择算符的 Service 的原理相同. 请求将被路由到用户定义的 Endpoint,YAML 中为:192.0.2.42:9376(TCP).

ExternalName Service 是 Service 的特例,它没有选择算符,但是使用 DNS 名称. 

* 超出容量的 Endpoints 
如果某个 Endpoints 资源中包含的端点个数超过 1000,则 Kubernetes v1.22 版本 (及更新版本)的集群会将为该 Endpoints 添加注解 endpoints.kubernetes.io/over-capacity: truncated. 这一注解表明所影响到的 Endpoints 对象已经超出容量,此外 Endpoints 控制器还会将 Endpoints 对象数量截断到 1000.

* EndpointSlices
FEATURE STATE: Kubernetes v1.21 [stable]
EndpointSlices 是一种 API 资源,可以为 Endpoints 提供更可扩展的替代方案. 尽管从概念上讲与 Endpoints 非常相似,但 EndpointSlices 允许跨多个资源分布网络端点. 默认情况下,一旦到达 100 个 Endpoint,该 EndpointSlice 将被视为"已满", 届时将创建其他 EndpointSlices 来存储任何其他 Endpoints.

EndpointSlices 提供了附加的属性和功能,这些属性和功能在 EndpointSlices 中有详细描述.

* 应用协议 
FEATURE STATE: Kubernetes v1.20 [stable]
appProtocol 字段提供了一种为每个 Service 端口指定应用协议的方式. 此字段的取值会被映射到对应的 Endpoints 和 EndpointSlices 对象.

该字段遵循标准的 Kubernetes 标签语法. 其值可以是 IANA 标准服务名称 或以域名为前缀的名称,如 mycompany.com/my-custom-protocol.


** 虚拟 IP 和 Service 代理
在 Kubernetes 集群中,每个 Node 运行一个 kube-proxy 进程. kube-proxy 负责为 Service 实现了一种 VIP(虚拟 IP)的形式,而不是 ExternalName 的形式.

* 为什么不使用 DNS 轮询?
时不时会有人问到为什么 Kubernetes 依赖代理将入站流量转发到后端.那其他方法呢? 例如,是否可以配置具有多个 A 值(或 IPv6 为 AAAA)的 DNS 记录,并依靠轮询名称解析?

使用服务代理有以下几个原因:

  * DNS 实现的历史由来已久,它不遵守记录 TTL,并且在名称查找结果到期后对其进行缓存.
  * 有些应用程序仅执行一次 DNS 查找,并无限期地缓存结果.
  * 即使应用和库进行了适当的重新解析,DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载,从而使管理变得困难.

* userspace 代理模式
这种模式,kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作. 对每个 Service,它会在本地 Node 上打开一个端口(随机选择). 任何连接到"代理端口"的请求,都会被代理到 Service 的后端 Pods 中的某个上面(如 Endpoints 所报告的一样). 使用哪个后端 Pod,是 kube-proxy 基于 SessionAffinity 来确定的.

最后,它配置 iptables 规则,捕获到达该 Service 的 clusterIP(是虚拟 IP) 和 Port 的请求,并重定向到代理端口,代理端口再代理请求到后端Pod.

默认情况下,用户空间模式下的 kube-proxy 通过轮转算法选择后端.

详细的拓扑图:
https://kubernetes.io/zh/docs/concepts/services-networking/service/

* iptables 代理模式
这种模式,kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除. 对每个 Service,它会配置 iptables 规则,从而捕获到达该 Service 的 clusterIP 和端口的请求,进而将请求重定向到 Service 的一组后端中的某个 Pod 上面. 对于每个 Endpoints 对象,它也会配置 iptables 规则,这个规则会选择一个后端组合.

默认的策略是,kube-proxy 在 iptables 模式下随机选择一个后端.

使用 iptables 处理流量具有较低的系统开销,因为流量由 Linux netfilter 处理, 而无需在用户空间和内核空间之间切换. 这种方法也可能更可靠.

如果 kube-proxy 在 iptables 模式下运行,并且所选的第一个 Pod 没有响应, 则连接失败. 这与用户空间模式不同:在这种情况下,kube-proxy 将检测到与第一个 Pod 的连接已失败, 并会自动使用其他后端 Pod 重试.

你可以使用 Pod 就绪探测器 验证后端 Pod 可以正常工作,以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端. 这样做意味着你避免将流量通过 kube-proxy 发送到已知已失败的 Pod.

详细的拓扑图:
https://kubernetes.io/zh/docs/concepts/services-networking/service/

* IPVS 代理模式   --- 说明: 这种模式,本质是在服务器上启动了 LVS ,每一个 services 都会启动一个对应于 LVS 中的 VIP,pod 的 IP 则为 LVS 中的后端节点 IP 
FEATURE STATE: Kubernetes v1.11 [stable]
在 ipvs 模式下,kube-proxy 监视 Kubernetes 服务和端点,调用 netlink 接口相应地创建 IPVS 规则, 并定期将 IPVS 规则与 Kubernetes 服务和端点同步. 该控制循环可确保IPVS 状态与所需状态匹配.访问服务时,IPVS 将流量定向到后端Pod之一.

IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数, 但是使用哈希表作为基础数据结构,并且在内核空间中工作. 这意味着,与 iptables 模式下的 kube-proxy 相比,IPVS 模式下的 kube-proxy 重定向通信的延迟要短,并且在同步代理规则时具有更好的性能. 与其他代理模式相比,IPVS 模式还支持更高的网络流量吞吐量.

IPVS 提供了更多选项来平衡后端 Pod 的流量. 这些是:

rr:轮替(Round-Robin)
lc:最少链接(Least Connection),即打开链接数量最少者优先
dh:目标地址哈希(Destination Hashing)
sh:源地址哈希(Source Hashing)
sed:最短预期延迟(Shortest Expected Delay)
nq:从不排队(Never Queue)
说明:
要在 IPVS 模式下运行 kube-proxy,必须在启动 kube-proxy 之前使 IPVS 在节点上可用.

当 kube-proxy 以 IPVS 代理模式启动时,它将验证 IPVS 内核模块是否可用. 如果未检测到 IPVS 内核模块,则 kube-proxy 将退回到以 iptables 代理模式运行.

在这些代理模型中,绑定到服务 IP 的流量: 在客户端不了解 Kubernetes 或服务或 Pod 的任何信息的情况下,将 Port 代理到适当的后端.

如果要确保每次都将来自特定客户端的连接传递到同一 Pod, 则可以通过将 service.spec.sessionAffinity 设置为 "ClientIP" (默认值是 "None"),来基于客户端的 IP 地址选择会话关联. 你还可以通过适当设置 service.spec.sessionAffinityConfig.clientIP.timeoutSeconds 来设置最大会话停留时间. (默认值为 10800 秒,即 3 小时).

详细的拓扑图:
https://kubernetes.io/zh/docs/concepts/services-networking/service/

这里有一个描述:
https://cloud.tencent.com/developer/article/1477638


** 多端口 Service 
对于某些服务,你需要公开多个端口. Kubernetes 允许你在 Service 对象上配置多个端口定义. 为服务使用多个端口时,必须提供所有端口名称,以使它们无歧义. 例如:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9377

说明:
与一般的Kubernetes名称一样,端口名称只能包含小写字母数字字符 和 -. 端口名称还必须以字母数字字符开头和结尾.

例如,名称 123-abc 和 web 有效,但是 123_abc 和 -web 无效.


** 选择自己的 IP 地址
在 Service 创建的请求中,可以通过设置 spec.clusterIP 字段来指定自己的集群 IP 地址. 比如,希望替换一个已经已存在的 DNS 条目,或者遗留系统已经配置了一个固定的 IP 且很难重新配置.

用户选择的 IP 地址必须合法,并且这个 IP 地址在 service-cluster-ip-range CIDR 范围内, 这对 API 服务器来说是通过一个标识来指定的. 如果 IP 地址不合法,API 服务器会返回 HTTP 状态码 422,表示值不合法.


** 流量策略 
* 外部流量策略 
你可以通过设置 spec.externalTrafficPolicy 字段来控制来自于外部的流量是如何路由的. 可选值有 Cluster 和 Local.字段设为 Cluster 会将外部流量路由到所有就绪的端点, 设为 Local 会只路由到当前节点上就绪的端点. 如果流量策略设置为 Local,而且当前节点上没有就绪的端点,kube-proxy 不会转发请求相关服务的任何流量.

说明:
FEATURE STATE: Kubernetes v1.22 [alpha]
如果你启用了 kube-proxy 的 ProxyTerminatingEndpoints 特性门控, kube-proxy 会检查节点是否有本地的端点,以及是否所有的本地端点都被标记为终止中.

如果本地有端点,而且所有端点处于终止中的状态,那么 kube-proxy 会忽略任何设为 Local 的外部流量策略. 在所有本地端点处于终止中的状态的同时,kube-proxy 将请求指定服务的流量转发到位于其它节点的 状态健康的端点,如同外部流量策略设为 Cluster.

针对处于正被终止状态的端点这一转发行为使得外部负载均衡器可以优雅地排出由 NodePort 服务支持的连接,就算是健康检查节点端口开始失败也是如此. 否则,当节点还在负载均衡器的节点池内,在 Pod 终止过程中的流量会被丢掉,这些流量可能会丢失.

* 内部流量策略 
FEATURE STATE: Kubernetes v1.22 [beta]
你可以设置 spec.internalTrafficPolicy 字段来控制内部来源的流量是如何转发的.可设置的值有 Cluster 和 Local. 将字段设置为 Cluster 会将内部流量路由到所有就绪端点,设置为 Local 只会路由到当前节点上就绪的端点. 如果流量策略是 Local,而且当前节点上没有就绪的端点,那么 kube-proxy 会丢弃流量.


** 服务发现 
Kubernetes 支持两种基本的服务发现模式 —— 环境变量和 DNS.

* 环境变量 
当 Pod 运行在 Node 上,kubelet 会为每个活跃的 Service 添加一组环境变量. 它同时支持 Docker links兼容 变量 (查看 makeLinkVariables)、 简单的 {SVCNAME}_SERVICE_HOST 和 {SVCNAME}_SERVICE_PORT 变量. 这里 Service 的名称需大写,横线被转换成下划线.

举个例子,一个名称为 redis-master 的 Service 暴露了 TCP 端口 6379, 同时给它分配了 Cluster IP 地址 10.0.0.11,这个 Service 生成了如下环境变量:

REDIS_MASTER_SERVICE_HOST=10.0.0.11
REDIS_MASTER_SERVICE_PORT=6379
REDIS_MASTER_PORT=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379
REDIS_MASTER_PORT_6379_TCP_PROTO=tcp
REDIS_MASTER_PORT_6379_TCP_PORT=6379
REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11

说明:
当你具有需要访问服务的 Pod 时,并且你正在使用环境变量方法将端口和集群 IP 发布到客户端 Pod 时,必须在客户端 Pod 出现 之前 创建服务. 否则,这些客户端 Pod 将不会设定其环境变量.

如果仅使用 DNS 查找服务的集群 IP,则无需担心此设定问题.

* DNS
你可以(几乎总是应该)使用附加组件 为 Kubernetes 集群设置 DNS 服务.

支持集群的 DNS 服务器(例如 CoreDNS)监视 Kubernetes API 中的新服务,并为每个服务创建一组 DNS 记录. 如果在整个集群中都启用了 DNS,则所有 Pod 都应该能够通过其 DNS 名称自动解析服务.

例如,如果你在 Kubernetes 命名空间 my-ns 中有一个名为 my-service 的服务, 则控制平面和 DNS 服务共同为 my-service.my-ns 创建 DNS 记录. my-ns 命名空间中的 Pod 应该能够通过按名检索 my-service 来找到服务 (my-service.my-ns 也可以工作).

其他命名空间中的 Pod 必须将名称限定为 my-service.my-ns. 这些名称将解析为为服务分配的集群 IP.

Kubernetes 还支持命名端口的 DNS SRV(服务)记录. 如果 my-service.my-ns 服务具有名为 http　的端口,且协议设置为 TCP, 则可以对 _http._tcp.my-service.my-ns 执行 DNS SRV 查询查询以发现该端口号, "http" 以及 IP 地址.

Kubernetes DNS 服务器是唯一的一种能够访问 ExternalName 类型的 Service 的方式.


** 无头服务(Headless Services) 
有时不需要或不想要负载均衡,以及单独的 Service IP. 遇到这种情况,可以通过指定 Cluster IP(spec.clusterIP)的值为 "None" 来创建 Headless Service.

你可以使用无头 Service 与其他服务发现机制进行接口,而不必与 Kubernetes 的实现捆绑在一起.

对这无头 Service 并不会分配 Cluster IP,kube-proxy 不会处理它们, 而且平台也不会为它们进行负载均衡和路由. DNS 如何实现自动配置,依赖于 Service 是否定义了选择算符.

* 带选择算符的服务
对定义了选择算符的无头服务,Endpoint 控制器在 API 中创建了 Endpoints 记录, 并且修改 DNS 配置返回 A 记录(IP 地址),通过这个地址直接到达 Service 的后端 Pod 上.

* 无选择算符的服务 
对没有定义选择算符的无头服务,Endpoint 控制器不会创建 Endpoints 记录. 然而 DNS 系统会查找和配置,无论是:

  * 对于 ExternalName 类型的服务,查找其 CNAME 记录

  * 对所有其他类型的服务,查找与 Service 名称相同的任何 Endpoints 的记录


** 发布服务(服务类型) 
对一些应用的某些部分(如前端),可能希望将其暴露给 Kubernetes 集群外部 的 IP 地址.

Kubernetes ServiceTypes 允许指定你所需要的 Service 类型,默认是 ClusterIP.

Type 的取值以及行为如下:

  * ClusterIP:通过集群的内部 IP 暴露服务,选择该值时服务只能够在集群内部访问. 这也是默认的 ServiceType.

  * NodePort:通过每个节点上的 IP 和静态端口(NodePort)暴露服务. NodePort 服务会路由到自动创建的 ClusterIP 服务. 通过请求 <节点 IP>:<节点端口>,你可以从集群的外部访问一个 NodePort 服务.

  * LoadBalancer:使用云提供商的负载均衡器向外部暴露服务. 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上.

  * ExternalName:通过返回 CNAME 和对应值,可以将服务映射到 externalName 字段的内容(例如,foo.bar.example.com). 无需创建任何类型代理.

说明: 你需要使用 kube-dns 1.7 及以上版本或者 CoreDNS 0.0.8 及以上版本才能使用 ExternalName 类型.
你也可以使用 Ingress 来暴露自己的服务. Ingress 不是一种服务类型,但它充当集群的入口点. 它可以将路由规则整合到一个资源中,因为它可以在同一IP地址下公开多个服务.

* NodePort 类型 
如果你将 type 字段设置为 NodePort,则 Kubernetes 控制平面将在 --service-node-port-range 标志指定的范围内分配端口(默认值:30000-32767). 每个节点将那个端口(每个节点上的相同端口号)代理到你的服务中. 你的服务在其 .spec.ports[*].nodePort 字段中要求分配的端口.

如果你想指定特定的 IP 代理端口,则可以设置 kube-proxy 中的 --nodeport-addresses 参数 或者将kube-proxy 配置文件 中的等效 nodePortAddresses 字段设置为特定的 IP 块. 该标志采用逗号分隔的 IP 块列表(例如,10.0.0.0/8、192.0.2.0/25)来指定 kube-proxy 应该认为是此节点本地的 IP 地址范围.

例如,如果你使用 --nodeport-addresses=127.0.0.0/8 标志启动 kube-proxy, 则 kube-proxy 仅选择 NodePort Services 的本地回路接口. --nodeport-addresses 的默认值是一个空列表. 这意味着 kube-proxy 应该考虑 NodePort 的所有可用网络接口. (这也与早期的 Kubernetes 版本兼容).

如果需要特定的端口号,你可以在 nodePort 字段中指定一个值. 控制平面将为你分配该端口或报告 API 事务失败. 这意味着你需要自己注意可能发生的端口冲突. 你还必须使用有效的端口号,该端口号在配置用于 NodePort 的范围内.

使用 NodePort 可以让你自由设置自己的负载均衡解决方案, 配置 Kubernetes 不完全支持的环境, 甚至直接暴露一个或多个节点的 IP.

需要注意的是,Service 能够通过 <NodeIP>:spec.ports[*].nodePort 和 spec.clusterIp:spec.ports[*].port 而对外可见. 如果设置了 kube-proxy 的 --nodeport-addresses 参数或 kube-proxy 配置文件中的等效字段, <NodeIP> 将被过滤 NodeIP.

例如:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: NodePort
  selector:
    app: MyApp
  ports:
      # 默认情况下,为了方便起见,`targetPort` 被设置为与 `port` 字段相同的值.
    - port: 80
      targetPort: 80
      # 可选字段
      # 默认情况下,为了方便起见,Kubernetes 控制平面会从某个范围内分配一个端口号(默认:30000-32767)
      nodePort: 30007

* LoadBalancer 类型 
在使用支持外部负载均衡器的云提供商的服务时,设置 type 的值为 "LoadBalancer", 将为 Service 提供负载均衡器. 负载均衡器是异步创建的,关于被提供的负载均衡器的信息将会通过 Service 的 status.loadBalancer 字段发布出去.

实例:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  clusterIP: 10.0.171.239
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
      - ip: 192.0.2.127

来自外部负载均衡器的流量将直接重定向到后端 Pod 上,不过实际它们是如何工作的,这要依赖于云提供商.

某些云提供商允许设置 loadBalancerIP. 在这些情况下,将根据用户设置的 loadBalancerIP 来创建负载均衡器. 如果没有设置 loadBalancerIP 字段,将会给负载均衡器指派一个临时 IP. 如果设置了 loadBalancerIP,但云提供商并不支持这种特性,那么设置的 loadBalancerIP 值将会被忽略掉.

说明:
在 Azure 上,如果要使用用户指定的公共类型 loadBalancerIP,则 首先需要创建静态类型的公共 IP 地址资源. 此公共 IP 地址资源应与集群中其他自动创建的资源位于同一资源组中. 例如,MC_myResourceGroup_myAKSCluster_eastus.

将分配的 IP 地址设置为 loadBalancerIP.确保你已更新云提供程序配置文件中的 securityGroupName. 有关对 CreatingLoadBalancerFailed 权限问题进行故障排除的信息, 请参阅 与 Azure Kubernetes 服务(AKS)负载平衡器一起使用静态 IP 地址 或在 AKS 集群上使用高级联网时出现 CreatingLoadBalancerFailed.

混合协议类型的负载均衡器
FEATURE STATE: Kubernetes v1.20 [alpha]
默认情况下,对于 LoadBalancer 类型的服务,当定义了多个端口时,所有 端口必须具有相同的协议,并且该协议必须是受云提供商支持的协议.

如果为 kube-apiserver 启用了 MixedProtocolLBService 特性门控, 则当定义了多个端口时,允许使用不同的协议.

说明: 可用于 LoadBalancer 类型服务的协议集仍然由云提供商决定.

* 禁用负载均衡器节点端口分配 
FEATURE STATE: Kubernetes v1.20 [alpha]

设置负载均衡器实现的类别 
FEATURE STATE: Kubernetes v1.22 [beta]

内部负载均衡器 

具体的链接:
https://kubernetes.io/zh/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation

* AWS TLS 支持

具体的链接:
https://kubernetes.io/zh/docs/concepts/services-networking/service/#ssl-support-on-aws

* ExternalName 类型 
类型为 ExternalName 的服务将服务映射到 DNS 名称,而不是典型的选择器,例如 my-service 或者 cassandra. 你可以使用 spec.externalName 参数指定这些服务.

例如,以下 Service 定义将 prod 名称空间中的 my-service 服务映射到 my.database.example.com:

apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
说明: ExternalName 服务接受 IPv4 地址字符串,但作为包含数字的 DNS 名称,而不是 IP 地址. 类似于 IPv4 地址的外部名称不能由 CoreDNS 或 ingress-nginx 解析,因为外部名称旨在指定规范的 DNS 名称. 要对 IP 地址进行硬编码,请考虑使用 headless Services.
当查找主机 my-service.prod.svc.cluster.local 时,集群 DNS 服务返回 CNAME 记录, 其值为 my.database.example.com. 访问 my-service 的方式与其他服务的方式相同,但主要区别在于重定向发生在 DNS 级别,而不是通过代理或转发. 如果以后你决定将数据库移到集群中,则可以启动其 Pod,添加适当的选择器或端点以及更改服务的 type.

警告:
对于一些常见的协议,包括 HTTP 和 HTTPS, 你使用 ExternalName 可能会遇到问题. 如果你使用 ExternalName,那么集群内客户端使用的主机名 与 ExternalName 引用的名称不同.

对于使用主机名的协议,此差异可能会导致错误或意外响应. HTTP 请求将具有源服务器无法识别的 Host: 标头;TLS 服 务器将无法提供与客户端连接的主机名匹配的证书.

* 外部 IP 
如果外部的 IP 路由到集群中一个或多个 Node 上,Kubernetes Service 会被暴露给这些 externalIPs. 通过外部 IP(作为目的 IP 地址)进入到集群,打到 Service 的端口上的流量, 将会被路由到 Service 的 Endpoint 上. externalIPs 不会被 Kubernetes 管理,它属于集群管理员的职责范畴.

根据 Service 的规定,externalIPs 可以同任意的 ServiceType 来一起指定. 在上面的例子中,my-service 可以在 "80.11.12.10:80"(externalIP:port) 上被客户端访问.

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 9376
  externalIPs:
    - 80.11.12.10


** 不足之处
为 VIP 使用用户空间代理,将只适合小型到中型规模的集群,不能够扩展到上千 Service 的大型集群. 查看最初设计方案 获取更多细节.

使用用户空间代理,隐藏了访问 Service 的数据包的源 IP 地址. 这使得一些类型的防火墙无法起作用. iptables 代理不会隐藏 Kubernetes 集群内部的 IP 地址,但却要求客户端请求 必须通过一个负载均衡器或 Node 端口.

Type 字段支持嵌套功能 —— 每一层需要添加到上一层里面. 不会严格要求所有云提供商(例如,GCE 就没必要为了使一个 LoadBalancer 能工作而分配一个 NodePort,但是 AWS 需要 ),但当前 API 是强制要求的.


** 虚拟IP实施
对很多想使用 Service 的人来说,前面的信息应该足够了. 然而,有很多内部原理性的内容,还是值去理解的.

避免冲突
Kubernetes 最主要的哲学之一,是用户不应该暴露那些能够导致他们操作失败、但又不是他们的过错的场景. 对于 Service 资源的设计,这意味着如果用户的选择有可能与他人冲突,那就不要让用户自行选择端口号. 这是一个隔离性的失败.

为了使用户能够为他们的 Service 选择一个端口号,我们必须确保不能有2个 Service 发生冲突. Kubernetes 通过为每个 Service 分配它们自己的 IP 地址来实现.

为了保证每个 Service 被分配到一个唯一的 IP,需要一个内部的分配器能够原子地更新 etcd 中的一个全局分配映射表, 这个更新操作要先于创建每一个 Service. 为了使 Service 能够获取到 IP,这个映射表对象必须在注册中心存在, 否则创建 Service 将会失败,指示一个 IP 不能被分配.

在控制平面中,一个后台 Controller 的职责是创建映射表 (需要支持从使用了内存锁的 Kubernetes 的旧版本迁移过来). 同时 Kubernetes 会通过控制器检查不合理的分配(如管理员干预导致的) 以及清理已被分配但不再被任何 Service 使用的 IP 地址.

Service IP 地址
不像 Pod 的 IP 地址,它实际路由到一个固定的目的地,Service 的 IP 实际上 不能通过单个主机来进行应答. 相反,我们使用 iptables(Linux 中的数据包处理逻辑)来定义一个 虚拟 IP 地址(VIP),它可以根据需要透明地进行重定向. 当客户端连接到 VIP 时,它们的流量会自动地传输到一个合适的 Endpoint. 环境变量和 DNS,实际上会根据 Service 的 VIP 和端口来进行填充.

kube-proxy支持三种代理模式: Userspace,iptables和IPVS;它们各自的操作略有不同.

Userspace 
作为一个例子,考虑前面提到的图片处理应用程序. 当创建后端 Service 时,Kubernetes master 会给它指派一个虚拟 IP 地址,比如 10.0.0.1. 假设 Service 的端口是 1234,该 Service 会被集群中所有的 kube-proxy 实例观察到. 当代理看到一个新的 Service, 它会打开一个新的端口,建立一个从该 VIP 重定向到 新端口的 iptables,并开始接收请求连接.

当一个客户端连接到一个 VIP,iptables 规则开始起作用,它会重定向该数据包到 "服务代理" 的端口. "服务代理" 选择一个后端,并将客户端的流量代理到后端上.

这意味着 Service 的所有者能够选择任何他们想使用的端口,而不存在冲突的风险. 客户端可以连接到一个 IP 和端口,而不需要知道实际访问了哪些 Pod.

iptables
再次考虑前面提到的图片处理应用程序. 当创建后端 Service 时,Kubernetes 控制面板会给它指派一个虚拟 IP 地址,比如 10.0.0.1. 假设 Service 的端口是 1234,该 Service 会被集群中所有的 kube-proxy 实例观察到. 当代理看到一个新的 Service, 它会配置一系列的 iptables 规则,从 VIP 重定向到每个 Service 规则. 该特定于服务的规则连接到特定于 Endpoint 的规则,而后者会重定向(目标地址转译)到后端.

当客户端连接到一个 VIP,iptables 规则开始起作用.一个后端会被选择(或者根据会话亲和性,或者随机), 数据包被重定向到这个后端. 不像用户空间代理,数据包从来不拷贝到用户空间,kube-proxy 不是必须为该 VIP 工作而运行, 并且客户端 IP 是不可更改的.

当流量打到 Node 的端口上,或通过负载均衡器,会执行相同的基本流程, 但是在那些案例中客户端 IP 是可以更改的.

IPVS
在大规模集群(例如 10000 个服务)中,iptables 操作会显着降低速度. IPVS 专为负载平衡而设计,并基于内核内哈希表. 因此,你可以通过基于 IPVS 的 kube-proxy 在大量服务中实现性能一致性. 同时,基于 IPVS 的 kube-proxy 具有更复杂的负载均衡算法(最小连接、局部性、 加权、持久性).



## Service 与 Pod 的 DNS
Kubernetes 为服务和 Pods 创建 DNS 记录. 你可以使用一致的 DNS 名称而非 IP 地址来访问服务.


** 介绍
Kubernetes DNS 在集群上调度 DNS Pod 和服务,并配置 kubelet 以告知各个容器 使用 DNS 服务的 IP 来解析 DNS 名称.

集群中定义的每个 Service (包括 DNS 服务器自身)都被赋予一个 DNS 名称. 默认情况下,客户端 Pod 的 DNS 搜索列表会包含 Pod 自身的名字空间和集群 的默认域.

* Service 的命名空间
概括起来,名字空间 test 中的 Pod 可以成功地解析 data.prod 或者 data.prod.svc.cluster.local.

* DNS 记录 
哪些对象会获得 DNS 记录呢?

  1. Services
  2. Pods

* 服务 
A/AAAA 记录
"普通" 服务(除了无头服务)会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被分配一个 DNS A 或 AAAA 记录,取决于服务的 IP 协议族. 该名称会解析成对应服务的集群 IP.

"无头(Headless)" 服务(没有集群 IP)也会以 my-svc.my-namespace.svc.cluster-domain.example 这种名字的形式被指派一个 DNS A 或 AAAA 记录, 具体取决于服务的 IP 协议族. 与普通服务不同,这一记录会被解析成对应服务所选择的 Pod 集合的 IP. 客户端要能够使用这组 IP,或者使用标准的轮转策略从这组 IP 中进行选择.

SRV 记录 
Kubernetes 会为命名端口创建 SRV 记录,这些端口是普通服务或 无头服务的一部分. 对每个命名端口,SRV 记录具有 _my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example 这种形式. 对普通服务,该记录会被解析成端口号和域名:my-svc.my-namespace.svc.cluster-domain.example. 对无头服务,该记录会被解析成多个结果,服务对应的每个后端 Pod 各一个; 其中包含 Pod 端口号和形为 auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example 的域名.


** Pods
* A/AAAA 记录
一般而言,Pod 会对应如下 DNS 名字解析:

pod-ip-address.my-namespace.pod.cluster-domain.example

例如,对于一个位于 default 名字空间,IP 地址为 172.17.0.3 的 Pod, 如果集群的域名为 cluster.local,则 Pod 会对应 DNS 名称:

172-17-0-3.default.pod.cluster.local.

通过 Service 暴露出来的所有 Pod 都会有如下 DNS 解析名称可用:

pod-ip-address.service-name.my-namespace.svc.cluster-domain.example.

* Pod 的 hostname 和 subdomain 字段
当前,创建 Pod 时其主机名取自 Pod 的 metadata.name 值.

Pod 规约中包含一个可选的 hostname 字段,可以用来指定 Pod 的主机名. 当这个字段被设置时,它将优先于 Pod 的名字成为该 Pod 的主机名. 举个例子,给定一个 hostname 设置为 "my-host" 的 Pod, 该 Pod 的主机名将被设置为 "my-host".

Pod 规约还有一个可选的 subdomain 字段,可以用来指定 Pod 的子域名. 举个例子,某 Pod 的 hostname 设置为 "foo",subdomain 设置为 "bar", 在名字空间 "my-namespace" 中对应的完全限定域名(FQDN)为 "foo.bar.my-namespace.svc.cluster-domain.example".

示例:

apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo # 实际上不需要指定端口号
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox2
  labels:
    name: busybox
spec:
  hostname: busybox-2
  subdomain: default-subdomain
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    name: busybox

如果某无头服务与某 Pod 在同一个名字空间中,且它们具有相同的子域名, 集群的 DNS 服务器也会为该 Pod 的全限定主机名返回 A 记录或 AAAA 记录. 例如,在同一个名字空间中,给定一个主机名为 "busybox-1"、 子域名设置为 "default-subdomain" 的 Pod,和一个名称为 "default-subdomain" 的无头服务,Pod 将看到自己的 FQDN 为 "busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example". DNS 会为此名字提供一个 A 记录或 AAAA 记录,指向该 Pod 的 IP. "busybox1" 和 "busybox2" 这两个 Pod 分别具有它们自己的 A 或 AAAA 记录.

Endpoints 对象可以为任何端点地址及其 IP 指定 hostname.

说明:
因为没有为 Pod 名称创建 A 记录或 AAAA 记录,所以要创建 Pod 的 A 记录 或 AAAA 记录需要 hostname.

没有设置 hostname 但设置了 subdomain 的 Pod 只会为 无头服务创建 A 或 AAAA 记录(default-subdomain.my-namespace.svc.cluster-domain.example) 指向 Pod 的 IP 地址. 另外,除非在服务上设置了 publishNotReadyAddresses=True,否则只有 Pod 进入就绪状态 才会有与之对应的记录.

* Pod 的 setHostnameAsFQDN 字段 
FEATURE STATE: Kubernetes v1.22 [stable]
前置条件:SetHostnameAsFQDN 特性门控 必须在 API 服务器 上启用.

当你在 Pod 规约中设置了 setHostnameAsFQDN: true 时,kubelet 会将 Pod 的全限定域名(FQDN)作为该 Pod 的主机名记录到 Pod 所在名字空间. 在这种情况下,hostname 和 hostname --fqdn 都会返回 Pod 的全限定域名.

说明:
在 Linux 中,内核的主机名字段(struct utsname 的 nodename 字段)限定 最多 64 个字符.

如果 Pod 启用这一特性,而其 FQDN 超出 64 字符,Pod 的启动会失败. Pod 会一直出于 Pending 状态(通过 kubectl 所看到的 ContainerCreating), 并产生错误事件,例如 "Failed to construct FQDN from pod hostname and cluster domain, FQDN long-FQDN is too long (64 characters is the max, 70 characters requested)." (无法基于 Pod 主机名和集群域名构造 FQDN,FQDN long-FQDN 过长,至多 64 字符,请求字符数为 70). 对于这种场景而言,改善用户体验的一种方式是创建一个 准入 Webhook 控制器, 在用户创建顶层对象(如 Deployment)的时候控制 FQDN 的长度.

* Pod 的 DNS 策略 
DNS 策略可以逐个 Pod 来设定.目前 Kubernetes 支持以下特定 Pod 的 DNS 策略. 这些策略可以在 Pod 规约中的 dnsPolicy 字段设置:

  * "Default": Pod 从运行所在的节点继承名称解析配置.参考 相关讨论 获取更多信息.
  * "ClusterFirst": 与配置的集群域后缀不匹配的任何 DNS 查询(例如 "www.kubernetes.io") 都将转发到从节点继承的上游名称服务器.集群管理员可能配置了额外的存根域和上游 DNS 服务器. 参阅相关讨论 了解在这些场景中如何处理 DNS 查询的信息.
  * "ClusterFirstWithHostNet":对于以 hostNetwork 方式运行的 Pod,应显式设置其 DNS 策略 "ClusterFirstWithHostNet".
  * "None": 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置.Pod 会使用其 dnsConfig 字段 所提供的 DNS 设置. 参见 Pod 的 DNS 配置节.

说明: "Default" 不是默认的 DNS 策略.如果未明确指定 dnsPolicy,则使用 "ClusterFirst".

下面的示例显示了一个 Pod,其 DNS 策略设置为 "ClusterFirstWithHostNet", 因为它已将 hostNetwork 设置为 true.

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox:1.28
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet

* Pod 的 DNS 配置 
FEATURE STATE: Kubernetes v1.14 [stable]

Pod 的 DNS 配置可让用户对 Pod 的 DNS 设置进行更多控制.

dnsConfig 字段是可选的,它可以与任何 dnsPolicy 设置一起使用. 但是,当 Pod 的 dnsPolicy 设置为 "None" 时,必须指定 dnsConfig 字段.

用户可以在 dnsConfig 字段中指定以下属性:

  * nameservers:将用作于 Pod 的 DNS 服务器的 IP 地址列表. 最多可以指定 3 个 IP 地址.当 Pod 的 dnsPolicy 设置为 "None" 时, 列表必须至少包含一个 IP 地址,否则此属性是可选的. 所列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器,并删除重复的地址.

  * searches:用于在 Pod 中查找主机名的 DNS 搜索域的列表.此属性是可选的. 指定此属性时,所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中. 重复的域名将被删除.Kubernetes 最多允许 6 个搜索域.

  * options:可选的对象列表,其中每个对象可能具有 name 属性(必需)和 value 属性(可选). 此属性中的内容将合并到从指定的 DNS 策略生成的选项. 重复的条目将被删除.

以下是具有自定义 DNS 设置的 Pod 示例:

service/networking/custom-dns.yaml 
apiVersion: v1
kind: Pod
metadata:
  namespace: default
  name: dns-example
spec:
  containers:
    - name: test
      image: nginx
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 1.2.3.4
    searches:
      - ns1.svc.cluster-domain.example
      - my.dns.search.suffix
    options:
      - name: ndots
        value: "2"
      - name: edns0

创建上面的 Pod 后,容器 test 会在其 /etc/resolv.conf 文件中获取以下内容:

nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0

对于 IPv6 设置,搜索路径和名称服务器应按以下方式设置:

kubectl exec -it dns-example -- cat /etc/resolv.conf

输出类似于

nameserver fd00:79:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5

扩展 DNS 配置 
FEATURE STATE: Kubernetes 1.22 [alpha]
对于 Pod DNS 配置,Kubernetes 默认允许最多 6 个 搜索域( Search Domain) 以及一个最多 256 个字符的搜索域列表.

如果启用 kube-apiserver 和 kubelet 的特性门控 ExpandedDNSConfig,Kubernetes 将可以有最多 32 个 搜索域以及一个最多 2048 个字符的搜索域列表.

* 功能的可用性
Pod DNS 配置和 DNS 策略 "None" 的可用版本对应如下所示.

k8s 版本	特性支持
1.14	        稳定
1.10	        Beta(默认启用)
1.9	            Alpha


^^^^^^^^^^^^^^^^^^^^
原文链接:
https://kubernetes.io/zh-cn/docs/tutorials/services/connect-applications-service/

## Kubernetes 连接容器的模型 
既然有了一个持续运行、可复制的应用,我们就能够将它暴露到网络上.

本指南使用一个简单的 Nginx 服务器来演示概念验证原型.

** 在集群中暴露 Pod
我们在之前的示例中已经做过,然而让我们以网络连接的视角再重做一遍. 创建一个 Nginx Pod,注意其中包含一个容器端口的规约:

service/networking/run-my-nginx.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
spec:
  selector:
    matchLabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80

这使得可以从集群中任何一个节点来访问它.检查节点,该 Pod 正在运行:

kubectl apply -f ./run-my-nginx.yaml
kubectl get pods -l run=my-nginx -o wide

NAME                        READY     STATUS    RESTARTS   AGE       IP            NODE
my-nginx-3800858182-jr4a2   1/1       Running   0          13s       10.244.3.4    kubernetes-minion-905m
my-nginx-3800858182-kna2y   1/1       Running   0          13s       10.244.2.5    kubernetes-minion-ljyd

检查 Pod 的 IP 地址:

kubectl get pods -l run=my-nginx -o yaml | grep podIP
    podIP: 10.244.3.4
    podIP: 10.244.2.5

你应该能够通过 ssh 登录到集群中的任何一个节点上,并使用诸如 curl 之类的工具向这两个 IP 地址发出查询请求. 需要注意的是,容器不会使用该节点上的 80 端口,也不会使用任何特定的 NAT 规则去路由流量到 Pod 上. 这意味着可以在同一个节点上运行多个 Nginx Pod,使用相同的 containerPort,并且可以从集群中任何其他的 Pod 或节点上使用 IP 的方式访问到它们. 如果你想的话,你依然可以将宿主节点的某个端口的流量转发到 Pod 中,但是出于网络模型的原因,你不必这么做.


** 创建 Service
我们有一组在一个扁平的、集群范围的地址空间中运行 Nginx 服务的 Pod. 理论上,你可以直接连接到这些 Pod,但如果某个节点死掉了会发生什么呢? Pod 会终止,Deployment 将创建新的 Pod,且使用不同的 IP.这正是 Service 要解决的问题.

Kubernetes Service 是集群中提供相同功能的一组 Pod 的抽象表达. 当每个 Service 创建时,会被分配一个唯一的 IP 地址(也称为 clusterIP). 这个 IP 地址与 Service 的生命周期绑定在一起,只要 Service 存在,它就不会改变. 可以配置 Pod 使它与 Service 进行通信,Pod 知道与 Service 通信将被自动地负载均衡到该 Service 中的某些 Pod 上.

可以使用 kubectl expose 命令为 2个 Nginx 副本创建一个 Service:

kubectl expose deployment/my-nginx

service/my-nginx exposed

这等价于使用 kubectl create -f 命令及如下的 yaml 文件创建:

service/networking/nginx-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    run: my-nginx

上述规约将创建一个 Service,该 Service 会将所有具有标签 run: my-nginx 的 Pod 的 TCP 80 端口暴露到一个抽象的 Service 端口上(targetPort:容器接收流量的端口;port:可任意取值的抽象的 Service 端口,其他 Pod 通过该端口访问 Service).
查看你的 Service 资源:

kubectl get svc my-nginx

NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
my-nginx   ClusterIP   10.0.162.149   <none>        80/TCP    21s

正如前面所提到的,一个 Service 由一组 Pod 提供支撑.这些 Pod 通过 endpoints 暴露出来. Service Selector 将持续评估,结果被 POST 到一个名称为 my-nginx 的 Endpoint 对象上. 当 Pod 终止后,它会自动从 Endpoint 中移除,新的能够匹配上 Service Selector 的 Pod 将自动地被添加到 Endpoint 中. 检查该 Endpoint,注意到 IP 地址与在第一步创建的 Pod 是相同的.

kubectl describe svc my-nginx

Name:                my-nginx
Namespace:           default
Labels:              run=my-nginx
Annotations:         <none>
Selector:            run=my-nginx
Type:                ClusterIP
IP:                  10.0.162.149
Port:                <unset> 80/TCP
Endpoints:           10.244.2.5:80,10.244.3.4:80
Session Affinity:    None
Events:              <none>

kubectl get ep my-nginx

NAME       ENDPOINTS                     AGE
my-nginx   10.244.2.5:80,10.244.3.4:80   1m

现在,你应该能够从集群中任意节点上使用 curl 命令向 <CLUSTER-IP>:<PORT> 发送请求以访问 Nginx Service. 注意 Service IP 完全是虚拟的,它从来没有走过网络


** 访问 Service
Kubernetes支持两种查找服务的主要模式: 环境变量和 DNS.

说明: 如果不需要服务环境变量(因为可能与预期的程序冲突,可能要处理的变量太多,或者仅使用DNS等),则可以通过在 pod spec 上将 enableServiceLinks 标志设置为 false 来禁用此模式.

* 环境变量
当 Pod 在节点上运行时,kubelet 会针对每个活跃的 Service 为 Pod 添加一组环境变量. 这就引入了一个顺序的问题.为解释这个问题,让我们先检查正在运行的 Nginx Pod 的环境变量(你的环境中的 Pod 名称将会与下面示例命令中的不同):

kubectl exec my-nginx-3800858182-jr4a2 -- printenv | grep SERVICE

KUBERNETES_SERVICE_HOST=10.0.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443

能看到环境变量中并没有你创建的 Service 相关的值.这是因为副本的创建先于 Service. 这样做的另一个缺点是,调度器可能会将所有 Pod 部署到同一台机器上,如果该机器宕机则整个 Service 都会离线. 要改正的话,我们可以先终止这 2 个 Pod,然后等待 Deployment 去重新创建它们. 这次 Service 会先于副本存在.这将实现调度器级别的 Pod 按 Service 分布(假定所有的节点都具有同样的容量),并提供正确的环境变量:

kubectl scale deployment my-nginx --replicas=0; kubectl scale deployment my-nginx --replicas=2;

kubectl get pods -l run=my-nginx -o wide

NAME                        READY     STATUS    RESTARTS   AGE     IP            NODE
my-nginx-3800858182-e9ihh   1/1       Running   0          5s      10.244.2.7    kubernetes-minion-ljyd
my-nginx-3800858182-j4rm4   1/1       Running   0          5s      10.244.3.8    kubernetes-minion-905m

你可能注意到,Pod 具有不同的名称,这是因为它们是被重新创建的.

kubectl exec my-nginx-3800858182-e9ihh -- printenv | grep SERVICE

KUBERNETES_SERVICE_PORT=443
MY_NGINX_SERVICE_HOST=10.0.162.149
KUBERNETES_SERVICE_HOST=10.0.0.1
MY_NGINX_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT_HTTPS=443

* DNS
Kubernetes 提供了一个自动为其它 Service 分配 DNS 名字的 DNS 插件 Service. 你可以通过如下命令检查它是否在工作:

kubectl get services kube-dns --namespace=kube-system

NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterIP   10.0.0.10    <none>        53/UDP,53/TCP   8m

本段剩余的内容假设你已经有一个拥有持久 IP 地址的 Service(my-nginx),以及一个为其 IP 分配名称的 DNS 服务器. 这里我们使用 CoreDNS 集群插件(应用名为 kube-dns), 所以在集群中的任何 Pod 中,你都可以使用标准方法(例如:gethostbyname())与该 Service 通信. 如果 CoreDNS 没有在运行,你可以参照 CoreDNS README 或者安装 CoreDNS 来启用它. 让我们运行另一个 curl 应用来进行测试:

kubectl run curl --image=radial/busyboxplus:curl -i --tty

Waiting for pod default/curl-131556218-9fnch to be running, status is Pending, pod ready: false
Hit enter for command prompt

然后,按回车并执行命令 nslookup my-nginx:

[ root@curl-131556218-9fnch:/ ]$ nslookup my-nginx
Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      my-nginx
Address 1: 10.0.162.149


** 保护 Service
到现在为止,我们只在集群内部访问了 Nginx 服务器.在将 Service 暴露到因特网之前,我们希望确保通信信道是安全的. 为实现这一目的,需要:

  * 用于 HTTPS 的自签名证书(除非已经有了一个身份证书)
  * 使用证书配置的 Nginx 服务器
  * 使 Pod 可以访问证书的 Secret

重要
你可以从 Nginx https 示例获取所有上述内容. 你需要安装 go 和 make 工具.如果你不想安装这些软件,可以按照后文所述的手动执行步骤执行操作.
Nginx https  示例如下:
https://github.com/kubernetes/examples/tree/master/staging/https-nginx/

手动执行简要过程如下:
https://kubernetes.io/zh/docs/concepts/services-networking/connect-applications-service/


** 暴露 Service
对应用的某些部分,你可能希望将 Service 暴露在一个外部 IP 地址上. Kubernetes 支持两种实现方式:NodePort 和 LoadBalancer. 在上一段创建的 Service 使用了 NodePort,因此,如果你的节点有一个公网 IP,那么 Nginx HTTPS 副本已经能够处理因特网上的流量.

kubectl get svc my-nginx -o yaml | grep nodePort -C 5
  uid: 07191fb3-f61a-11e5-8ae5-42010af00002
spec:
  clusterIP: 10.0.162.149
  ports:
  - name: http
    nodePort: 31704
    port: 8080
    protocol: TCP
    targetPort: 80
  - name: https
    nodePort: 32453
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    run: my-nginx
kubectl get nodes -o yaml | grep ExternalIP -C 1
    - address: 104.197.41.11
      type: ExternalIP
    allocatable:
--
    - address: 23.251.152.56
      type: ExternalIP
    allocatable:
...

$ curl https://<EXTERNAL-IP>:<NODE-PORT> -k
...
<h1>Welcome to nginx!</h1>

让我们重新创建一个 Service 以使用云负载均衡器. 将 my-nginx Service 的 Type 由 NodePort 改成 LoadBalancer:

kubectl edit svc my-nginx
kubectl get svc my-nginx

NAME        TYPE           CLUSTER-IP     EXTERNAL-IP        PORT(S)               AGE
my-nginx   LoadBalancer   10.0.162.149   xx.xxx.xxx.xxx     8080:30163/TCP        21s
curl https://<EXTERNAL-IP> -k
...
<title>Welcome to nginx!</title>
在 EXTERNAL-IP 列中的 IP 地址能在公网上被访问到.CLUSTER-IP 只能从集群/私有云网络中访问.

vvvvvvvvvvvvvvvvvvvv

## Ingress
FEATURE STATE: Kubernetes v1.19 [stable]
Ingress 是对集群中服务的外部访问进行管理的 API 对象,典型的访问方式是 HTTP.

Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管.


** 术语
为了表达更加清晰,本指南定义了以下术语:

  * 节点(Node): Kubernetes 集群中的一台工作机器,是集群的一部分.
  * 集群(Cluster): 一组运行由 Kubernetes 管理的容器化应用程序的节点. 在此示例和在大多数常见的 Kubernetes 部署环境中,集群中的节点都不在公共网络中.
  * 边缘路由器(Edge Router): 在集群中强制执行防火墙策略的路由器.可以是由云提供商管理的网关,也可以是物理硬件.
  * 集群网络(Cluster Network): 一组逻辑的或物理的连接,根据 Kubernetes 网络模型在集群内实现通信.
  * 服务(Service):Kubernetes 服务(Service),使用标签选择器(selectors)辨认一组 Pod. 除非另有说明,否则假定服务只具有在集群网络中可路由的虚拟 IP.


** Ingress 是什么?
Ingress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由. 流量路由由 Ingress 资源上定义的规则控制.

下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例:

                                               |----------------------------------------------------------------------|                                                                
                                               |                               cluster                       |------> Pod      |
                 Ingress-管理的     |                                                                 |                        |
客户端 ---------------------------|--> Ingress --- 路由规则 ---> Service--- |                        |
                   负载均衡器         |                                                                 |                        |   
                                                |                                                                 |------> Pod      |
                                                |----------------------------------------------------------------------|

Ingress 可为 Service 提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS,以及基于名称的虚拟托管. Ingress 控制器 通常负责通过负载均衡器来实现 Ingress,尽管它也可以配置边缘路由器或其他前端来帮助处理流量.

Ingress 不会公开任意端口或协议. 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时,通常使用 Service.Type=NodePort 或 Service.Type=LoadBalancer 类型的 Service.


** 环境准备
你必须拥有一个 Ingress 控制器 才能满足 Ingress 的要求. 仅创建 Ingress 资源本身没有任何效果.

你可能需要部署 Ingress 控制器,例如 ingress-nginx. 你可以从许多 Ingress 控制器 中进行选择.

理想情况下,所有 Ingress 控制器都应符合参考规范.但实际上,不同的 Ingress 控制器操作略有不同.

说明: 确保你查看了 Ingress 控制器的文档,以了解选择它的注意事项.

ingress-nginx
https://kubernetes.github.io/ingress-nginx/deploy/

Ingress 控制器
https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers


** Ingress 资源 - ingress 定义的是一组规则,即转发规则,后续的 ingress controller 则是具体的程序,比如 nginx, 两者就像在 nginx 中定义的转发规则,和 nginx 这个软件本身,二者是分开的

一个最小的 Ingress 资源示例:

service/networking/minimal-ingress.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: minimal-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        pathType: Prefix
        backend:
          service:
            name: test
            port:
              number: 80

与所有其他 Kubernetes 资源一样,Ingress 需要指定 apiVersion、kind 和 metadata 字段. Ingress 对象的命名必须是合法的 DNS 子域名名称. 关于如何使用配置文件,请参见部署应用、 配置容器、 管理资源. Ingress 经常使用注解(annotations)来配置一些选项,具体取决于 Ingress 控制器,例如重写目标注解. 不同的 Ingress 控制器支持不同的注解. 查看你所选的 Ingress 控制器的文档,以了解其支持哪些注解.

Ingress 规约 提供了配置负载均衡器或者代理服务器所需的所有信息. 最重要的是,其中包含与所有传入请求匹配的规则列表. Ingress 资源仅支持用于转发 HTTP(S) 流量的规则.

如果 ingressClassName 被省略,那么你应该定义一个默认 Ingress 类.

有一些 Ingress 控制器不需要定义默认的 IngressClass.比如:Ingress-NGINX 控制器可以通过参数 --watch-ingress-without-class 来配置. 不过仍然推荐 按下文所示来设置默认的 IngressClass.

* Ingress 规则 
每个 HTTP 规则都包含以下信息:

  * 可选的 host.在此示例中,未指定 host,因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信. 如果提供了 host(例如 foo.bar.com),则 rules 适用于该 host.
  * 路径列表 paths(例如,/testpath),每个路径都有一个由 serviceName 和 servicePort 定义的关联后端. 在负载均衡器将流量定向到引用的服务之前,主机和路径都必须匹配传入请求的内容.
  * backend(后端)是 Service 文档中所述的服务和端口名称的组合. 与规则的 host 和 path 匹配的对 Ingress 的 HTTP(和 HTTPS )请求将发送到列出的 backend.

通常在 Ingress 控制器中会配置 defaultBackend(默认后端),以服务于无法与规约中 path 匹配的所有请求.

* 默认后端 
没有设置规则的 Ingress 将所有流量发送到同一个默认后端,而 .spec.defaultBackend 则是在这种情况下处理请求的那个默认后端. defaultBackend 通常是 Ingress 控制器的配置选项,而非在 Ingress 资源中指定. 如果未设置任何的 .spec.rules,那么必须指定 .spec.defaultBackend. 如果未设置 defaultBackend,那么如何处理所有与规则不匹配的流量将交由 Ingress 控制器决定(请参考你的 Ingress 控制器的文档以了解它是如何处理那些流量的).

如果没有 hosts 或 paths 与 Ingress 对象中的 HTTP 请求匹配,则流量将被路由到默认后端.

* 资源后端 
Resource 后端是一个引用,指向同一命名空间中的另一个 Kubernetes 资源,将其作为 Ingress 对象. Resource 后端与 Service 后端是互斥的,在二者均被设置时会无法通过合法性检查. Resource 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端.

service/networking/ingress-resource-backend.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-resource-backend
spec:
  defaultBackend:
    resource:
      apiGroup: k8s.example.com
      kind: StorageBucket
      name: static-assets
  rules:
    - http:
        paths:
          - path: /icons
            pathType: ImplementationSpecific
            backend:
              resource:
                apiGroup: k8s.example.com
                kind: StorageBucket
                name: icon-assets

创建了如上的 Ingress 之后,你可以使用下面的命令查看它:

kubectl describe ingress ingress-resource-backend

* 路径类型 
Ingress 中的每个路径都需要有对应的路径类型(Path Type).未明确设置 pathType 的路径无法通过合法性检查.当前支持的路径类型有三种:

  * ImplementationSpecific:对于这种路径类型,匹配方法取决于 IngressClass. 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理.

 * Exact:精确匹配 URL 路径,且区分大小写.

  * Prefix:基于以 / 分隔的 URL 路径前缀匹配.匹配区分大小写,并且对路径中的元素逐个完成. 路径元素指的是由 / 分隔符分隔的路径中的标签列表. 如果每个 p 都是请求路径 p 的元素前缀,则请求与路径 p 匹配.

说明: 如果路径的最后一个元素是请求路径中最后一个元素的子字符串,则不会匹配 (例如:/foo/bar 匹配 /foo/bar/baz, 但不匹配 /foo/barbaz).

* 示例
类型	  路径	                                   请求路径	      匹配与否?
Prefix	/	                                           (所有路径)	    是
Exact	  /foo	                                      /foo	                  是
Exact	  /foo	                                      /bar	                  否
Exact	  /foo	                                      /foo/	                否
Exact	  /foo/	                                    /foo	                  否
Prefix	/foo	                                      /foo, /foo/	        是
Prefix	/foo/	                                    /foo, /foo/	        是
Prefix	/aaa/bb	                                /aaa/bbb	          否
Prefix	/aaa/bbb	                              /aaa/bbb	          是
Prefix	/aaa/bbb/	                              /aaa/bbb	          是,忽略尾部斜线
Prefix	/aaa/bbb	                              /aaa/bbb/	          是,匹配尾部斜线
Prefix	/aaa/bbb	                              /aaa/bbb/ccc	    是,匹配子路径
Prefix	/aaa/bbb	                              /aaa/bbbxyz	    否,字符串前缀不匹配
Prefix	/, /aaa	                                  /aaa/ccc	            是,匹配 /aaa 前缀
Prefix	/, /aaa, /aaa/bbb	                  /aaa/bbb	          是,匹配 /aaa/bbb 前缀
Prefix	/, /aaa, /aaa/bbb	                  /ccc	                  是,匹配 / 前缀
Prefix	/aaa	                                      /ccc	                  否,使用默认后端
混合	  /foo (Prefix), /foo (Exact)	  /foo	                  是,优选 Exact 类型

多重匹配 
在某些情况下,Ingress 中的多条路径会匹配同一个请求. 这种情况下最长的匹配路径优先. 如果仍然有两条同等的匹配路径,则精确路径类型优先于前缀路径类型.


** 主机名通配符 
主机名可以是精确匹配(例如"foo.bar.com")或者使用通配符来匹配 (例如"*.foo.com"). 精确匹配要求 HTTP host 头部字段与 host 字段值完全匹配. 通配符匹配则要求 HTTP host 头部字段与通配符规则中的后缀部分相同.

  主机	        host 头部	          匹配与否?
  *.foo.com	bar.foo.com	        基于相同的后缀匹配
  *.foo.com	baz.bar.foo.com	不匹配,通配符仅覆盖了一个 DNS 标签
  *.foo.com	foo.com	              不匹配,通配符仅覆盖了一个 DNS 标签

service/networking/ingress-wildcard-host.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host
spec:
  rules:
  - host: "foo.bar.com"
    http:
      paths:
      - pathType: Prefix
        path: "/bar"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: "*.foo.com"
    http:
      paths:
      - pathType: Prefix
        path: "/foo"
        backend:
          service:
            name: service2
            port:
              number: 80


** Ingress 类 - 不同类型的 Ingress Controller 对应的 Ingress 配置通常也是不同的,当集群中存在多于一个的 Ingress Controller 时,就需要为 Ingress 指定对应的 Controller 是哪个.

Ingress 可以由不同的控制器实现,通常使用不同的配置. 每个 Ingress 应当指定一个类,也就是一个对 IngressClass 资源的引用. IngressClass 资源包含额外的配置,其中包括应当实现该类的控制器名称.

service/networking/external-lb.yaml 

apiVersion: networking.k8s.io/v1

kind: IngressClass
metadata:
  name: external-lb
spec:
  controller: example.com/ingress-controller
  parameters:
    apiGroup: k8s.example.com
    kind: IngressParameters
    name: external-lb
IngressClass 中的 .spec.parameters 字段可用于引用其他资源以提供额外的相关配置.

参数(parameters)的具体类型取决于你在 .spec.controller 字段中指定的 Ingress 控制器.

* IngressClass 的作用域
取决于你的 Ingress 控制器,你可能可以使用集群范围设置的参数或某个名字空间范围的参数.

集群作用域
IngressClass 的参数默认是集群范围的.

如果你设置了 .spec.parameters 字段且未设置 .spec.parameters.scope 字段,或是将 .spec.parameters.scope 字段设为了 Cluster,那么该 IngressClass 所指代的即是一个集群作用域的资源. 参数的 kind(和 apiGroup 一起)指向一个集群作用域的 API(可能是一个定制资源(Custom Resource)),而它的 name 则为此 API 确定了一个具体的集群作用域的资源.

示例:

---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: external-lb-1
spec:
  controller: example.com/ingress-controller
  parameters:
    # 此 IngressClass 的配置定义在一个名为 "external-config-1" 的
    # ClusterIngressParameter(API 组为 k8s.example.net)资源中.
    # 这项定义告诉 Kubernetes 去寻找一个集群作用域的参数资源.
    scope: Cluster
    apiGroup: k8s.example.net
    kind: ClusterIngressParameter
    name: external-config-1

命名空间作用域
FEATURE STATE: Kubernetes v1.23 [stable]

如果你设置了 .spec.parameters 字段且将 .spec.parameters.scope 字段设为了 Namespace,那么该 IngressClass 将会引用一个命名空间作用域的资源. .spec.parameters.namespace 必须和此资源所处的命名空间相同.

参数的 kind(和 apiGroup 一起)指向一个命名空间作用域的 API(例如:ConfigMap),而它的 name 则确定了一个位于你指定的命名空间中的具体的资源.

命名空间作用域的参数帮助集群操作者将控制细分到用于工作负载的各种配置中(比如:负载均衡设置、API 网关定义).如果你使用集群作用域的参数,那么你必须从以下两项中选择一项执行:

每次修改配置,集群操作团队需要批准其他团队的修改.
集群操作团队定义具体的准入控制,比如 RBAC 角色与角色绑定,以使得应用程序团队可以修改集群作用域的配置参数资源.
IngressClass API 本身是集群作用域的.

这里是一个引用命名空间作用域的配置参数的 IngressClass 的示例:

---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: external-lb-2
spec:
  controller: example.com/ingress-controller
  parameters:
    # 此 IngressClass 的配置定义在一个名为 "external-config" 的
    # IngressParameter(API 组为 k8s.example.com)资源中,
    # 该资源位于 "external-configuration" 命名空间中.
    scope: Namespace
    apiGroup: k8s.example.com
    kind: IngressParameter
    namespace: external-configuration
    name: external-config

* 废弃的注解 
在 Kubernetes 1.18 版本引入 IngressClass 资源和 ingressClassName 字段之前,Ingress 类是通过 Ingress 中的一个 kubernetes.io/ingress.class 注解来指定的. 这个注解从未被正式定义过,但是得到了 Ingress 控制器的广泛支持.

Ingress 中新的 ingressClassName 字段是该注解的替代品,但并非完全等价. 该注解通常用于引用实现该 Ingress 的控制器的名称,而这个新的字段则是对一个包含额外 Ingress 配置的 IngressClass 资源的引用,包括 Ingress 控制器的名称.

* 默认 Ingress 类 
你可以将一个特定的 IngressClass 标记为集群默认 Ingress 类. 将一个 IngressClass 资源的 ingressclass.kubernetes.io/is-default-class 注解设置为 true 将确保新的未指定 ingressClassName 字段的 Ingress 能够分配为这个默认的 IngressClass.

注意: 如果集群中有多个 IngressClass 被标记为默认,准入控制器将阻止创建新的未指定 ingressClassName 的 Ingress 对象. 解决这个问题只需确保集群中最多只能有一个 IngressClass 被标记为默认.
有一些 Ingress 控制器不需要定义默认的 IngressClass.比如:Ingress-NGINX 控制器可以通过参数 --watch-ingress-without-class 来配置. 不过仍然推荐 设置默认的 IngressClass.

service/networking/default-ingressclass.yaml 

apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    app.kubernetes.io/component: controller
  name: nginx-example
  annotations:
    ingressclass.kubernetes.io/is-default-class: "true"
spec:
  controller: k8s.io/ingress-nginx


** Ingress 类型 
* 由单个 Service 来完成的 Ingress 
现有的 Kubernetes 概念允许你暴露单个 Service (参见替代方案). 你也可以通过指定无规则的 默认后端 来对 Ingress 进行此操作.

service/networking/test-ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
spec:
  defaultBackend:
    service:
      name: test
      port:
        number: 80

如果使用 kubectl apply -f 创建此 Ingress,则应该能够查看刚刚添加的 Ingress 的状态:

kubectl get ingress test-ingress

NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s

其中 203.0.113.123 是由 Ingress 控制器分配以满足该 Ingress 的 IP.

说明: 入口控制器和负载平衡器可能需要一两分钟才能分配 IP 地址. 在此之前,你通常会看到地址字段的值被设定为 <pending>.

* 简单扇出 
一个扇出(fanout)配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service. Ingress 允许你将负载均衡器的数量降至最低.例如,这样的设置:
                                                                                             cluster
                                                                                                                                                                     |------> pod
                                                                                              |------ /foo -----> Service service1:4200 ---|  
               ingress-管理的                                                     |                                                                      |------> pod
 客户端 ----------------------> ingress, 178.91.123.132 -- |
                负载均衡器器                                                      |                                                                       |------> pod
                                                                                              |------- /bar -----> Service service2:8080 --- |
                                                                                                                                                                      |------> pod


将需要一个如下所示的 Ingress:

service/networking/simple-fanout-example.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: simple-fanout-example
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        pathType: Prefix
        backend:
          service:
            name: service1
            port:
              number: 4200
      - path: /bar
        pathType: Prefix
        backend:
          service:
            name: service2
            port:
              number: 8080

当你使用 kubectl apply -f 创建 Ingress 时:

kubectl describe ingress simple-fanout-example

Ingress 控制器将提供实现特定的负载均衡器来满足 Ingress,只要 Service (service1,service2) 存在. 当它这样做时,你会在 Address 字段看到负载均衡器的地址.

说明: 取决于你所使用的 Ingress 控制器,你可能需要创建默认 HTTP 后端服务.

* 基于名称的虚拟托管 
基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上.

                                                                                             cluster
                                                                                                                                                                                            |------> pod
                                                                                              |------ Host: foo.bar.com -----> Service service1:4200 --- |  
               ingress-管理的                                                     |                                                                                              |------> pod
 客户端 ----------------------> ingress, 178.91.123.132 -- |
                负载均衡器器                                                      |                                                                                              |------> pod
                                                                                              |------- Host: bar.foo.com -----> Service service2:8080 --- |
                                                                                                                                                                                             |------> pod

以下 Ingress 让后台负载均衡器基于host 头部字段 来路由请求.

service/networking/name-virtual-host-ingress.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: bar.foo.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80

如果你创建的 Ingress 资源没有在 rules 中定义的任何 hosts,则可以匹配指向 Ingress 控制器 IP 地址的任何网络流量,而无需基于名称的虚拟主机.

例如,以下 Ingress 会将请求 first.bar.com 的流量路由到 service1,将请求 second.bar.com 的流量路由到 service2,而所有其他流量都会被路由到 service3.

service/networking/name-virtual-host-ingress-no-third-host.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: name-virtual-host-ingress-no-third-host
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service1
            port:
              number: 80
  - host: second.bar.com
    http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service2
            port:
              number: 80
  - http:
      paths:
      - pathType: Prefix
        path: "/"
        backend:
          service:
            name: service3
            port:
              number: 80

* TLS
你可以通过设定包含 TLS 私钥和证书的Secret 来保护 Ingress. Ingress 只支持单个 TLS 端口 443,并假定 TLS 连接终止于 Ingress 节点(与 Service 及其 Pod 之间的流量都以明文传输). 如果 Ingress 中的 TLS 配置部分指定了不同的主机,那么它们将根据通过 SNI TLS 扩展指定的主机名(如果 Ingress 控制器支持 SNI)在同一端口上进行复用. TLS Secret 的数据中必须包含用于 TLS 的以键名 tls.crt 保存的证书和以键名 tls.key 保存的私钥. 例如:

apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 编码的证书
  tls.key: base64 编码的私钥
type: kubernetes.io/tls

在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道. 你需要确保创建的 TLS Secret 创建自包含 https-example.foo.com 的公用名称(CN)的证书. 这里的公共名称也被称为全限定域名(FQDN).

说明:
注意,默认规则上无法使用 TLS,因为需要为所有可能的子域名发放证书. 因此,tls 字段中的 hosts 的取值需要与 rules 字段中的 host 完全匹配.

service/networking/tls-example-ingress.yaml 

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tls-example-ingress
spec:
  tls:
  - hosts:
      - https-example.foo.com
    secretName: testsecret-tls
  rules:
  - host: https-example.foo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: service1
            port:
              number: 80

说明:各种 Ingress 控制器所支持的 TLS 功能之间存在差异.请参阅有关 nginx、 GCE 或者任何其他平台特定的 Ingress 控制器的文档,以了解 TLS 如何在你的环境中工作.

* 负载均衡 
Ingress 控制器启动引导时使用一些适用于所有 Ingress 的负载均衡策略设置,例如负载均衡算法、后端权重方案等. 更高级的负载均衡概念(例如持久会话、动态权重)尚未通过 Ingress 公开. 你可以通过用于服务的负载均衡器来获取这些功能.

值得注意的是,尽管健康检查不是通过 Ingress 直接暴露的,在 Kubernetes 中存在并行的概念,比如 就绪检查, 允许你实现相同的目的. 请检查特定控制器的说明文档(nginx、 GCE)以了解它们是怎样处理健康检查的.


** 更新 Ingress 
要更新现有的 Ingress 以添加新的 Host,可以通过编辑资源来对其进行更新:

kubectl describe ingress test

Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test

kubectl edit ingress test

这一命令将打开编辑器,允许你以 YAML 格式编辑现有配置. 修改它来增加新的主机:

spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
        path: /foo
        pathType: Prefix
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
        path: /foo
        pathType: Prefix
..

保存更改后,kubectl 将更新 API 服务器中的资源,该资源将告诉 Ingress 控制器重新配置负载均衡器.

验证:

kubectl describe ingress test

Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test

你也可以通过 kubectl replace -f 命令调用修改后的 Ingress yaml 文件来获得同样的结果.



## Ingress 控制器
为了让 Ingress 资源工作,集群必须有一个正在运行的 Ingress 控制器.

与作为 kube-controller-manager 可执行文件的一部分运行的其他类型的控制器不同, Ingress 控制器不是随集群自动启动的. 基于此页面,你可选择最适合你的集群的 ingress 控制器实现.

Kubernetes 作为一个项目,目前支持和维护 AWS、 GCE 和 Nginx Ingress 控制器.

Nginx Ingress 控制器
https://kubernetes.github.io/ingress-nginx/deploy/

** 其他控制器 
详细描述:
https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/


** 使用多个 Ingress 控制器
你可以使用 Ingress 类在集群中部署任意数量的 Ingress 控制器. 请注意你的 Ingress 类资源的 .metadata.name 字段. 当你创建 Ingress 时,你需要用此字段的值来设置 Ingress 对象的 ingressClassName 字段(请参考 IngressSpec v1 reference). ingressClassName 是之前的注解做法的替代.

如果你不为 Ingress 指定一个 IngressClass,并且你的集群中只有一个 IngressClass 被标记为了集群默认,那么 Kubernetes 会应用此默认 IngressClass. 你可以通过将 ingressclass.kubernetes.io/is-default-class 注解 的值设置为 "true" 来将一个 IngressClass 标记为集群默认.

理想情况下,所有 Ingress 控制器都应满足此规范,但各种 Ingress 控制器的操作略有不同.

说明: 确保你查看了 ingress 控制器的文档,以了解选择它的注意事项.



## 拓扑感知提示
特性状态: Kubernetes v1.23 [beta]

拓扑感知提示 包含客户怎么使用服务端点的建议,从而实现了拓扑感知的路由功能. 这种方法添加了元数据,以启用 EndpointSlice 和/或 Endpoints 对象的调用者, 这样,访问这些网络端点的请求流量就可以在它的发起点附近就近路由.

例如,你可以在一个地域内路由流量,以降低通信成本,或提高网络性能.


** 动机
Kubernetes 集群越来越多的部署到多区域环境中. 拓扑感知提示 提供了一种把流量限制在它的发起区域之内的机制. 这个概念一般被称之为 "拓扑感知路由". 在计算 服务(Service) 的端点时, EndpointSlice 控制器会评估每一个端点的拓扑(地域和区域),填充提示字段,并将其分配到某个区域. 集群组件,例如kube-proxy 就可以使用这些提示信息,并用他们来影响流量的路由(倾向于拓扑上相邻的端点).


** 使用拓扑感知提示
你可以通过把注解 service.kubernetes.io/topology-aware-hints 的值设置为 auto, 来激活服务的拓扑感知提示功能. 这告诉 EndpointSlice 控制器在它认为安全的时候来设置拓扑提示. 重要的是,这并不能保证总会设置提示(hints).


** 工作原理
此特性启用的功能分为两个组件:EndpointSlice 控制器和 kube-proxy. 本节概述每个组件如何实现此特性.

* EndpointSlice 控制器
此特性开启后,EndpointSlice 控制器负责在 EndpointSlice 上设置提示信息. 控制器按比例给每个区域分配一定比例数量的端点. 这个比例来源于此区域中运行节点的 可分配 CPU 核心数. 例如,如果一个区域拥有 2 CPU 核心,而另一个区域只有 1 CPU 核心, 那控制器将给那个有 2 CPU 的区域分配两倍数量的端点.

以下示例展示了提供提示信息后 EndpointSlice 的样子:

apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: example-hints
  labels:
    kubernetes.io/service-name: example-svc
addressType: IPv4
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
      - "10.1.2.3"
    conditions:
      ready: true
    hostname: pod-1
    zone: zone-a
    hints:
      forZones:
        - name: "zone-a"

* kube-proxy
kube-proxy 组件依据 EndpointSlice 控制器设置的提示,过滤由它负责路由的端点. 在大多数场合,这意味着 kube-proxy 可以把流量路由到同一个区域的端点. 有时,控制器从某个不同的区域分配端点,以确保在多个区域之间更平均的分配端点. 这会导致部分流量被路由到其他区域.


** 保护措施
Kubernetes 控制平面和每个节点上的 kube-proxy,在使用拓扑感知提示功能前,会应用一些保护措施规则. 如果没有检出,kube-proxy 将无视区域限制,从集群中的任意节点上选择端点.

  1. 端点数量不足: 如果一个集群中,端点数量少于区域数量,控制器不创建任何提示.
  2. 不可能实现均衡分配: 在一些场合中,不可能实现端点在区域中的平衡分配. 例如,假设 zone-a 比 zone-b 大两倍,但只有 2 个端点, 那分配到 zone-a 的端点可能收到比 zone-b多两倍的流量. 如果控制器不能确定此"期望的过载"值低于每一个区域可接受的阈值,控制器将不指派提示信息. 重要的是,这不是基于实时反馈.所以对于单独的端点仍有可能超载.
  3. 一个或多个节点信息不足: 如果任一节点没有设置标签 topology.kubernetes.io/zone, 或没有上报可分配的 CPU 数据,控制平面将不会设置任何拓扑感知提示, 继而 kube-proxy 也就不能通过区域过滤端点.
  4. 一个或多个端点没有设置区域提示: 当这类事情发生时, kube-proxy 会假设这是正在执行一个从/到拓扑感知提示的转移. 在这种场合下过滤Service 的端点是有风险的,所以 kube-proxy 回撤为使用所有的端点.
  5. 不在提示中的区域: 如果 kube-proxy 不能根据一个指示在它所在的区域中发现一个端点, 它回撤为使用所有节点的端点.当你的集群新增一个新的区域时,这种情况发生概率很高.


** 限制
  * 当 Service 的 externalTrafficPolicy 或 internalTrafficPolicy 设置值为 Local 时, 拓扑感知提示功能不可用. 你可以在一个集群的不同服务中使用这两个特性,但不能在同一个服务中这么做.
  * 这种方法不适用于大部分流量来自于一部分区域的服务. 相反的,这里假设入站流量将根据每个区域中节点的服务能力按比例的分配.
  * EndpointSlice 控制器在计算每一个区域的容量比例时,会忽略未就绪的节点. 在大量节点未就绪的场景下,这样做会带来非预期的结果.
  * EndpointSlice 控制器在计算每一个区域的部署比例时,并不会考虑 容忍度. 如果服务后台的 Pod 被限制只能运行在集群节点的一个子集上,这些信息并不会被使用.
  * 这种方法和自动扩展机制之间不能很好的协同工作.例如,如果大量流量来源于一个区域, 那只有分配到该区域的端点才可用来处理流量.这会导致 Pod 自动水平扩展 要么不能拾取此事件,要么新增 Pod 被启动到其他区域.



## 服务内部流量策略
特性状态: Kubernetes v1.23 [beta]

服务内部流量策略 开启了内部流量限制,只路由内部流量到和发起方处于相同节点的服务端点.这里的"内部"流量指当前集群中的 Pod 所发起的流量.这种机制有助于节省开销,提升效率.


** 使用服务内部流量策略
ServiceInternalTrafficPolicy 特性门控 是 Beta 功能,默认启用.启用该功能后,你就可以通过将 Services 的 .spec.internalTrafficPolicy 项设置为 Local, 来为它指定一个内部专用的流量策略.此设置就相当于告诉 kube-proxy 对于集群内部流量只能使用本地的服务端口.

说明: 如果某节点上的 Pod 均不提供指定 Service 的服务端点, 即使该 Service 在其他节点上有可用的服务端点, Service 的行为看起来也像是它只有 0 个服务端点(只针对此节点上的 Pod).

以下示例展示了把 Service 的 .spec.internalTrafficPolicy 项设为 Local 时, Service 的样子:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  internalTrafficPolicy: Local


**  工作原理
kube-proxy 基于 spec.internalTrafficPolicy 的设置来过滤路由的目标服务端点.当它的值设为 Local 时,只选择节点本地的服务端点.当它的值设为 Cluster 或缺省时,则选择所有的服务端点.启用特性门控 ServiceInternalTrafficPolicy 后, spec.internalTrafficPolicy 的值默认设为 Cluster.


** 限制
  * 在一个Service上,当 externalTrafficPolicy 已设置为 Local时,服务内部流量策略无法使用.换句话说,在一个集群的不同 Service 上可以同时使用这两个特性,但在一个 Service 上不行.



## 端点切片(Endpoint Slices) --- 未知工作场景,及原理
特性状态: Kubernetes v1.21 [stable]
端点切片(EndpointSlices) 提供了一种简单的方法来跟踪 Kubernetes 集群中的网络端点 (network endpoints).它们为 Endpoints 提供了一种可伸缩和可拓展的替代方案.


** 动机 
Endpoints API 提供了在 Kubernetes 跟踪网络端点的一种简单而直接的方法.不幸的是,随着 Kubernetes 集群和 服务 逐渐开始为更多的后端 Pods 处理和发送请求,原来的 API 的局限性变得越来越明显.最重要的是那些因为要处理大量网络端点而带来的挑战.

由于任一服务的所有网络端点都保存在同一个 Endpoints 资源中,这类资源可能变得 非常巨大,而这一变化会影响到 Kubernetes 组件(比如主控组件)的性能,并 在 Endpoints 变化时产生大量的网络流量和额外的处理.EndpointSlice 能够帮助你缓解这一问题,还能为一些诸如拓扑路由这类的额外 功能提供一个可扩展的平台.


** Endpoint Slice 资源
在 Kubernetes 中,EndpointSlice 包含对一组网络端点的引用.指定选择器后控制面会自动为设置了 选择算符 的 Kubernetes 服务创建 EndpointSlice.这些 EndpointSlice 将包含对与服务选择算符匹配的所有 Pod 的引用.EndpointSlice 通过唯一的协议、端口号和服务名称将网络端点组织在一起.EndpointSlice 的名称必须是合法的 DNS 子域名.

例如,下面是 Kubernetes 服务 example 的 EndpointSlice 资源示例.

apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: example-abc
  labels:
    kubernetes.io/service-name: example
addressType: IPv4
ports:
  - name: http
    protocol: TCP
    port: 80
endpoints:
  - addresses:
    - "10.1.2.3"
    conditions:
      ready: true
    hostname: pod-1
    nodeName: node-1
    zone: us-west2-a

默认情况下,控制面创建和管理的 EndpointSlice 将包含不超过 100 个端点.你可以使用 kube-controller-manager 的 --max-endpoints-per-slice 标志设置此值,最大值为 1000.

当涉及如何路由内部流量时,EndpointSlice 可以充当 kube-proxy 的决策依据.启用该功能后,在服务的端点数量庞大时会有可观的性能提升.


** 地址类型
EndpointSlice 支持三种地址类型:

  * IPv4
  * IPv6
  * FQDN (完全合格的域名)

* 状况
EndpointSlice API 存储了可能对使用者有用的、有关端点的状况.这三个状况分别是 ready、serving 和 terminating.

Ready(就绪)
ready 状况是映射 Pod 的 Ready 状况的.处于运行中的 Pod,它的 Ready 状况被设置为 True,应该将此 EndpointSlice 状况也设置为 true.出于兼容性原因,当 Pod 处于终止过程中,ready 永远不会为 true.消费者应参考 serving 状况来检查处于终止中的 Pod 的就绪情况.该规则的唯一例外是将 spec.publishNotReadyAddresses 设置为 true 的服务.这些服务(Service)的端点将始终将 ready 状况设置为 true.

Serving(服务中)
特性状态: Kubernetes v1.20 [alpha]
serving 状况与 ready 状况相同,不同之处在于它不考虑终止状态.如果 EndpointSlice API 的使用者关心 Pod 终止时的就绪情况,就应检查此状况.

说明:
尽管 serving 与 ready 几乎相同,但是它是为防止破坏 ready 的现有含义而增加的.如果对于处于终止中的端点,ready 可能是 true,那么对于现有的客户端来说可能是有些意外的, 因为从始至终,Endpoints 或 EndpointSlice API 从未包含处于终止中的端点.出于这个原因,ready 对于处于终止中的端点 总是 false, 并且在 v1.20 中添加了新的状况 serving,以便客户端可以独立于 ready 的现有语义来跟踪处于终止中的 Pod 的就绪情况.

Terminating(终止中)
特性状态: Kubernetes v1.20 [alpha]
Terminating 是表示端点是否处于终止中的状况.对于 Pod 来说,这是设置了删除时间戳的 Pod.

* 拓扑信息 
EndpointSlice 中的每个端点都可以包含一定的拓扑信息.拓扑信息包括端点的位置,对应节点、可用区的信息.这些信息体现为 EndpointSlices 的如下端点字段:

  * nodeName - 端点所在的 Node 名称;
  * zone - 端点所处的可用区.

说明:
在 v1 API 中,逐个端点设置的 topology 实际上被去除,以鼓励使用专用 的字段 nodeName 和 zone.

对 EndpointSlice 对象的 endpoint 字段设置任意的拓扑结构信息这一操作已被 废弃,不再被 v1 API 所支持.取而代之的是 v1 API 所支持的 nodeName 和 zone 这些独立的字段.这些字段可以在不同的 API 版本之间自动完成转译.例如,v1beta1 API 中 topology 字段的 topology.kubernetes.io/zone 取值可以 在 v1 API 中通过 zone 字段访问.

* 管理 
通常,控制面(尤其是端点切片的 控制器) 会创建和管理 EndpointSlice 对象.EndpointSlice 对象还有一些其他使用场景, 例如作为服务网格(Service Mesh)的实现.这些场景都会导致有其他实体 或者控制器负责管理额外的 EndpointSlice 集合.

为了确保多个实体可以管理 EndpointSlice 而且不会相互产生干扰,Kubernetes 定义了 标签 endpointslice.kubernetes.io/managed-by,用来标明哪个实体在管理某个 EndpointSlice.端点切片控制器会在自己所管理的所有 EndpointSlice 上将该标签值设置 为 endpointslice-controller.k8s.io.管理 EndpointSlice 的其他实体也应该为此标签设置一个唯一值.

* 属主关系 
在大多数场合下,EndpointSlice 都由某个 Service 所有,(因为)该端点切片正是 为该服务跟踪记录其端点.这一属主关系是通过为每个 EndpointSlice 设置一个 属主(owner)引用,同时设置 kubernetes.io/service-name 标签来标明的, 目的是方便查找隶属于某服务的所有 EndpointSlice.

* EndpointSlice 镜像 
在某些场合,应用会创建定制的 Endpoints 资源.为了保证这些应用不需要并发 的更改 Endpoints 和 EndpointSlice 资源,集群的控制面将大多数 Endpoints 映射到对应的 EndpointSlice 之上.

控制面对 Endpoints 资源进行映射的例外情况有:

  * Endpoints 资源上标签 endpointslice.kubernetes.io/skip-mirror 值为 true.
  * Endpoints 资源包含标签 control-plane.alpha.kubernetes.io/leader.
  * 对应的 Service 资源不存在.
  * 对应的 Service 的选择算符不为空.

每个 Endpoints 资源可能会被翻译到多个 EndpointSlices 中去.当 Endpoints 资源中包含多个子网或者包含多个 IP 地址族(IPv4 和 IPv6)的端点时, 就有可能发生这种状况.每个子网最多有 1000 个地址会被镜像到 EndpointSlice 中.

* EndpointSlices 的分布问题 
每个 EndpointSlice 都有一组端口值,适用于资源内的所有端点.当为服务使用命名端口时,Pod 可能会就同一命名端口获得不同的端口号,因而需要 不同的 EndpointSlice.这有点像 Endpoints 用来对子网进行分组的逻辑.

控制面尝试尽量将 EndpointSlice 填满,不过不会主动地在若干 EndpointSlice 之间 执行再平衡操作.这里的逻辑也是相对直接的:

  1. 列举所有现有的 EndpointSlices,移除那些不再需要的端点并更新那些已经 变化的端点.
  2. 列举所有在第一步中被更改过的 EndpointSlices,用新增加的端点将其填满.
  3. 如果还有新的端点未被添加进去,尝试将这些端点添加到之前未更改的切片中, 或者创建新切片.

这里比较重要的是,与在 EndpointSlice 之间完成最佳的分布相比,第三步中更看重 限制 EndpointSlice 更新的操作次数.例如,如果有 10 个端点待添加,有两个 EndpointSlice 中各有 5 个空位,上述方法会创建一个新的 EndpointSlice 而不是 将现有的两个 EndpointSlice 都填满.换言之,与执行多个 EndpointSlice 更新操作 相比较,方法会优先考虑执行一个 EndpointSlice 创建操作.

由于 kube-proxy 在每个节点上运行并监视 EndpointSlice 状态,EndpointSlice 的 每次变更都变得相对代价较高,因为这些状态变化要传递到集群中每个节点上.这一方法尝试限制要发送到所有节点上的变更消息个数,即使这样做可能会导致有 多个 EndpointSlice 没有被填满.

在实践中,上面这种并非最理想的分布是很少出现的.大多数被 EndpointSlice 控制器 处理的变更都是足够小的,可以添加到某已有 EndpointSlice 中去的.并且,假使无法 添加到已有的切片中,不管怎样都会快就会需要一个新的 EndpointSlice 对象.Deployment 的滚动更新为重新为 EndpointSlice 打包提供了一个自然的机会,所有 Pod 及其对应的端点在这一期间都会被替换掉.

* 重复的端点 
由于 EndpointSlice 变化的自身特点,端点可能会同时出现在不止一个 EndpointSlice 中.鉴于不同的 EndpointSlice 对象在不同时刻到达 Kubernetes 的监视/缓存中, 这种情况的出现是很自然的.使用 EndpointSlice 的实现必须能够处理端点出现在多个切片中的状况.关于如何执行端点去重(deduplication)的参考实现,你可以在 kube-proxy 的 EndpointSlice 实现中找到.



## 网络策略
如果你希望在 IP 地址或端口层面(OSI 第 3 层或第 4 层)控制网络流量, 则你可以考虑为集群中特定应用使用 Kubernetes 网络策略(NetworkPolicy).NetworkPolicy 是一种以应用为中心的结构,允许你设置如何允许 Pod 与网络上的各类网络"实体" (我们这里使用实体以避免过度使用诸如"端点"和"服务"这类常用术语, 这些术语在 Kubernetes 中有特定含义)通信.

Pod 可以通信的 Pod 是通过如下三个标识符的组合来辩识的:

  1. 其他被允许的 Pods(例外:Pod 无法阻塞对自身的访问)
  2. 被允许的名字空间
  3. IP 组块(例外:与 Pod 运行所在的节点的通信总是被允许的, 无论 Pod 或节点的 IP 地址)

在定义基于 Pod 或名字空间的 NetworkPolicy 时,你会使用 选择算符 来设定哪些流量 可以进入或离开与该算符匹配的 Pod.

同时,当基于 IP 的 NetworkPolicy 被创建时,我们基于 IP 组块(CIDR 范围) 来定义策略.


** 前置条件 
网络策略通过网络插件(类似calico) 来实现.要使用网络策略,你必须使用支持 NetworkPolicy 的网络解决方案.创建一个 NetworkPolicy 资源对象而没有控制器来使它生效的话,是没有任何作用的.


** 隔离和非隔离的 Pod 
默认情况下,Pod 是非隔离的,它们接受任何来源的流量.

Pod 在被某 NetworkPolicy 选中时进入被隔离状态.一旦名字空间中有 NetworkPolicy 选择了特定的 Pod,该 Pod 会拒绝该 NetworkPolicy 所不允许的连接.(名字空间下其他未被 NetworkPolicy 所选择的 Pod 会继续接受所有的流量)

网络策略不会冲突,它们是累积的.如果任何一个或多个策略选择了一个 Pod, 则该 Pod 受限于这些策略的 入站(Ingress)/出站(Egress)规则的并集.因此评估的顺序并不会影响策略的结果.

为了允许两个 Pods 之间的网络数据流,源端 Pod 上的出站(Egress)规则和 目标端 Pod 上的入站(Ingress)规则都需要允许该流量.如果源端的出站(Egress)规则或目标端的入站(Ingress)规则拒绝该流量, 则流量将被拒绝.


** NetworkPolicy 资源 

NetworkPolicy 资源的完整定义
https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#networkpolicy-v1-networking-k8s-io

下面是一个 NetworkPolicy 的示例:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978

说明: 除非选择支持网络策略的网络解决方案,否则将上述示例发送到API服务器没有任何效果.

必需字段:与所有其他的 Kubernetes 配置一样,NetworkPolicy 需要 apiVersion、 kind 和 metadata 字段.关于配置文件操作的一般信息,请参考 使用 ConfigMap 配置容器, 和对象管理.

spec:NetworkPolicy 规约 中包含了在一个名字空间中定义特定网络策略所需的所有信息.

podSelector:每个 NetworkPolicy 都包括一个 podSelector,它对该策略所 适用的一组 Pod 进行选择.示例中的策略选择带有 "role=db" 标签的 Pod.空的 podSelector 选择名字空间下的所有 Pod.

policyTypes: 每个 NetworkPolicy 都包含一个 policyTypes 列表,其中包含 Ingress 或 Egress 或两者兼具.policyTypes 字段表示给定的策略是应用于 进入所选 Pod 的入站流量还是来自所选 Pod 的出站流量,或两者兼有.如果 NetworkPolicy 未指定 policyTypes 则默认情况下始终设置 Ingress; 如果 NetworkPolicy 有任何出口规则的话则设置 Egress.

ingress: 每个 NetworkPolicy 可包含一个 ingress 规则的白名单列表.每个规则都允许同时匹配 from 和 ports 部分的流量.示例策略中包含一条 简单的规则: 它匹配某个特定端口,来自三个来源中的一个,第一个通过 ipBlock 指定,第二个通过 namespaceSelector 指定,第三个通过 podSelector 指定.

egress: 每个 NetworkPolicy 可包含一个 egress 规则的白名单列表.每个规则都允许匹配 to 和 port 部分的流量.该示例策略包含一条规则, 该规则将指定端口上的流量匹配到 10.0.0.0/24 中的任何目的地.

所以,该网络策略示例:

  1. 隔离 "default" 名字空间下 "role=db" 的 Pod (如果它们不是已经被隔离的话).

  2.(Ingress 规则)允许以下 Pod 连接到 "default" 名字空间下的带有 "role=db" 标签的所有 Pod 的 6379 TCP 端口:

    * "default" 名字空间下带有 "role=frontend" 标签的所有 Pod
    * 带有 "project=myproject" 标签的所有名字空间中的 Pod
    * IP 地址范围为 172.17.0.0–172.17.0.255 和 172.17.2.0–172.17.255.255 (即,除了 172.17.1.0/24 之外的所有 172.17.0.0/16) --- ip 针对 pod 层的 ip 地址

  3.(Egress 规则)允许从带有 "role=db" 标签的名字空间下的任何 Pod 到 CIDR 10.0.0.0/24 下 5978 TCP 端口的连接.
    

** 选择器 to 和 from 的行为 
可以在 ingress 的 from 部分或 egress 的 to 部分中指定四种选择器:

podSelector: 此选择器将在与 NetworkPolicy 相同的名字空间中选择特定的 Pod,应将其允许作为入站流量来源或出站流量目的地.

namespaceSelector:此选择器将选择特定的名字空间,应将所有 Pod 用作其 入站流量来源或出站流量目的地.

namespaceSelector 和 podSelector: 一个指定 namespaceSelector 和 podSelector 的 to/from 条目选择特定名字空间中的特定 Pod.注意使用正确的 YAML 语法;下面的策略:

  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...

在 from 数组中仅包含一个元素,只允许来自标有 role=client 的 Pod 且 该 Pod 所在的名字空间中标有 user=alice 的连接.但是 这项 策略:

  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...

在 from 数组中包含两个元素,允许来自本地名字空间中标有 role=client 的 Pod 的连接,或 来自任何名字空间中标有 user=alice 的任何 Pod 的连接.

如有疑问,请使用 kubectl describe 查看 Kubernetes 如何解释该策略.

ipBlock: 此选择器将选择特定的 IP CIDR 范围以用作入站流量来源或出站流量目的地.这些应该是集群外部 IP,因为 Pod IP 存在时间短暂的且随机产生.

集群的入站和出站机制通常需要重写数据包的源 IP 或目标 IP.在发生这种情况时,不确定在 NetworkPolicy 处理之前还是之后发生, 并且对于网络插件、云提供商、Service 实现等的不同组合,其行为可能会有所不同.

对入站流量而言,这意味着在某些情况下,你可以根据实际的原始源 IP 过滤传入的数据包, 而在其他情况下,NetworkPolicy 所作用的 源IP 则可能是 LoadBalancer 或 Pod 的节点等.

对于出站流量而言,这意味着从 Pod 到被重写为集群外部 IP 的 Service IP 的连接可能会或可能不会受到基于 ipBlock 的策略的约束.


** 默认策略 
默认情况下,如果名字空间中不存在任何策略,则所有进出该名字空间中 Pod 的流量都被允许.以下示例使你可以更改该名字空间中的默认行为.

默认拒绝所有入站流量 
你可以通过创建选择所有容器但不允许任何进入这些容器的入站流量的 NetworkPolicy 来为名字空间创建 "default" 隔离策略.

* 默认拒绝所有入站流量
你可以通过创建选择所有容器但不允许任何进入这些容器的入站流量的 NetworkPolicy 来为名字空间创建 "default" 隔离策略.

service/networking/network-policy-default-deny-ingress.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: {}
  policyTypes:
  - Ingress

这样可以确保即使容器没有选择其他任何 NetworkPolicy,也仍然可以被隔离.此策略不会更改默认的出口隔离行为.

* 默认允许所有入站流量
如果要允许所有流量进入某个名字空间中的所有 Pod(即使添加了导致某些 Pod 被视为 "隔离"的策略),则可以创建一个策略来明确允许该名字空间中的所有流量.

service/networking/network-policy-allow-all-ingress.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-ingress
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress

* 默认拒绝所有出站流量 
你可以通过创建选择所有容器但不允许来自这些容器的任何出站流量的 NetworkPolicy 来为名字空间创建 "default" egress 隔离策略.

service/networking/network-policy-default-deny-egress.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
spec:
  podSelector: {}
  policyTypes:
  - Egress

此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被允许流出流量.此策略不会更改默认的入站流量隔离行为.

* 默认允许所有出站流量
如果要允许来自名字空间中所有 Pod 的所有流量(即使添加了导致某些 Pod 被视为"隔离"的策略), 则可以创建一个策略,该策略明确允许该名字空间中的所有出站流量.

service/networking/network-policy-allow-all-egress.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress

* 默认拒绝所有入口和所有出站流量
你可以为名字空间创建"默认"策略,以通过在该名字空间中创建以下 NetworkPolicy 来阻止所有入站和出站流量.

service/networking/network-policy-default-deny-all.yaml 

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

此策略可以确保即使没有被其他任何 NetworkPolicy 选择的 Pod 也不会被 允许入站或出站流量.


**SCTP 支持 
特性状态: Kubernetes v1.20 [stable]

作为一个稳定特性,SCTP 支持默认是被启用的.要在集群层面禁用 SCTP,你(或你的集群管理员)需要为 API 服务器指定 --feature-gates=SCTPSupport=false,... 来禁用 SCTPSupport 特性门控.启用该特性门控后,用户可以将 NetworkPolicy 的 protocol 字段设置为 SCTP.

说明:
你必须使用支持 SCTP 协议网络策略的 CNI 插件.


** 针对某个端口范围 
特性状态: Kubernetes v1.22 [beta]

在编写 NetworkPolicy 时,你可以针对一个端口范围而不是某个固定端口.

这一目的可以通过使用 endPort 字段来实现,如下例所示:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: multi-port-egress
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 32000
      endPort: 32768

上面的规则允许名字空间 default 中所有带有标签 db 的 Pod 使用 TCP 协议 与 10.0.0.0/24 范围内的 IP 通信,只要目标端口介于 32000 和 32768 之间就可以.

使用此字段时存在以下限制:

  * 作为一种 Beta 阶段的特性,端口范围设定默认是被启用的.要在整个集群 范围内禁止使用 endPort 字段,你(或者你的集群管理员)需要为 API 服务器设置 -feature-gates=NetworkPolicyEndPort=false,... 以禁用 NetworkPolicyEndPort 特性门控.
  * endPort 字段必须等于或者大于 port 字段的值.
  * 两个字段的设置值都只能是数字.

说明:
你的集群所使用的 CNI 插件 必须支持在 NetworkPolicy 规约中使用 endPort 字段.如果你的网络插件 不支持 endPort 字段,而你指定了一个包含 endPort 字段的 NetworkPolicy, 策略只对单个 port 字段生效.


** 基于名字指向某名字空间 
特性状态: Kubernetes 1.21 [beta]
只要 NamespaceDefaultLabelName 特性门控 被启用,Kubernetes 控制面会在所有名字空间上设置一个不可变更的标签 kubernetes.io/metadata.name.该标签的值是名字空间的名称.

如果 NetworkPolicy 无法在某些对象字段中指向某名字空间,你可以使用标准的 标签方式来指向特定名字空间.


** 通过网络策略(至少目前还)无法完成的工作
到 Kubernetes 1.23 为止,NetworkPolicy API 还不支持以下功能,不过 你可能可以使用操作系统组件(如 SELinux、OpenVSwitch、IPTables 等等) 或者第七层技术(Ingress 控制器、服务网格实现)或准入控制器来实现一些 替代方案.如果你对 Kubernetes 中的网络安全性还不太了解,了解使用 NetworkPolicy API 还无法实现下面的用户场景是很值得的.

  * 强制集群内部流量经过某公用网关(这种场景最好通过服务网格或其他代理来实现);
  * 与 TLS 相关的场景(考虑使用服务网格或者 Ingress 控制器);
  * 特定于节点的策略(你可以使用 CIDR 来表达这一需求不过你无法使用节点在 Kubernetes 中的其他标识信息来辩识目标节点);
  * 基于名字来选择服务(不过,你可以使用 标签 来选择目标 Pod 或名字空间,这也通常是一种可靠的替代方案);
  * 创建或管理由第三方来实际完成的"策略请求";
  * 实现适用于所有名字空间或 Pods 的默认策略(某些第三方 Kubernetes 发行版本 或项目可以做到这点);
  * 高级的策略查询或者可达性相关工具;
  * 生成网络安全事件日志的能力(例如,被阻塞或接收的连接请求);
  * 显式地拒绝策略的能力(目前,NetworkPolicy 的模型默认采用拒绝操作, 其唯一的能力是添加允许策略);
  * 禁止本地回路或指向宿主的网络流量(Pod 目前无法阻塞 localhost 访问, 它们也无法禁止来自所在节点的访问请求).

## IPv4/IPv6 双协议栈

-------------------
# 存储
## 卷
Container 中的文件在磁盘上是临时存放的,这给 Container 中运行的较重要的应用程序带来一些问题. 问题之一是当容器崩溃时文件丢失. kubelet 会重新启动容器,但容器会以干净的状态重启. 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现. Kubernetes 卷(Volume) 这一抽象概念能够解决这两个问题.


** 背景 
Docker 也有 卷(Volume) 的概念,但对它只有少量且松散的管理. Docker 卷是磁盘上或者另外一个容器内的一个目录. Docker 提供卷驱动程序,但是其功能非常有限.

Kubernetes 支持很多类型的卷. Pod 可以同时使用任意数目的卷类型. 临时卷类型的生命周期与 Pod 相同,但持久卷可以比 Pod 的存活期长. 当 Pod 不再存在时,Kubernetes 也会销毁临时卷;不过 Kubernetes 不会销毁持久卷. 对于给定 Pod 中任何类型的卷,在容器重启期间数据都不会丢失.

卷的核心是一个目录,其中可能存有数据,Pod 中的容器可以访问该目录中的数据. 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容.

使用卷时, 在 .spec.volumes 字段中设置为 Pod 提供的卷,并在 .spec.containers[*].volumeMounts 字段中声明卷在容器中的挂载位置. 容器中的进程看到的文件系统视图是由它们的 容器镜像 的初始内容以及挂载在容器中的卷(如果定义了的话)所组成的. 其中根文件系统同容器镜像的内容相吻合. 任何在该文件系统下的写入操作,如果被允许的话,都会影响接下来容器中进程访问文件系统时所看到的内容.

卷挂载在镜像中的指定路径下. Pod 配置中的每个容器必须独立指定各个卷的挂载位置.

卷不能挂载到其他卷之上(不过存在一种使用 subPath 的相关机制),也不能与其他卷有硬链接.


** 卷类型 
Kubernetes 支持下列类型的卷:

* awsElasticBlockStore
awsElasticBlockStore 卷将 Amazon Web服务(AWS)EBS 卷 挂载到你的 Pod 中.与 emptyDir 在 Pod 被删除时也被删除不同,EBS 卷的内容在删除 Pod 时会被保留,卷只是被卸载掉了. 这意味着 EBS 卷可以预先填充数据,并且该数据可以在 Pod 之间共享.

说明: 你在使用 EBS 卷之前必须使用 aws ec2 create-volume 命令或者 AWS API 创建该卷.

* azureDisk
azureDisk 卷类型用来在 Pod 上挂载 Microsoft Azure 数据盘(Data Disk) . 

* azureFile
azureFile 卷类型用来在 Pod 上挂载 Microsoft Azure 文件卷(File Volume)(SMB 2.1 和 3.0).

* cephfs
cephfs 卷允许你将现存的 CephFS 卷挂载到 Pod 中. 不像 emptyDir 那样会在 Pod 被删除的同时也会被删除,cephfs 卷的内容在 Pod 被删除时会被保留,只是卷被卸载了. 这意味着 cephfs 卷可以被预先填充数据,且这些数据可以在 Pod 之间共享.同一 cephfs 卷可同时被多个写者挂载.

说明: 在使用 Ceph 卷之前,你的 Ceph 服务器必须已经运行并将要使用的 share 导出(exported).

CephFS 示例
https://github.com/kubernetes/examples/tree/master/volumes/cephfs/

* cinder
说明: Kubernetes 必须配置了 OpenStack Cloud Provider.
cinder 卷类型用于将 OpenStack Cinder 卷挂载到 Pod 中.

* configMap
configMap 卷提供了向 Pod 注入配置数据的方法. ConfigMap 对象中存储的数据可以被 configMap 类型的卷引用,然后被 Pod 中运行的容器化应用使用.

引用 configMap 对象时,你可以在 volume 中通过它的名称来引用. 你可以自定义 ConfigMap 中特定条目所要使用的路径. 下面的配置显示了如何将名为 log-config 的 ConfigMap 挂载到名为 configmap-pod 的 Pod 中:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: test
      image: busybox:1.28
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: log-config
        items:
          - key: log_level
            path: log_level

log-config ConfigMap 以卷的形式挂载,并且存储在 log_level 条目中的所有内容都被挂载到 Pod 的 /etc/config/log_level 路径下. 请注意,这个路径来源于卷的 mountPath 和 log_level 键对应的 path.

说明:
  * 在使用 ConfigMap 之前你首先要创建它.
  * 容器以 subPath 卷挂载方式使用 ConfigMap 时,将无法接收 ConfigMap 的更新.
  * 文本数据挂载成文件时采用 UTF-8 字符编码.如果使用其他字符编码形式,可使用 binaryData 字段.

* downwardAPI
downwardAPI 卷用于使 downward API 数据对应用程序可用. 这种卷类型挂载一个目录并在纯文本文件中写入所请求的数据.

说明: 容器以 subPath 卷挂载方式使用 downwardAPI 时,将不能接收到它的更新.

* emptyDir
当 Pod 分派到某个 Node 上时,emptyDir 卷会被创建,并且在 Pod 在该节点上运行期间,卷一直存在. 就像其名称表示的那样,卷最初是空的. 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同,这些容器都可以读写 emptyDir 卷中相同的文件. 当 Pod 因为某些原因被从节点上删除时,emptyDir 卷中的数据也会被永久删除.

说明: 容器崩溃并不会导致 Pod 被从节点上移除,因此容器崩溃期间 emptyDir 卷中的数据是安全的.

emptyDir 的一些用途:

  * 缓存空间,例如基于磁盘的归并排序.
  * 为耗时较长的计算任务提供检查点,以便任务能方便地从崩溃前状态恢复执行.
  * 在 Web 服务器容器服务数据时,保存内容管理器容器获取的文件.

取决于你的环境,emptyDir 卷存储在该节点所使用的介质上;这里的介质可以是磁盘或 SSD 或网络存储.但是,你可以将 emptyDir.medium 字段设置为 "Memory",以告诉 Kubernetes 为你挂载 tmpfs(基于 RAM 的文件系统). 虽然 tmpfs 速度非常快,但是要注意它与磁盘不同. tmpfs 在节点重启时会被清除,并且你所写入的所有文件都会计入容器的内存消耗,受容器内存限制约束.

说明: 当启用 SizeMemoryBackedVolumes 特性门控 时,你可以为基于内存提供的卷指定大小. 如果未指定大小,则基于内存的卷的大小为 Linux 主机上内存的 50％.

emptyDir 配置示例

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}

* fc (光纤通道)
fc 卷类型允许将现有的光纤通道块存储卷挂载到 Pod 中. 可以使用卷配置中的参数 targetWWNs 来指定单个或多个目标 WWN(World Wide Names). 如果指定了多个 WWN,targetWWNs 期望这些 WWN 来自多路径连接.

说明: 你必须配置 FC SAN Zoning,以便预先向目标 WWN 分配和屏蔽这些 LUN(卷),这样 Kubernetes 主机才可以访问它们.

* flocker (已弃用)
Flocker 是一个开源的、集群化的容器数据卷管理器. Flocker 提供了由各种存储后端所支持的数据卷的管理和编排.

* gcePersistentDisk
gcePersistentDisk 卷能将谷歌计算引擎 (GCE) 持久盘(PD) 挂载到你的 Pod 中. 不像 emptyDir 那样会在 Pod 被删除的同时也会被删除,持久盘卷的内容在删除 Pod 时会被保留,卷只是被卸载了. 这意味着持久盘卷可以被预先填充数据,并且这些数据可以在 Pod 之间共享.

注意: 在使用 PD 前,你必须使用 gcloud 或者 GCE API 或 UI 创建它.

* gitRepo (已弃用) 
警告: gitRepo 卷类型已经被废弃.如果需要在容器中提供 git 仓库,请将一个 EmptyDir 卷挂载到 InitContainer 中,使用 git 命令完成仓库的克隆操作,然后将 EmptyDir 卷挂载到 Pod 的容器中.

* glusterfs
glusterfs 卷能将 Glusterfs (一个开源的网络文件系统) 挂载到你的 Pod 中.不像 emptyDir 那样会在删除 Pod 的同时也会被删除,glusterfs 卷的内容在删除 Pod 时会被保存,卷只是被卸载. 这意味着 glusterfs 卷可以被预先填充数据,并且这些数据可以在 Pod 之间共享. GlusterFS 可以被多个写者同时挂载.

说明: 在使用前你必须先安装运行自己的 GlusterFS.

GlusterFS 示例
https://github.com/kubernetes/examples/tree/master/volumes/glusterfs

* hostPath
警告:
HostPath 卷存在许多安全风险,最佳做法是尽可能避免使用 HostPath. 当必须使用 HostPath 卷时,它的范围应仅限于所需的文件或目录,并以只读方式挂载.

如果通过 AdmissionPolicy 限制 HostPath 对特定目录的访问,则必须要求 volumeMounts 使用 readOnly 挂载以使策略生效.

hostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中. 虽然这不是大多数 Pod 需要的,但是它为一些应用程序提供了强大的逃生舱.

例如,hostPath 的一些用法有:

  * 运行一个需要访问 Docker 内部机制的容器;可使用 hostPath 挂载 /var/lib/docker 路径.
  * 在容器中运行 cAdvisor 时,以 hostPath 方式挂载 /sys.
  * 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在,是否应该创建以及应该以什么方式存在.

除了必需的 path 属性之外,用户可以选择性地为 hostPath 卷指定 type.

支持的 type 值如下:

取值	                      行为
                                空字符串(默认)用于向后兼容,这意味着在安装 hostPath 卷之前不会执行任何检查.
DirectoryOrCreate	  如果在给定路径上什么都不存在,那么将根据需要创建空目录,权限设置为 0755,具有与 kubelet 相同的组和属主信息.
Directory	                在给定路径上必须存在的目录.
FileOrCreate	          如果在给定路径上什么都不存在,那么将在那里根据需要创建空文件,权限设置为 0644,具有与 kubelet 相同的组和所有权.
File	                          在给定路径上必须存在的文件.
Socket	                    在给定路径上必须存在的 UNIX 套接字.
CharDevice	            在给定路径上必须存在的字符设备.
BlockDevice	          在给定路径上必须存在的块设备.

当使用这种类型的卷时要小心,因为:

  * HostPath 卷可能会暴露特权系统凭据(例如 Kubelet)或特权 API(例如容器运行时套接字),可用于容器逃逸或攻击集群的其他部分.
  * 具有相同配置(例如基于同一 PodTemplate 创建)的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为.
  * 下层主机上创建的文件或目录只能由 root 用户写入.你需要在 特权容器 中以 root 身份运行进程,或者修改主机上的文件权限以便容器能够写入 hostPath 卷.

hostPath 配置示例:

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # 宿主上目录位置
      path: /data
      # 此字段为可选
      type: Directory

注意: FileOrCreate 模式不会负责创建文件的父目录. 如果欲挂载的文件的父目录不存在,Pod 启动会失败. 为了确保这种模式能够工作,可以尝试把文件和它对应的目录分开挂载,如 FileOrCreate 配置 所示.

hostPath FileOrCreate 配置示例 

apiVersion: v1
kind: Pod
metadata:
  name: test-webserver
spec:
  containers:
  - name: test-webserver
    image: k8s.gcr.io/test-webserver:latest
    volumeMounts:
    - mountPath: /var/local/aaa
      name: mydir
    - mountPath: /var/local/aaa/1.txt
      name: myfile
  volumes:
  - name: mydir
    hostPath:
      # 确保文件所在目录成功创建.
      path: /var/local/aaa
      type: DirectoryOrCreate
  - name: myfile
    hostPath:
      path: /var/local/aaa/1.txt
      type: FileOrCreate

* iscsi
iscsi 卷能将 iSCSI (基于 IP 的 SCSI) 卷挂载到你的 Pod 中. 不像 emptyDir 那样会在删除 Pod 的同时也会被删除,iscsi 卷的内容在删除 Pod 时会被保留,卷只是被卸载. 这意味着 iscsi 卷可以被预先填充数据,并且这些数据可以在 Pod 之间共享.

注意: 在使用 iSCSI 卷之前,你必须拥有自己的 iSCSI 服务器,并在上面创建卷.

* local
local 卷所代表的是某个被挂载的本地存储设备,例如磁盘、分区或者目录.

local 卷只能用作静态创建的持久卷.尚不支持动态配置.

与 hostPath 卷相比,local 卷能够以持久和可移植的方式使用,而无需手动将 Pod 调度到节点.系统通过查看 PersistentVolume 的节点亲和性配置,就能了解卷的节点约束.

然而,local 卷仍然取决于底层节点的可用性,并不适合所有应用程序. 如果节点变得不健康,那么 local 卷也将变得不可被 Pod 访问.使用它的 Pod 将不能运行. 使用 local 卷的应用程序必须能够容忍这种可用性的降低,以及因底层磁盘的耐用性特征而带来的潜在的数据丢失风险.

下面是一个使用 local 卷和 nodeAffinity 的持久卷示例:

* apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/ssd1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node

使用 local 卷时,你需要设置 PersistentVolume 对象的 nodeAffinity 字段. Kubernetes 调度器使用 PersistentVolume 的 nodeAffinity 信息来将使用 local 卷的 Pod 调度到正确的节点.

PersistentVolume 对象的 volumeMode 字段可被设置为 "Block" (而不是默认值 "Filesystem"),以将 local 卷作为原始块设备暴露出来.

使用 local 卷时,建议创建一个 StorageClass 并将其 volumeBindingMode 设置为 WaitForFirstConsumer.要了解更多详细信息,请参考 local StorageClass 示例. 延迟卷绑定的操作可以确保 Kubernetes 在为 PersistentVolumeClaim 作出绑定决策时,会评估 Pod 可能具有的其他节点约束,例如:如节点资源需求、节点选择器、Pod亲和性和 Pod 反亲和性.

你可以在 Kubernetes 之外单独运行静态驱动以改进对 local 卷的生命周期管理. 请注意,此驱动尚不支持动态配置. 有关如何运行外部 local 卷驱动,请参考 local 卷驱动用户指南.

说明: 如果不使用外部静态驱动来管理卷的生命周期,用户需要手动清理和删除 local 类型的持久卷.

 local StorageClass 示例
https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#local

* nfs
nfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中. 不像 emptyDir 那样会在删除 Pod 的同时也会被删除,nfs 卷的内容在删除 Pod 时会被保存,卷只是被卸载. 这意味着 nfs 卷可以被预先填充数据,并且这些数据可以在 Pod 之间共享.

注意: 在使用 NFS 卷之前,你必须运行自己的 NFS 服务器并将目标 share 导出备用.

* persistentVolumeClaim
persistentVolumeClaim 卷用来将持久卷(PersistentVolume)挂载到 Pod 中. 持久卷申领(PersistentVolumeClaim)是用户在不知道特定云环境细节的情况下"申领"持久存储(例如 GCE PersistentDisk 或者 iSCSI 卷)的一种方法.

持久卷示例
https://kubernetes.io/zh/docs/concepts/storage/persistent-volumes/

* portworxVolume
portworxVolume 是一个可伸缩的块存储层,能够以超融合(hyperconverged)的方式与 Kubernetes 一起运行. Portworx 支持对服务器上存储的指纹处理、基于存储能力进行分层以及跨多个服务器整合存储容量. Portworx 可以以 in-guest 方式在虚拟机中运行,也可以在裸金属 Linux 节点上运行.

* projected (投射) 
投射卷能将若干现有的卷来源映射到同一目录上.更多详情请参考投射卷.

* quobyte (已弃用)
quobyte 卷允许将现有的 Quobyte 卷挂载到你的 Pod 中.

说明: 在使用 Quobyte 卷之前,你首先要进行安装 Quobyte 并创建好卷.

* rbd
rbd 卷允许将 Rados 块设备卷挂载到你的 Pod 中. 不像 emptyDir 那样会在删除 Pod 的同时也会被删除,rbd 卷的内容在删除 Pod 时会被保存,卷只是被卸载. 这意味着 rbd 卷可以被预先填充数据,并且这些数据可以在 Pod 之间共享.

说明: 在使用 RBD 之前,你必须安装运行 Ceph.

* secret
secret 卷用来给 Pod 传递敏感信息,例如密码.你可以将 Secret 存储在 Kubernetes API 服务器上,然后以文件的形式挂在到 Pod 中,无需直接与 Kubernetes 耦合. secret 卷由 tmpfs(基于 RAM 的文件系统)提供存储,因此它们永远不会被写入非易失性(持久化的)存储器.

说明: 使用前你必须在 Kubernetes API 中创建 secret.
说明: 容器以 subPath 卷挂载方式挂载 Secret 时,将感知不到 Secret 的更新.

配置 Secrets
https://kubernetes.io/zh/docs/concepts/configuration/secret/

* storageOS (已弃用)
storageos 卷允许将现有的 StorageOS 卷挂载到你的 Pod 中.

* vsphereVolume
说明: 你必须配置 Kubernetes 的 vSphere 云驱动.云驱动的配置方法请参考 vSphere 使用指南.
vsphereVolume 用来将 vSphere VMDK 卷挂载到你的 Pod 中. 在卸载卷时,卷的内容会被保留. vSphereVolume 卷类型支持 VMFS 和 VSAN 数据仓库.

注意: 在挂载到 Pod 之前,你必须用下列方式之一创建 VMDK.


** 使用 subPath 
有时,在单个 Pod 中共享卷以供多方使用是很有用的. volumeMounts.subPath 属性可用于指定所引用的卷内的子路径,而不是其根路径.

下面例子展示了如何配置某包含 LAMP 堆栈(Linux Apache MySQL PHP)的 Pod 使用同一共享卷. 此示例中的 subPath 配置不建议在生产环境中使用. PHP 应用的代码和相关数据映射到卷的 html 文件夹,MySQL 数据库存储在卷的 mysql 文件夹中:

apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "rootpasswd"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
 
* 使用带有扩展环境变量的 subPath 
特性状态: Kubernetes v1.17 [stable]
使用 subPathExpr 字段可以基于 Downward API 环境变量来构造 subPath 目录名. subPath 和 subPathExpr 属性是互斥的.

在这个示例中,Pod 使用 subPathExpr 来 hostPath 卷 /var/log/pods 中创建目录 pod1. hostPath 卷采用来自 downwardAPI 的 Pod 名称生成目录名. 宿主目录 /var/log/pods/pod1 被挂载到容器的 /logs 中.

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: container1
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: busybox:1.28
    command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ]
    volumeMounts:
    - name: workdir1
      mountPath: /logs
      # 包裹变量名的是小括号,而不是大括号
      subPathExpr: $(POD_NAME)
  restartPolicy: Never
  volumes:
  - name: workdir1
    hostPath:
      path: /var/log/pods


** 资源 
emptyDir 卷的存储介质(磁盘、SSD 等)是由保存 kubelet 数据的根目录(通常是 /var/lib/kubelet)的文件系统的介质确定. Kubernetes 对 emptyDir 卷或者 hostPath 卷可以消耗的空间没有限制,容器之间或 Pod 之间也没有隔离.


** 树外(Out-of-Tree)卷插件

* CSI

* flexVolume 
特性状态: Kubernetes v1.23 [deprecated]


** 挂载卷的传播 
挂载卷的传播能力允许将容器安装的卷共享到同一 Pod 中的其他容器,甚至共享到同一节点上的其他 Pod.

卷的挂载传播特性由 Container.volumeMounts 中的 mountPropagation 字段控制. 它的值包括:

  * None - 此卷挂载将不会感知到主机后续在此卷或其任何子目录上执行的挂载变化. 类似的,容器所创建的卷挂载在主机上是不可见的.这是默认模式.

该模式等同于 Linux 内核文档 中描述的 private 挂载传播选项.

  * HostToContainer - 此卷挂载将会感知到主机后续针对此卷或其任何子目录的挂载操作.

换句话说,如果主机在此挂载卷中挂载任何内容,容器将能看到它被挂载在那里.

类似的,配置了 Bidirectional 挂载传播选项的 Pod 如果在同一卷上挂载了内容,挂载传播设置为 HostToContainer 的容器都将能看到这一变化.

该模式等同于 Linux 内核文档 中描述的 rslave 挂载传播选项.

  * Bidirectional - 这种卷挂载和 HostToContainer 挂载表现相同. 另外,容器创建的卷挂载将被传播回至主机和使用同一卷的所有 Pod 的所有容器.

该模式等同于 Linux 内核文档 中描述的 rshared 挂载传播选项.

警告: Bidirectional 形式的挂载传播可能比较危险. 它可以破坏主机操作系统,因此它只被允许在特权容器中使用. 强烈建议你熟悉 Linux 内核行为. 此外,由 Pod 中的容器创建的任何卷挂载必须在终止时由容器销毁(卸载).

* 配置 
在某些部署环境中,挂载传播正常工作前,必须在 Docker 中正确配置挂载共享(mount share),如下所示.

编辑你的 Docker systemd 服务文件,按下面的方法设置 MountFlags:

MountFlags=shared

或者,如果存在 MountFlags=slave 就删除掉.然后重启 Docker 守护进程:

sudo systemctl daemon-reload
sudo systemctl restart docker



## 持久卷
本文描述 Kubernetes 中的 持久卷(Persistent Volume) . 建议先熟悉卷(Volume)的概念.


** 介绍 
存储的管理是一个与计算实例的管理完全不同的问题.PersistentVolume 子系统为用户 和管理员提供了一组 API,将存储如何供应的细节从其如何被使用中抽象出来. 为了实现这点,我们引入了两个新的 API 资源:PersistentVolume 和 PersistentVolumeClaim.

持久卷(PersistentVolume,PV)是集群中的一块存储,可以由管理员事先供应,或者 使用存储类(Storage Class)来动态供应. 持久卷是集群资源,就像节点也是集群资源一样.PV 持久卷和普通的 Volume 一样,也是使用 卷插件来实现的,只是它们拥有独立于任何使用 PV 的 Pod 的生命周期. 此 API 对象中记述了存储的实现细节,无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统.

持久卷申领(PersistentVolumeClaim,PVC)表达的是用户对存储的请求.概念上与 Pod 类似. Pod 会耗用节点资源,而 PVC 申领会耗用 PV 资源.Pod 可以请求特定数量的资源(CPU 和内存);同样 PVC 申领也可以请求特定的大小和访问模式 (例如,可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载,参见访问模式).

尽管 PersistentVolumeClaim 允许用户消耗抽象的存储资源,常见的情况是针对不同的 问题用户需要的是具有不同属性(如,性能)的 PersistentVolume 卷. 集群管理员需要能够提供不同性质的 PersistentVolume,并且这些 PV 卷之间的差别不 仅限于卷大小和访问模式,同时又不能将卷是如何实现的这些细节暴露给用户. 为了满足这类需求,就有了 存储类(StorageClass) 资源.


** 卷和申领的生命周期 
PV 卷是集群中的资源.PVC 申领是对这些资源的请求,也被用来执行对资源的申领检查. PV 卷和 PVC 申领之间的互动遵循如下生命周期:

* 供应 
PV 卷的供应有两种方式:静态供应或动态供应.

静态供应 
集群管理员创建若干 PV 卷.这些卷对象带有真实存储的细节信息,并且对集群 用户可用(可见).PV 卷对象存在于 Kubernetes API 中,可供用户消费(使用).

动态供应 
如果管理员所创建的所有静态 PV 卷都无法与用户的 PersistentVolumeClaim 匹配, 集群可以尝试为该 PVC 申领动态供应一个存储卷. 这一供应操作是基于 StorageClass 来实现的:PVC 申领必须请求某个 存储类,同时集群管理员必须 已经创建并配置了该类,这样动态供应卷的动作才会发生. 如果 PVC 申领指定存储类为 "",则相当于为自身禁止使用动态供应的卷.

为了基于存储类完成动态的存储供应,集群管理员需要在 API 服务器上启用 DefaultStorageClass 准入控制器. 举例而言,可以通过保证 DefaultStorageClass 出现在 API 服务器组件的 --enable-admission-plugins 标志值中实现这点;该标志的值可以是逗号 分隔的有序列表.关于 API 服务器标志的更多信息,可以参考 kube-apiserver 文档.

* 绑定 
用户创建一个带有特定存储容量和特定访问模式需求的 PersistentVolumeClaim 对象; 在动态供应场景下,这个 PVC 对象可能已经创建完毕. 主控节点中的控制回路监测新的 PVC 对象,寻找与之匹配的 PV 卷(如果可能的话), 并将二者绑定到一起. 如果为了新的 PVC 申领动态供应了 PV 卷,则控制回路总是将该 PV 卷绑定到这一 PVC 申领. 否则,用户总是能够获得他们所请求的资源,只是所获得的 PV 卷可能会超出所请求的配置. 一旦绑定关系建立,则 PersistentVolumeClaim 绑定就是排他性的,无论该 PVC 申领是 如何与 PV 卷建立的绑定关系. PVC 申领与 PV 卷之间的绑定是一种一对一的映射,实现上使用 ClaimRef 来记述 PV 卷 与 PVC 申领间的双向绑定关系.

如果找不到匹配的 PV 卷,PVC 申领会无限期地处于未绑定状态. 当与之匹配的 PV 卷可用时,PVC 申领会被绑定. 例如,即使某集群上供应了很多 50 Gi 大小的 PV 卷,也无法与请求 100 Gi 大小的存储的 PVC 匹配.当新的 100 Gi PV 卷被加入到集群时,该 PVC 才有可能被绑定.

* 使用 
Pod 将 PVC 申领当做存储卷来使用.集群会检视 PVC 申领,找到所绑定的卷,并 为 Pod 挂载该卷.对于支持多种访问模式的卷,用户要在 Pod 中以卷的形式使用申领 时指定期望的访问模式.

一旦用户有了申领对象并且该申领已经被绑定,则所绑定的 PV 卷在用户仍然需要它期间 一直属于该用户.用户通过在 Pod 的 volumes 块中包含 persistentVolumeClaim 节区来调度 Pod,访问所申领的 PV 卷.

* 保护使用中的存储对象 
保护使用中的存储对象(Storage Object in Use Protection)这一功能特性的目的 是确保仍被 Pod 使用的 PersistentVolumeClaim(PVC)对象及其所绑定的 PersistentVolume(PV)对象在系统中不会被删除,因为这样做可能会引起数据丢失.

说明: 当使用某 PVC 的 Pod 对象仍然存在时,认为该 PVC 仍被此 Pod 使用.

如果用户删除被某 Pod 使用的 PVC 对象,该 PVC 申领不会被立即移除. PVC 对象的移除会被推迟,直至其不再被任何 Pod 使用. 此外,如果管理员删除已绑定到某 PVC 申领的 PV 卷,该 PV 卷也不会被立即移除. PV 对象的移除也要推迟到该 PV 不再绑定到 PVC.

你可以看到当 PVC 的状态为 Terminating 且其 Finalizers 列表中包含 kubernetes.io/pvc-protection 时,PVC 对象是处于被保护状态的.

kubectl describe pvc hostpath
Name:          hostpath
Namespace:     default
StorageClass:  example-hostpath
Status:        Terminating
Volume:
Labels:        <none>
Annotations:   volume.beta.kubernetes.io/storage-class=example-hostpath
               volume.beta.kubernetes.io/storage-provisioner=example.com/hostpath
Finalizers:    [kubernetes.io/pvc-protection]
...

你也可以看到当 PV 对象的状态为 Terminating 且其 Finalizers 列表中包含 kubernetes.io/pv-protection 时,PV 对象是处于被保护状态的.

kubectl describe pv task-pv-volume

Name:            task-pv-volume
Labels:          type=local
Annotations:     <none>
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Terminating
Claim:
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        1Gi
Message:
Source:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/data
    HostPathType:
Events:            <none>

* 回收 
当用户不再使用其存储卷时,他们可以从 API 中将 PVC 对象删除,从而允许 该资源被回收再利用.PersistentVolume 对象的回收策略告诉集群,当其被 从申领中释放时如何处理该数据卷. 目前,数据卷可以被 Retained(保留)、Recycled(回收)或 Deleted(删除).

保留(Retain) 
回收策略 Retain 使得用户可以手动回收资源.当 PersistentVolumeClaim 对象 被删除时,PersistentVolume 卷仍然存在,对应的数据卷被视为"已释放(released)". 由于卷上仍然存在这前一申领人的数据,该卷还不能用于其他申领. 管理员可以通过下面的步骤来手动回收该卷:

  1. 删除 PersistentVolume 对象.与之相关的、位于外部基础设施中的存储资产 (例如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷)在 PV 删除之后仍然存在.
  2. 根据情况,手动清除所关联的存储资产上的数据.
  3. 手动删除所关联的存储资产.

如果你希望重用该存储资产,可以基于存储资产的定义创建新的 PersistentVolume 卷对象.

删除(Delete) 
对于支持 Delete 回收策略的卷插件,删除动作会将 PersistentVolume 对象从 Kubernetes 中移除,同时也会从外部基础设施(如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷)中移除所关联的存储资产. 动态供应的卷会继承其 StorageClass 中设置的回收策略,该策略默认 为 Delete. 管理员需要根据用户的期望来配置 StorageClass;否则 PV 卷被创建之后必须要被 编辑或者修补.

回收(Recycle) 
警告: 回收策略 Recycle 已被废弃.取而代之的建议方案是使用动态供应.
如果下层的卷插件支持,回收策略 Recycle 会在卷上执行一些基本的 擦除(rm -rf /thevolume/*)操作,之后允许该卷用于新的 PVC 申领.

* 预留 PersistentVolume 
通过在 PersistentVolumeClaim 中指定 PersistentVolume,你可以声明该特定 PV 与 PVC 之间的绑定关系.如果该 PersistentVolume 存在且未被通过其 claimRef 字段预留给 PersistentVolumeClaim,则该 PersistentVolume 会和该 PersistentVolumeClaim 绑定到一起.

绑定操作不会考虑某些卷匹配条件是否满足,包括节点亲和性等等. 控制面仍然会检查 存储类、访问模式和所请求的 存储尺寸都是合法的.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: foo-pvc
  namespace: foo
spec:
  storageClassName: "" # 此处须显式设置空字符串,否则会被设置为默认的 StorageClass
  volumeName: foo-pv
  ...

此方法无法对 PersistentVolume 的绑定特权做出任何形式的保证. 如果有其他 PersistentVolumeClaim 可以使用你所指定的 PV,则你应该首先预留 该存储卷.你可以将 PV 的 claimRef 字段设置为相关的 PersistentVolumeClaim 以确保其他 PVC 不会绑定到该 PV 卷.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: foo-pv
spec:
  storageClassName: ""
  claimRef:
    name: foo-pvc
    namespace: foo
  ...

如果你想要使用 claimPolicy 属性设置为 Retain 的 PersistentVolume 卷 时,包括你希望复用现有的 PV 卷时,这点是很有用的

* 扩充 PVC 申领 
特性状态: Kubernetes v1.11 [beta]

现在,对扩充 PVC 申领的支持默认处于被启用状态.你可以扩充以下类型的卷:

azureDisk
azureFile
awsElasticBlockStore
cinder (deprecated)
csi
flexVolume (deprecated)
gcePersistentDisk
glusterfs
rbd
portworxVolume

只有当 PVC 的存储类中将 allowVolumeExpansion 设置为 true 时,你才可以扩充该 PVC 申领.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gluster-vol-default
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://192.168.10.100:8080"
  restuser: ""
  secretNamespace: ""
  secretName: ""
allowVolumeExpansion: true

如果要为某 PVC 请求较大的存储卷,可以编辑 PVC 对象,设置一个更大的尺寸值. 这一编辑操作会触发为下层 PersistentVolume 提供存储的卷的扩充. Kubernetes 不会创建新的 PV 卷来满足此申领的请求. 与之相反,现有的卷会被调整大小.

警告: 直接编辑 PersistentVolume 的大小可以阻止该卷自动调整大小. 如果对 PersistentVolume 的容量进行编辑,然后又将其所对应的 PersistentVolumeClaim 的 .spec 进行编辑,使该 PersistentVolumeClaim 的大小匹配 PersistentVolume 的话,则不会发生存储大小的调整. Kubernetes 控制平面将看到两个资源的所需状态匹配,并认为其后备卷的大小 已被手动增加,无需调整.

-* CSI 卷的扩充 
特性状态: Kubernetes v1.16 [beta]
对 CSI 卷的扩充能力默认是被启用的,不过扩充 CSI 卷要求 CSI 驱动支持 卷扩充操作.

-* 重设包含文件系统的卷的大小
只有卷中包含的文件系统是 XFS、Ext3 或者 Ext4 时,你才可以重设卷的大小.

当卷中包含文件系统时,只有在 Pod 使用 ReadWrite 模式来使用 PVC 申领的 情况下才能重设其文件系统的大小. 文件系统扩充的操作或者是在 Pod 启动期间完成,或者在下层文件系统支持在线 扩充的前提下在 Pod 运行期间完成.

如果 FlexVolumes 的驱动将 RequiresFSResize 能力设置为 true,则该 FlexVolume 卷(于 Kubernetes v1.23 弃用)可以在 Pod 重启期间调整大小.

-* 重设使用中 PVC 申领的大小 
特性状态: Kubernetes v1.15 [beta]

说明: Kubernetes 从 1.15 版本开始将调整使用中 PVC 申领大小这一能力作为 Beta 特性支持;该特性在 1.11 版本以来处于 Alpha 阶段. ExpandInUsePersistentVolumes 特性必须被启用;在很多集群上,与此类似的 Beta 阶段的特性是自动启用的. 可参考特性门控 文档了解更多信息.

在这种情况下,你不需要删除和重建正在使用某现有 PVC 的 Pod 或 Deployment. 所有使用中的 PVC 在其文件系统被扩充之后,立即可供其 Pod 使用. 此功能特性对于没有被 Pod 或 Deployment 使用的 PVC 而言没有效果. 你必须在执行扩展操作之前创建一个使用该 PVC 的 Pod.

与其他卷类型类似,FlexVolume 卷也可以在被 Pod 使用期间执行扩充操作.

说明: FlexVolume 卷的重设大小只能在下层驱动支持重设大小的时候才可进行.

说明: 扩充 EBS 卷的操作非常耗时.同时还存在另一个配额限制: 每 6 小时只能执行一次(尺寸)修改操作.

-* 处理扩充卷过程中的失败 
如果用户指定的新大小过大,底层存储系统无法满足,PVC 的扩展将不断重试, 直到用户或集群管理员采取一些措施.这种情况是不希望发生的,因此 Kubernetes 提供了以下从此类故障中恢复的方法.

集群管理员手动处理
如果扩充下层存储的操作失败,集群管理员可以手动地恢复 PVC 申领的状态并 取消重设大小的请求.否则,在没有管理员干预的情况下,控制器会反复重试 重设大小的操作.

  1. 将绑定到 PVC 申领的 PV 卷标记为 Retain 回收策略;
  2. 删除 PVC 对象.由于 PV 的回收策略为 Retain,我们不会在重建 PVC 时丢失数据.
  3. 删除 PV 规约中的 claimRef 项,这样新的 PVC 可以绑定到该卷. 这一操作会使得 PV 卷变为 "可用(Available)".
  4. 使用小于 PV 卷大小的尺寸重建 PVC,设置 PVC 的 volumeName 字段为 PV 卷的名称. 这一操作将把新的 PVC 对象绑定到现有的 PV 卷.
  5. 不要忘记恢复 PV 卷上设置的回收策略.

-* 通过请求扩展为更小尺寸
特性状态: Kubernetes v1.23 [alpha]

说明: Kubernetes 从 1.23 版本开始将允许用户恢复失败的 PVC 扩展这一能力作为 alpha 特性支持. RecoverVolumeExpansionFailure 必须被启用以允许使用此功能. 可参考特性门控 文档了解更多信息.

如果集群中的特性门控 ExpandPersistentVolumes 和 RecoverVolumeExpansionFailure 都已启用,在 PVC 的扩展发生失败时,你可以使用比先前请求的值更小的尺寸来重试扩展. 要使用一个更小的尺寸尝试请求新的扩展,请编辑该 PVC 的 .spec.resources 并选择 一个比你之前所尝试的值更小的值. 如果由于容量限制而无法成功扩展至更高的值,这将很有用. 如果发生了这种情况,或者你怀疑可能发生了这种情况,你可以通过指定一个在底层存储供应容量 限制内的尺寸来重试扩展.你可以通过查看 .status.resizeStatus 以及 PVC 上的事件 来监控调整大小操作的状态.

请注意, 尽管你可以指定比之前的请求更低的存储量,新值必须仍然高于 .status.capacity. Kubernetes 不支持将 PVC 缩小到小于其当前的尺寸.


** 持久卷的类型 
PV 持久卷是用插件的形式来实现的.Kubernetes 目前支持以下插件:

  * awsElasticBlockStore - AWS 弹性块存储(EBS)
  * azureDisk - Azure Disk
  * azureFile - Azure File
  * cephfs - CephFS volume
  * csi - 容器存储接口 (CSI)
  * fc - Fibre Channel (FC) 存储
  * gcePersistentDisk - GCE 持久化盘
  * glusterfs - Glusterfs 卷
  * hostPath - HostPath 卷 (仅供单节点测试使用;不适用于多节点集群; 请尝试使用 local 卷作为替代)
  * iscsi - iSCSI (SCSI over IP) 存储
  * local - 节点上挂载的本地存储设备
  * nfs - 网络文件系统 (NFS) 存储
  * portworxVolume - Portworx 卷
  * rbd - Rados 块设备 (RBD) 卷
  * vsphereVolume - vSphere VMDK 卷

以下的持久卷已被弃用.这意味着当前仍是支持的,但是 Kubernetes 将来的发行版会将其移除.

  * cinder - Cinder(OpenStack 块存储)(于 v1.18 弃用)
  * flexVolume - FlexVolume (于 v1.23 弃用)
  * flocker - Flocker 存储(于 v1.22 弃用)
  * quobyte - Quobyte 卷 (于 v1.22 弃用)
  * storageos - StorageOS 卷(于 v1.22 弃用)

旧版本的 Kubernetes 仍支持这些"树内(In-Tree)"持久卷类型:

  * photonPersistentDisk - Photon 控制器持久化盘.(v1.15 之后 不可用)
  * scaleIO - ScaleIO 卷(v1.21 之后 不可用)


** 持久卷 
每个 PV 对象都包含 spec 部分和 status 部分,分别对应卷的规约和状态. PersistentVolume 对象的名称必须是合法的 DNS 子域名.

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2

说明: 在集群中使用持久卷存储通常需要一些特定于具体卷类型的辅助程序. 在这个例子中,PersistentVolume 是 NFS 类型的,因此需要辅助程序 /sbin/mount.nfs 来支持挂载 NFS 文件系统.

* 容量 
一般而言,每个 PV 卷都有确定的存储容量. 容量属性是使用 PV 对象的 capacity 属性来设置的. 参考词汇表中的 量纲(Quantity) 词条,了解 capacity 字段可以接受的单位.

量纲(Quantity)
https://kubernetes.io/zh/docs/reference/glossary/?all=true#term-quantity

目前,存储大小是可以设置和请求的唯一资源. 未来可能会包含 IOPS、吞吐量等属性.

* 卷模式 
特性状态: Kubernetes v1.18 [stable]

针对 PV 持久卷,Kubernetes 支持两种卷模式(volumeModes):Filesystem(文件系统) 和 Block(块). volumeMode 是一个可选的 API 参数. 如果该参数被省略,默认的卷模式是 Filesystem.

volumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载(Mount) 到某个目录. 如果卷的存储来自某块设备而该设备目前为空,Kuberneretes 会在第一次挂载卷之前 在设备上创建文件系统.

你可以将 volumeMode 设置为 Block,以便将卷作为原始块设备来使用. 这类卷以块设备的方式交给 Pod 使用,其上没有任何文件系统. 这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助,Pod 和 卷之间不存在文件系统层.另外,Pod 中运行的应用必须知道如何处理原始块设备.

* 访问模式 
PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上. 如下表所示,提供者(驱动)的能力不同,每个 PV 卷的访问模式都会设置为 对应卷所支持的模式值. 例如,NFS 可以支持多个读写客户,但是某个特定的 NFS PV 卷可能在服务器 上以只读的方式导出.每个 PV 卷都会获得自身的访问模式集合,描述的是 特定 PV 卷的能力.

访问模式有:

  ReadWriteOnce
  卷可以被一个节点以读写方式挂载. ReadWriteOnce 访问模式也允许运行在同一节点上的多个 Pod 访问卷.
  
  ReadOnlyMany  
  卷可以被多个节点以只读方式挂载.

  ReadWriteMany
  卷可以被多个节点以读写方式挂载.

  ReadWriteOncePod
  卷可以被单个 Pod 以读写方式挂载. 如果你想确保整个集群中只有一个 Pod 可以读取或写入该 PVC, 请使用ReadWriteOncePod 访问模式.这只支持 CSI 卷以及需要 Kubernetes 1.22 以上版本.

在命令行接口(CLI)中,访问模式也使用以下缩写形式:

  * RWO - ReadWriteOnce
  * ROX - ReadOnlyMany
  * RWX - ReadWriteMany
  * RWOP - ReadWriteOncePod

重要提醒！ 每个卷同一时刻只能以一种访问模式挂载,即使该卷能够支持 多种访问模式.例如,一个 GCEPersistentDisk 卷可以被某节点以 ReadWriteOnce 模式挂载,或者被多个节点以 ReadOnlyMany 模式挂载,但不可以同时以两种模式 挂载.

卷插件	                          ReadWriteOnce	  ReadOnlyMany	ReadWriteMany	  ReadWriteOncePod
AWSElasticBlockStore	  ✓	                        -                         -	                          -
AzureFile	                        ✓	                       ✓	                        ✓	                        -
AzureDisk	                      ✓	                       -	                        -	                          -
CephFS	                          ✓	                       ✓	                        ✓	                        -
Cinder	                            ✓	                       -	                        -	                          -
CSI	                                  取决于驱动	     取决于驱动	      取决于驱动	      取决于驱动
FC	                                  ✓	                       ✓	                         -	                         -
FlexVolume     	              ✓	                       ✓	                         取决于驱动	       -
Flocker	                          ✓	                       -	                         -	                         -
GCEPersistentDisk	       ✓	                         ✓ 	                       -	                         -
Glusterfs 	                     ✓	                         ✓	                         ✓	                         -
HostPath	                     ✓                         -	                         -	                         -
iSCSI	                             ✓                         ✓                         -	                         -
Quobyte	                       ✓                         ✓	                        ✓	                         -
NFS	                               ✓	                         ✓                        	✓	                         -
RBD	                               ✓	                         ✓                        -	                           -
VsphereVolume	           ✓	                         -	                       -(Pod 运行于同	   -
                                                                                              一节点上时可行)
PortworxVolume	         ✓	                         -	                        ✓	                         -
StorageOS	                     ✓	                         -	                        -	                           -

* 类 
每个 PV 可以属于某个类(Class),通过将其 storageClassName 属性设置为某个 StorageClass 的名称来指定. 特定类的 PV 卷只能绑定到请求该类存储卷的 PVC 申领. 未设置 storageClassName 的 PV 卷没有类设定,只能绑定到那些没有指定特定 存储类的 PVC 申领.

早前,Kubernetes 使用注解 volume.beta.kubernetes.io/storage-class 而不是 storageClassName 属性.这一注解目前仍然起作用,不过在将来的 Kubernetes 发布版本中该注解会被彻底废弃.

*回收策略 
目前的回收策略有:

  * Retain -- 手动回收
  * Recycle -- 基本擦除 (rm -rf /thevolume/*)
  * Delete -- 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除

目前,仅 NFS 和 HostPath 支持回收(Recycle). AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除(Delete).

* 挂载选项 
Kubernetes 管理员可以指定持久卷被挂载到节点上时使用的附加挂载选项.

说明: 并非所有持久卷类型都支持挂载选项.

以下卷类型支持挂载选项:

  * awsElasticBlockStore
  * azureDisk
  * azureFile
  * cephfs
  * cinder (已弃用于 v1.18)
  * gcePersistentDisk
  * glusterfs
  * iscsi
  * nfs
  * quobyte (已弃用于 v1.22)
  * rbd
  * storageos (已弃用于 v1.22)
  * vsphereVolume

Kubernetes 不对挂载选项执行合法性检查.如果挂载选项是非法的,挂载就会失败.

早前,Kubernetes 使用注解 volume.beta.kubernetes.io/mount-options 而不是 mountOptions 属性.这一注解目前仍然起作用,不过在将来的 Kubernetes 发布版本中该注解会被彻底废弃.

* 节点亲和性 
每个 PV 卷可以通过设置节点亲和性来定义一些约束,进而限制从哪些节点上可以访问此卷. 使用这些卷的 Pod 只会被调度到节点亲和性规则所选择的节点上执行. 要设置节点亲和性,配置 PV 卷 .spec 中的 nodeAffinity. 持久卷 API 参考关于该字段的更多细节.

说明: 对大多数类型的卷而言,你不需要设置节点亲和性字段. AWS EBS、 GCE PD 和 Azure Disk 卷类型都能 自动设置相关字段. 你需要为 local 卷显式地设置 此属性.

* 阶段 
每个卷会处于以下阶段(Phase)之一:

  * Available(可用)-- 卷是一个空闲资源,尚未绑定到任何申领;
  * Bound(已绑定)-- 该卷已经绑定到某申领;
  * Released(已释放)-- 所绑定的申领已被删除,但是资源尚未被集群回收;
  * Failed(失败)-- 卷的自动回收操作失败.

命令行接口能够显示绑定到某 PV 卷的 PVC 对象.


** PersistentVolumeClaims 
每个 PVC 对象都有 spec 和 status 部分,分别对应申领的规约和状态. PersistentVolumeClaim 对象的名称必须是合法的 DNS 子域名.

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}

* 访问模式
申领在请求具有特定访问模式的存储时,使用与卷相同的访问模式约定.

* 卷模式
申领使用与卷相同的约定来表明是将卷作为文件系统还是块设备来使用.

* 资源 
申领和 Pod 一样,也可以请求特定数量的资源.在这个上下文中,请求的资源是存储. 卷和申领都使用相同的 资源模型.

* 选择算符 
申领可以设置标签选择算符 来进一步过滤卷集合.只有标签与选择算符相匹配的卷能够绑定到申领上. 选择算符包含两个字段:

  * matchLabels - 卷必须包含带有此值的标签
  * matchExpressions - 通过设定键(key)、值列表和操作符(operator) 来构造的需求.合法的操作符有 In、NotIn、Exists 和 DoesNotExist.

来自 matchLabels 和 matchExpressions 的所有需求都按逻辑与的方式组合在一起. 这些需求都必须被满足才被视为匹配.

* 类 
申领可以通过为 storageClassName 属性设置 StorageClass 的名称来请求特定的存储类. 只有所请求的类的 PV 卷,即 storageClassName 值与 PVC 设置相同的 PV 卷, 才能绑定到 PVC 申领.

PVC 申领不必一定要请求某个类.如果 PVC 的 storageClassName 属性值设置为 "", 则被视为要请求的是没有设置存储类的 PV 卷,因此这一 PVC 申领只能绑定到未设置 存储类的 PV 卷(未设置注解或者注解值为 "" 的 PersistentVolume(PV)对象在系统中不会被删除,因为这样做可能会引起数据丢失. 未设置 storageClassName 的 PVC 与此大不相同,也会被集群作不同处理. 具体筛查方式取决于 DefaultStorageClass 准入控制器插件 是否被启用.

  * 如果准入控制器插件被启用,则管理员可以设置一个默认的 StorageClass. 所有未设置 storageClassName 的 PVC 都只能绑定到隶属于默认存储类的 
     PV 卷. 设置默认 StorageClass 的工作是通过将对应 StorageClass 对象的注解 storageclass.kubernetes.io/is-default-class 赋值为 true 来完成的. 如果
     管理员未设置默认存储类,集群对 PVC 创建的处理方式与未启用准入控制器插件 时相同.如果设定的默认存储类不止一个,准入控制插件会禁止所
     有创建 PVC 操作.
  * 如果准入控制器插件被关闭,则不存在默认 StorageClass 的说法. 所有未设置 storageClassName 的 PVC 都只能绑定到未设置存储类的 PV 卷. 在这
     种情况下,未设置 storageClassName 的 PVC 与 storageClassName 设置未 "" 的 PVC 的处理方式相同.

取决于安装方法,默认的 StorageClass 可能在集群安装期间由插件管理器(Addon Manager)部署到集群中.

当某 PVC 除了请求 StorageClass 之外还设置了 selector,则这两种需求会按 逻辑与关系处理:只有隶属于所请求类且带有所请求标签的 PV 才能绑定到 PVC.

说明: 目前,设置了非空 selector 的 PVC 对象无法让集群为其动态供应 PV 卷.
早前,Kubernetes 使用注解 volume.beta.kubernetes.io/storage-class 而不是 storageClassName 属性.这一注解目前仍然起作用,不过在将来的 Kubernetes 发布版本中该注解会被彻底废弃.


** 使用申领作为卷 
Pod 将申领作为卷来使用,并藉此访问存储资源. 申领必须位于使用它的 Pod 所在的同一名字空间内. 集群在 Pod 的名字空间中查找申领,并使用它来获得申领所使用的 PV 卷. 之后,卷会被挂载到宿主上并挂载到 Pod 中.

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

* 关于名字空间的说明 
PersistentVolume 卷的绑定是排他性的. 由于 PersistentVolumeClaim 是名字空间作用域的对象,使用 "Many" 模式(ROX、RWX)来挂载申领的操作只能在同一名字空间内进行.

* 类型为 hostpath 的 PersistentVolume 
hostPath PersistentVolume 使用节点上的文件或目录来模拟网络附加(network-attached)存储.


** 原始块卷支持 
特性状态: Kubernetes v1.18 [stable]

以下卷插件支持原始块卷,包括其动态供应(如果支持的话)的卷:

  * AWSElasticBlockStore
  * AzureDisk
  * CSI
  * FC (光纤通道)
  * GCEPersistentDisk
  * iSCSI
  * Local 卷
  * OpenStack Cinder
  * RBD (Ceph 块设备)
  * VsphereVolume

* 使用原始块卷的持久卷 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: block-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  persistentVolumeReclaimPolicy: Retain
  fc:
    targetWWNs: ["50060e801049cfd1"]
    lun: 0
    readOnly: false

* 申请原始块卷的 PVC 申领 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 10Gi


** 卷填充器(Populator)与数据源 
特性状态: Kubernetes v1.22 [alpha]

说明:
Kubernetes 支持自定义的卷填充器;Kubernetes 1.18 版本引入了这个 alpha 特性. Kubernetes 1.22 使用重新设计的 API 重新实现了该机制. 确认你正在阅读与你的集群版本一致的 Kubernetes 文档. 要获知版本信息,请输入 kubectl version.

要使用自定义的卷填充器,你必须为 kube-apiserver 和 kube-controller-manager 启用 AnyVolumeDataSource 特性门控.

卷填充器利用了 PVC 规约字段 dataSourceRef. 不像 dataSource 字段只能包含对另一个持久卷申领或卷快照的引用, dataSourceRef 字段可以包含对同一命名空间中任何对象的引用(不包含除 PVC 以外的核心资源). 对于启用了特性门控的集群,使用 dataSourceRef 比 dataSource 更好.


** 数据源引用 
dataSourceRef 字段的行为与 dataSource 字段几乎相同. 如果其中一个字段被指定而另一个字段没有被指定,API 服务器将给两个字段相同的值. 这两个字段都不能在创建后改变,如果试图为这两个字段指定不同的值,将导致验证错误. 因此,这两个字段将总是有相同的内容.

在 dataSourceRef 字段和 dataSource 字段之间有两个用户应该注意的区别:

  * dataSource 字段会忽略无效的值(如同是空值), 而 dataSourceRef 字段永远不会忽略值,并且若填入一个无效的值,会导致错误. 无效值指的是 PVC 之外的核心对象(没有 apiGroup 的对象).
  * dataSourceRef 字段可以包含不同类型的对象,而 dataSource 字段只允许 PVC 和卷快照.

用户应该始终在启用了特性门控的集群上使用 dataSourceRef,而在没有启用特性门控的集群上使用 dataSource. 在任何情况下都没有必要查看这两个字段. 这两个字段的值看似相同但是语义稍微不一样,是为了向后兼容. 特别是混用旧版本和新版本的控制器时,它们能够互通.


** 使用卷填充器 --- 未知如何使用
卷填充器是能创建非空卷的控制器, 其卷的内容通过一个自定义资源决定. 用户通过使用 dataSourceRef 字段引用自定义资源来创建一个被填充的卷:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: populated-pvc
spec:
  dataSourceRef:
    name: example-name
    kind: ExampleDataSource
    apiGroup: example.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

因为卷填充器是外部组件,如果没有安装所有正确的组件,试图创建一个使用卷填充器的 PVC 就会失败. 外部控制器应该在 PVC 上产生事件,以提供创建状态的反馈,包括在由于缺少某些组件而无法创建 PVC 的情况下发出警告.

你可以把 alpha 版本的卷数据源验证器 控制器安装到你的集群中. 如果没有填充器处理该数据源的情况下,该控制器会在 PVC 上产生警告事件. 当一个合适的填充器被安装到 PVC 上时,该控制器的职责是上报与卷创建有关的事件,以及在该过程中发生的问题.

* 在容器中添加原始块设备路径的 Pod 规约

apiVersion: v1
kind: Pod
metadata:
  name: pod-with-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: [ "tail -f /dev/null" ]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: block-pvc

说明: 向 Pod 中添加原始块设备时,你要在容器内设置设备路径而不是挂载路径.

* 绑定块卷 
如果用户通过 PersistentVolumeClaim 规约的 volumeMode 字段来表明对原始 块设备的请求,绑定规则与之前版本中未在规约中考虑此模式的实现略有不同. 下面列举的表格是用户和管理员可以为请求原始块设备所作设置的组合. 此表格表明在不同的组合下卷是否会被绑定.

静态供应卷的卷绑定矩阵:

PV volumeMode	  PVC volumeMode	  Result
未指定	                未指定	                  绑定
未指定	                Block	                      不绑定
未指定	                Filesystem	            绑定
Block	                    未指定	                  不绑定
Block	                    Block	                      绑定
Block	                    Filesystem	            不绑定
Filesystem	          Filesystem	            绑定
Filesystem	          Block	                      不绑定
Filesystem	          未指定	                  绑定

说明: Alpha 发行版本中仅支持静态供应的卷. 管理员需要在处理原始块设备时小心处理这些值.


** 对卷快照及从卷快照中恢复卷的支持 
特性状态: Kubernetes v1.17 [beta]

卷快照(Volume Snapshot)功能的添加仅是为了支持 CSI 卷插件.

* 基于卷快照创建 PVC 申领


** 卷克隆 
卷克隆功能特性仅适用于 CSI 卷插件.

* 基于现有 PVC 创建新的 PVC 申领 


** 编写可移植的配置 
如果你要编写配置模板和示例用来在很多集群上运行并且需要持久性存储,建议你使用以下模式:

  * 将 PersistentVolumeClaim 对象包含到你的配置包(Bundle)中,和 Deployment 以及 ConfigMap 等放在一起.
  * 不要在配置中包含 PersistentVolume 对象,因为对配置进行实例化的用户很可能 没有创建 PersistentVolume 的权限.
  * 为用户提供在实例化模板时指定存储类名称的能力.
      * 仍按用户提供存储类名称,将该名称放到 persistentVolumeClaim.storageClassName 字段中. 这样会使得 PVC 在集群被管理员启用了存储类支持时能够匹配到正确的存储类,
      *如果用户未指定存储类名称,将 persistentVolumeClaim.storageClassName 留空(nil). 这样,集群会使用默认 StorageClass 为用户自动供应一个存储卷. 很多集群环境都配置了默认的 StorageClass,或者管理员也可以自行创建默认的 StorageClass.
  * 在你的工具链中,监测经过一段时间后仍未被绑定的 PVC 对象,要让用户知道这些对象, 因为这可能意味着集群不支持动态存储(因而用户必须先创建一个匹配的 PV),或者 集群没有配置存储系统(因而用户无法配置需要 PVC 的工作负载配置).



## 投射卷
本文档描述 Kubernet 中的投射卷(Projected Volumes). 


** 介绍 
一个 projected 卷可以将若干现有的卷源映射到同一个目录之上.

目前,以下类型的卷源可以被投射:

  * secret
  * downwardAPI
  * configMap
  * serviceAccountToken

所有的卷源都要求处于 Pod 所在的同一个名字空间内.

* 带有 Secret、DownwardAPI 和 ConfigMap 的配置示例

pods/storage/projected-secret-downwardapi-configmap.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox:1.28
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config

* 带有非默认权限模式设置的 Secret 的配置示例

pods/storage/projected-secrets-nondefault-permission-mode.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox:1.28
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511

每个被投射的卷源都列举在规约中的 sources 下面.参数几乎相同,只有两个例外:

对于 Secret,secretName 字段被改为 name 以便于 ConfigMap 的命名一致;
defaultMode 只能在投射层(使用的 pod 时)级设置,不能在卷源层级(pv,pvc,vloume层)设置.不过,正如上面所展示的, 你可以显式地为每个投射单独设置 mode 属性.


** serviceAccountToken 投射卷 
当 TokenRequestProjection 特性被启用时,你可以将当前 服务账号 的令牌注入到 Pod 中特定路径下.例如:

pods/storage/projected-service-account-token.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: sa-token-test
spec:
  containers:
  - name: container-test
    image: busybox:1.28
    volumeMounts:
    - name: token-vol
      mountPath: "/service-account"
      readOnly: true
  serviceAccountName: default
  volumes:
  - name: token-vol
    projected:
      sources:
      - serviceAccountToken:
          audience: api
          expirationSeconds: 3600
          path: token

示例 Pod 中包含一个投射卷,其中包含注入的服务账号令牌. 此 Pod 中的容器可以使用该令牌访问 Kubernetes API 服务器, 使用 pod 的 ServiceAccount 进行身份验证.audience 字段包含令牌所针对的受众. 收到令牌的主体必须使用令牌受众中所指定的某个标识符来标识自身,否则应该拒绝该令牌. 此字段是可选的,默认值为 API 服务器的标识.

字段 expirationSeconds 是服务账号令牌预期的生命期长度.默认值为 1 小时, 必须至少为 10 分钟(600 秒).管理员也可以通过设置 API 服务器的命令行参数 --service-account-max-token-expiration 来为其设置最大值上限.path 字段给出 与投射卷挂载点之间的相对路径.

说明:
以 subPath 形式使用投射卷源的容器无法收到对应卷源的更新.


** 与 SecurityContext 间的关系 
关于在投射的服务账号卷中处理文件访问权限的提案 介绍了如何使得所投射的文件具有合适的属主访问权限.

Linux
在包含了投射卷并在 SecurityContext 中设置了 RunAsUser 属性的 Linux Pod 中,投射文件具有正确的属主属性设置, 其中包含了容器用户属主.

Windows
......

说明:
总体而言,为容器授予访问宿主系统的权限这种做法是不推荐的,因为这样做可能会打开潜在的安全性攻击之门.

在创建 Windows Pod 时,如过在其 SecurityContext 中设置了 RunAsUser, Pod 会一直阻塞在 ContainerCreating 状态.因此,建议不要在 Windows 节点上使用仅针对 Linux 的 RunAsUser 选项.



## 临时卷
有些应用程序需要额外的存储,但并不关心数据在重启后仍然可用. 例如,缓存服务经常受限于内存大小,将不常用的数据转移到比内存慢、但对总体性能的影响很小的存储中.

另有些应用程序需要以文件形式注入的只读数据,比如配置数据或密钥.

临时卷 就是为此类用例设计的.因为卷会遵从 Pod 的生命周期,与 Pod 一起创建和删除, 所以停止和重新启动 Pod 时,不会受持久卷在何处可用的限制.

临时卷在 Pod 规范中以 内联 方式定义,这简化了应用程序的部署和管理.


** 临时卷的类型
Kubernetes 为了不同的目的,支持几种不同类型的临时卷:

  * emptyDir: Pod 启动时为空,存储空间来自本地的 kubelet 根目录(通常是根磁盘)或内存
  * configMap、 downwardAPI、 secret: 将不同类型的 Kubernetes 数据注入到 Pod 中
  * CSI 临时卷: 类似于前面的卷类型,但由专门支持此特性 的指定 CSI 驱动程序提供
  * 通用临时卷: 它可以由所有支持持久卷的存储驱动程序提供

emptyDir、configMap、downwardAPI、secret 是作为 本地临时存储 提供的.它们由各个节点上的 kubelet 管理.

CSI 临时卷 必须 由第三方 CSI 存储驱动程序提供.

通用临时卷 可以 由第三方 CSI 存储驱动程序提供,也可以由支持动态配置的任何其他存储驱动程序提供. 一些专门为 CSI 临时卷编写的 CSI 驱动程序,不支持动态供应:因此这些驱动程序不能用于通用临时卷.

使用第三方驱动程序的优势在于,它们可以提供 Kubernetes 本身不支持的功能, 例如,与 kubelet 管理的磁盘具有不同运行特征的存储,或者用来注入不同的数据


** CSI 临时卷
特性状态: Kubernetes v1.16 [beta]

该特性需要启用参数 CSIInlineVolume 特性门控(feature gate). 该参数从 Kubernetes 1.16 开始默认启用.

说明: 只有一部分 CSI 驱动程序支持 CSI 临时卷.Kubernetes CSI 驱动程序列表 显示了支持临时卷的驱动程序.


** CSI 驱动程序限制
特性状态: Kubernetes v1.21 [deprecated]
作为一个集群管理员,你可以使用 PodSecurityPolicy 来控制在 Pod 中可以使用哪些 CSI 驱动程序, 具体则是通过 allowedCSIDrivers 字段 指定.

说明: PodSecurityPolicy 已弃用,并将在 Kubernetes v1.25 版本中移除.
说明: CSI 临时卷仅有 CSI 驱动程序的一个子集支持. Kubernetes CSI 驱动列表显示了哪些驱动程序支持临时卷.


** 通用临时卷 
特性状态: Kubernetes v1.23 [stable]
通用临时卷类似于 emptyDir 卷,因为它为每个 Pod 提供临时数据存放目录, 在最初制备完毕时一般为空.不过通用临时卷也有一些额外的功能特性:

  * 存储可以是本地的,也可以是网络连接的.
  * 卷可以有固定的大小,pod不能超量使用.
  * 卷可能有一些初始数据,这取决于驱动程序和参数.
  * 当驱动程序支持,卷上的典型操作将被支持,包括 (快照、 克隆、 调整大小和 存储容量跟踪).

示例:

kind: Pod
apiVersion: v1
metadata:
  name: my-app
spec:
  containers:
    - name: my-frontend
      image: busybox:1.28
      volumeMounts:
      - mountPath: "/scratch"
        name: scratch-volume
      command: [ "sleep", "1000000" ]
  volumes:
    - name: scratch-volume
      ephemeral:
        volumeClaimTemplate:
          metadata:
            labels:
              type: my-frontend-volume
          spec:
            accessModes: [ "ReadWriteOnce" ]
            storageClassName: "scratch-storage-class"
            resources:
              requests:
                storage: 1Gi 


** 生命周期和 PersistentVolumeClaim
关键的设计思想是在 Pod 的卷来源中允许使用 卷申领的参数. PersistentVolumeClaim 的标签、注解和整套字段集均被支持. 创建这样一个 Pod 后, 临时卷控制器在 Pod 所属的命名空间中创建一个实际的 PersistentVolumeClaim 对象, 并确保删除 Pod 时,同步删除 PersistentVolumeClaim.

如上设置将触发卷的绑定与/或准备操作,相应动作或者在 StorageClass 使用即时卷绑定时立即执行, 或者当 Pod 被暂时性调度到某节点时执行 (WaitForFirstConsumer 卷绑定模式). 对于常见的临时卷,建议采用后者,这样调度器就可以自由地为 Pod 选择合适的节点. 对于即时绑定,调度器则必须选出一个节点,使得在卷可用时,能立即访问该卷.

就资源所有权而言, 拥有通用临时存储的 Pod 是提供临时存储 (ephemeral storage) 的 PersistentVolumeClaim 的所有者. 当 Pod 被删除时,Kubernetes 垃圾收集器会删除 PVC, 然后 PVC 通常会触发卷的删除,因为存储类的默认回收策略是删除卷. 你可以使用带有 retain 回收策略的 StorageClass 创建准临时 (quasi-ephemeral) 本地存储: 该存储比 Pod 寿命长,在这种情况下,你需要确保单独进行卷清理.

当这些 PVC 存在时,它们可以像其他 PVC 一样使用. 特别是,它们可以被引用作为批量克隆或快照的数据源. PVC对象还保持着卷的当前状态.


** PersistentVolumeClaim 的命名
自动创建的 PVCs 的命名是确定的:此名称是 Pod 名称和卷名称的组合,中间由连字符(-)连接. 在上面的示例中,PVC 将命名为 my-app-scratch-volume . 这种确定性命名方式使得与 PVC 交互变得更容易,因为一旦知道 Pod 名称和卷名,就不必搜索它.

这种确定性命名方式也引入了潜在的冲突, 比如在不同的 Pod 之间(名为 "Pod-a" 的 Pod 挂载名为 "scratch" 的卷, 和名为 "pod" 的 Pod 挂载名为 "a-scratch" 的卷,这两者均会生成名为 "pod-a-scratch" 的PVC),或者在 Pod 和手工创建的 PVC 之间.

以下冲突会被检测到:如果 PVC 是为 Pod 创建的,那么它只用于临时卷. 此检测基于所有权关系.现有的 PVC 不会被覆盖或修改. 但这并不能解决冲突,因为如果没有正确的 PVC,Pod 就无法启动.

注意: 当命名 Pods 和卷出现在同一个命名空间中时,要小心,以防止发生此类冲突.


** 安全
启用 GenericEphemeralVolume 特性会导致那些没有 PVCs 创建权限的用户, 在创建 Pods 时,被允许间接的创建 PVCs. 集群管理员必须意识到这一点. 如果这不符合他们的安全模型,他们有如下选择:

  * 通过特性门控显式禁用该特性.
  * 使用一个准入 Webhook 拒绝包含通用临时卷的 Pods.
  * 当 volumes 列表不包含 ephemeral 卷类型时,使用 Pod 安全策略. (这一方式在 Kubernetes 1.21 版本已经弃用)

为 PVC 卷所设置的逐名字空间的配额 仍然有效,因此即使允许用户使用这种新机制,他们也不能使用它来规避其他策略.



## 存储类
本文描述了 Kubernetes 中 StorageClass 的概念.


** 介绍
StorageClass 为管理员提供了描述存储 "类" 的方法. 不同的类型可能会映射到不同的服务质量等级或备份策略,或是由集群管理员制定的任意策略. Kubernetes 本身并不清楚各种类代表的什么.这个类的概念在其他存储系统中有时被称为 "配置文件".


** StorageClass 资源
每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段, 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到.

StorageClass 对象的命名很重要,用户使用这个命名来请求生成一个特定的类. 当创建 StorageClass 对象时,管理员设置 StorageClass 对象的命名和其他参数,一旦创建了对象就不能再对其更新.

管理员可以为没有申请绑定到特定 StorageClass 的 PVC 指定一个默认的存储类

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - debug
volumeBindingMode: Immediate

* 存储制备器 
每个 StorageClass 都有一个制备器(Provisioner),用来决定使用哪个卷插件制备 PV. 该字段必须指定.

卷插件	                          内置制备器	配置例子
AWSElasticBlockStore	  ✓	                  AWS EBS
AzureFile                       	✓	                  Azure File
AzureDisk	                      ✓	                  Azure Disk
CephFS	                          -	                    -
Cinder	                            ✓	                  OpenStack Cinder
FC	                                  -	                    -
FlexVolume	                    -                     -
Flocker	                          ✓	                  -
GCEPersistentDisk	        ✓	                  GCE PD
Glusterfs	                        ✓	                  Glusterfs
iSCSI	                              -	                    -
Quobyte	                        ✓	                  Quobyte
NFS	                                -	                    NFS
RBD	                                ✓	                  Ceph RBD
VsphereVolume	            ✓	                  vSphere
PortworxVolume	          ✓	                  Portworx Volume
ScaleIO	                          ✓	                  ScaleIO
StorageOS	                      ✓	                  StorageOS
Local	                              -	                    Local

你不限于指定此处列出的 "内置" 制备器(其名称前缀为 "kubernetes.io" 并打包在 Kubernetes 中). 你还可以运行和指定外部制备器,这些独立的程序遵循由 Kubernetes 定义的 规范. 外部供应商的作者完全可以自由决定他们的代码保存于何处、打包方式、运行方式、使用的插件(包括 Flex)等. 代码仓库 kubernetes-sigs/sig-storage-lib-external-provisioner 包含一个用于为外部制备器编写功能实现的类库.你可以访问代码仓库 kubernetes-sigs/sig-storage-lib-external-provisioner 了解外部驱动列表.

kubernetes-sigs/sig-storage-lib-external-provisioner 外部驱动列表
https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner

* 回收策略
由 StorageClass 动态创建的 PersistentVolume 会在类的 reclaimPolicy 字段中指定回收策略,可以是 Delete 或者 Retain.如果 StorageClass 对象被创建时没有指定 reclaimPolicy,它将默认为 Delete.

通过 StorageClass 手动创建并管理的 PersistentVolume 会使用它们被创建时指定的回收政策.

* 允许卷扩展
特性状态: Kubernetes v1.11 [beta]

PersistentVolume 可以配置为可扩展.将此功能设置为 true 时,允许用户通过编辑相应的 PVC 对象来调整卷大小.

当下层 StorageClass 的 allowVolumeExpansion 字段设置为 true 时,以下类型的卷支持卷扩展.

卷类型	                        Kubernetes 版本要求
gcePersistentDisk	      1.11
awsElasticBlockStore	  1.11
Cinder	                          1.11
glusterfs	                      1.11
rbd	                                1.11
Azure File	                    1.11
Azure Disk	                    1.11
Portworx	                      1.11
FlexVolume	                  1.13
CSI	                                1.14 (alpha), 1.16 (beta)

说明: 此功能仅可用于扩容卷,不能用于缩小卷.

* 挂载选项
由 StorageClass 动态创建的 PersistentVolume 将使用类中 mountOptions 字段指定的挂载选项.

如果卷插件不支持挂载选项,却指定了挂载选项,则制备操作会失败. 挂载选项在 StorageClass 和 PV 上都不会做验证,如果其中一个挂载选项无效,那么这个 PV 挂载操作就会失败.

* 卷绑定模式 
volumeBindingMode 字段控制了卷绑定和动态制备 应该发生在什么时候.

默认情况下,Immediate 模式表示一旦创建了 PersistentVolumeClaim 也就完成了卷绑定和动态制备. 对于由于拓扑限制而非集群所有节点可达的存储后端,PersistentVolume 会在不知道 Pod 调度要求的情况下绑定或者制备.

集群管理员可以通过指定 WaitForFirstConsumer 模式来解决此问题. 该模式将延迟 PersistentVolume 的绑定和制备,直到使用该 PersistentVolumeClaim 的 Pod 被创建. PersistentVolume 会根据 Pod 调度约束指定的拓扑来选择或制备.这些包括但不限于 资源需求、 节点筛选器、 pod 亲和性和互斥性、 以及污点和容忍度.

以下插件支持动态供应的 WaitForFirstConsumer 模式:

  * AWSElasticBlockStore
  * GCEPersistentDisk
  * AzureDisk

以下插件支持预创建绑定 PersistentVolume 的 WaitForFirstConsumer 模式:

  * 上述全部
  * Local

特性状态: Kubernetes v1.17 [stable]

动态配置和预先创建的 PV 也支持 CSI卷, 但是你需要查看特定 CSI 驱动程序的文档以查看其支持的拓扑键名和例子.

说明:
如果你选择使用 WaitForFirstConsumer,请不要在 Pod 规约中使用 nodeName 来指定节点亲和性. 如果在这种情况下使用 nodeName,Pod 将会绕过调度程序,PVC 将停留在 pending 状态.

相反,在这种情况下,你可以使用节点选择器作为主机名,如下所示

apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:
  nodeSelector:
    kubernetes.io/hostname: kube-01
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage

* 允许的拓扑结构 
特性状态: Kubernetes v1.12 [beta]

当集群操作人员使用了 WaitForFirstConsumer 的卷绑定模式, 在大部分情况下就没有必要将制备限制为特定的拓扑结构. 然而,如果还有需要的话,可以使用 allowedTopologies.

这个例子描述了如何将供应卷的拓扑限制在特定的区域,在使用时应该根据插件 支持情况替换 zone 和 zones 参数.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
volumeBindingMode: WaitForFirstConsumer
allowedTopologies:
- matchLabelExpressions:
  - key: failure-domain.beta.kubernetes.io/zone
    values:
    - us-central-1a
    - us-central-1b


** 参数
Storage Classes 的参数描述了存储类的卷.取决于制备器,可以接受不同的参数. 例如,参数 type 的值 io1 和参数 iopsPerGB 特定于 EBS PV. 当参数被省略时,会使用默认值.

一个 StorageClass 最多可以定义 512 个参数.这些参数对象的总长度不能 超过 256 KiB, 包括参数的键和值.

详细的参数描述
https://kubernetes.io/zh/docs/concepts/storage/storage-classes/#%E5%8F%82%E6%95%B0

* AWS EBS 
* GCE PD
* Glusterfs
* NFS
* OpenStack Cinder
* vSphere
* Ceph RBD
* Quobyte
* Azure 磁盘
* Azure 文件
* Portworx 卷
* ScaleIO
* StorageOS
* 本地
特性状态: Kubernetes v1.14 [stable]
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

本地卷还不支持动态制备,然而还是需要创建 StorageClass 以延迟卷绑定, 直到完成 Pod 的调度.这是由 WaitForFirstConsumer 卷绑定模式指定的.

延迟卷绑定使得调度器在为 PersistentVolumeClaim 选择一个合适的 PersistentVolume 时能考虑到所有 Pod 的调度限制.



## 动态卷供应
动态卷供应允许按需创建存储卷. 如果没有动态供应,集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷, 然后在 Kubernetes 集群创建 PersistentVolume 对象来表示这些卷. 动态供应功能消除了集群管理员预先配置存储的需要. 相反,它在用户请求时自动供应存储.


** 背景
动态卷供应的实现基于 storage.k8s.io API 组中的 StorageClass API 对象. 集群管理员可以根据需要定义多个 StorageClass 对象,每个对象指定一个卷插件(又名 provisioner), 卷插件向卷供应商提供在创建卷时需要的数据卷信息及相关参数.

集群管理员可以在集群中定义和公开多种存储(来自相同或不同的存储系统),每种都具有自定义参数集. 该设计也确保终端用户不必担心存储供应的复杂性和细微差别,但仍然能够从多个存储选项中进行选择.


** 启用动态卷供应 
要启用动态供应功能,集群管理员需要为用户预先创建一个或多个 StorageClass 对象. StorageClass 对象定义当动态供应被调用时,哪一个驱动将被使用和哪些参数将被传递给驱动. 以下清单创建了一个 StorageClass 存储类 "slow",它提供类似标准磁盘的永久磁盘.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard

以下清单创建了一个 "fast" 存储类,它提供类似 SSD 的永久磁盘.

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd


** 使用动态卷供应
用户通过在 PersistentVolumeClaim 中包含存储类来请求动态供应的存储. 在 Kubernetes v1.9 之前,这通过 volume.beta.kubernetes.io/storage-class 注解实现.然而,这个注解自 v1.6 起就不被推荐使用了. 用户现在能够而且应该使用 PersistentVolumeClaim 对象的 storageClassName 字段. 这个字段的值必须能够匹配到集群管理员配置的 StorageClass 名称(见下面).

例如,要选择 "fast" 存储类,用户将创建如下的 PersistentVolumeClaim:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 30Gi

该声明会自动供应一块类似 SSD 的永久磁盘. 在删除该声明后,这个卷也会被销毁.


** 设置默认值的行为
可以在群集上启用动态卷供应,以便在未指定存储类的情况下动态设置所有声明. 集群管理员可以通过以下方式启用此行为:

  * 标记一个 StorageClass 为 默认;
  * 确保 DefaultStorageClass 准入控制器在 API 服务端被启用.

管理员可以通过向其添加 storageclass.kubernetes.io/is-default-class 注解来将特定的 StorageClass 标记为默认. 当集群中存在默认的 StorageClass 并且用户创建了一个未指定 storageClassName 的 PersistentVolumeClaim 时, DefaultStorageClass 准入控制器会自动向其中添加指向默认存储类的 storageClassName 字段.

请注意,群集上最多只能有一个 默认 存储类,否则无法创建没有明确指定 storageClassName 的 PersistentVolumeClaim.


** 拓扑感知
在多区域集群中,Pod 可以被分散到多个区域. 单区域存储后端应该被供应到 Pod 被调度到的区域. 这可以通过设置卷绑定模式来实现.



## 卷快照
特性状态: Kubernetes 1.17 [beta]

在 Kubernetes 中,卷快照是一个存储系统上卷的快照


** 介绍
与 PersistentVolume 和 PersistentVolumeClaim 两个 API 资源用于给用户和管理员提供卷类似,VolumeSnapshotContent 和 VolumeSnapshot 两个 API 资源用于给用户和管理员创建卷快照.

VolumeSnapshotContent 是一种快照,从管理员已提供的集群中的卷获取.就像持久卷是集群的资源一样,它也是集群中的资源.

VolumeSnapshot 是用户对于卷的快照的请求.它类似于持久卷声明.

VolumeSnapshotClass 允许指定属于 VolumeSnapshot 的不同属性.在从存储系统的相同卷上获取的快照之间,这些属性可能有所不同,因此不能通过使用与 PersistentVolumeClaim 相同的 StorageClass 来表示.

卷快照能力为 Kubernetes 用户提供了一种标准的方式来在指定时间点 复制卷的内容,并且不需要创建全新的卷.例如,这一功能使得数据库管理员 能够在执行编辑或删除之类的修改之前对数据库执行备份.

当使用该功能时,用户需要注意以下几点:

  * API 对象 VolumeSnapshot,VolumeSnapshotContent 和 VolumeSnapshotClass 是 CRDs, 不属于核心 API.
  * VolumeSnapshot 支持仅可用于 CSI 驱动.
  * 作为 VolumeSnapshot 部署过程的一部分,Kubernetes 团队提供了一个部署于控制平面的快照控制器, 并且提供了一个叫做 csi-snapshotter 的边车(Sidecar)辅助容器,和 CSI 驱动程序一起部署. 快照控制器监视 VolumeSnapshot 和 VolumeSnapshotContent 对象, 并且负责创建和删除 VolumeSnapshotContent 对象. 边车 csi-snapshotter 监视 VolumeSnapshotContent 对象, 并且触发针对 CSI 端点的 CreateSnapshot 和 DeleteSnapshot 的操作.
  * 还有一个验证性质的 Webhook 服务器,可以对快照对象进行更严格的验证. Kubernetes 发行版应将其与快照控制器和 CRD(而非 CSI 驱动程序)一起安装. 此服务器应该安装在所有启用了快照功能的 Kubernetes 集群中.
  * CSI 驱动可能实现,也可能没有实现卷快照功能.CSI 驱动可能会使用 csi-snapshotter 来提供对卷快照的支持.详见 CSI 驱动程序文档
  * Kubernetes 负责 CRDs 和快照控制器的安装.


** 卷快照和卷快照内容的生命周期 
详细
https://kubernetes.io/zh/docs/concepts/storage/volume-snapshots/

* 供应卷快照
* 绑定
* 快照源的持久性卷声明保护



## 卷快照类
详细
https://kubernetes.io/zh/docs/concepts/storage/volume-snapshot-classes/#introduction

** 介绍
就像 StorageClass 为管理员提供了一种在配置卷时描述存储"类"的方法, VolumeSnapshotClass 提供了一种在配置卷快照时描述存储"类"的方法.


** VolumeSnapshotClass 资源 
每个 VolumeSnapshotClass 都包含 driver、deletionPolicy 和 parameters 字段, 在需要动态配置属于该类的 VolumeSnapshot 时使用.

VolumeSnapshotClass 对象的名称很重要,是用户可以请求特定类的方式. 管理员在首次创建 VolumeSnapshotClass 对象时设置类的名称和其他参数, 对象一旦创建就无法更新.

说明: CRD 的安装是 Kubernetes 发行版的责任. 如果不存在所需的 CRD,则 VolumeSnapshotClass 的创建将失败.

* 驱动程序 

* 删除策略



## CSI 卷克隆
详细
https://kubernetes.io/zh/docs/concepts/storage/volume-pvc-datasource/#%E4%BB%8B%E7%BB%8D



## 存储容量
存储容量是有限的,并且会因为运行 Pod 的节点不同而变化: 网络存储可能并非所有节点都能够访问,或者对于某个节点存储是本地的.

特性状态: Kubernetes v1.21 [beta]

本页面描述了 Kubernetes 如何跟踪存储容量以及调度程序如何为了余下的尚未挂载的卷使用该信息将 Pod 调度到能够访问到足够存储容量的节点上. 如果没有跟踪存储容量,调度程序可能会选择一个没有足够容量来提供卷的节点,并且需要多次调度重试.

容器存储接口(CSI)驱动程序支持跟踪存储容量, 并且在安装 CSI 驱动程序时需要启用该功能.


** API
这个特性有两个 API 扩展接口:

  * CSIStorageCapacity 对象:这些对象由 CSI 驱动程序在安装驱动程序的命名空间中产生. 每个对象都包含一个存储类的容量信息,并定义哪些节点可以访问该存储.
  * CSIDriverSpec.StorageCapacity 字段: 设置为 true 时,Kubernetes 调度程序将考虑使用 CSI 驱动程序的卷的存储容量.


** 调度
如果有以下情况,存储容量信息将会被 Kubernetes 调度程序使用:

  * CSIStorageCapacity 特性门控被设置为 true,
  * Pod 使用的卷还没有被创建,
  * 卷使用引用了 CSI 驱动的 StorageClass, 并且使用了 WaitForFirstConsumer 卷绑定模式,
  * 驱动程序的 CSIDriver 对象的 StorageCapacity 被设置为 true.

在这种情况下,调度程序仅考虑将 Pod 调度到有足够存储容量的节点上.这个检测非常简单, 仅将卷的大小与 CSIStorageCapacity 对象中列出的容量进行比较,并使用包含该节点的拓扑.

对于具有 Immediate 卷绑定模式的卷,存储驱动程序将决定在何处创建该卷,而不取决于将使用该卷的 Pod. 然后,调度程序将 Pod 调度到创建卷后可使用该卷的节点上.

对于 CSI 临时卷,调度总是在不考虑存储容量的情况下进行. 这是基于这样的假设:该卷类型仅由节点本地的特殊 CSI 驱动程序使用,并且不需要大量资源.


** 重新调度
当为带有 WaitForFirstConsumer 的卷的 Pod 来选择节点时,该决定仍然是暂定的. 下一步是要求 CSI 存储驱动程序创建卷,并提示该卷在被选择的节点上可用.

因为 Kubernetes 可能会根据已经过时的存储容量信息来选择一个节点,因此可能无法真正创建卷. 然后就会重置节点选择,Kubernetes 调度器会再次尝试为 Pod 查找节点.

** 限制
存储容量跟踪增加了调度器第一次尝试即成功的机会,但是并不能保证这一点,因为调度器必须根据可能过期的信息来进行决策. 通常,与没有任何存储容量信息的调度相同的重试机制可以处理调度失败.

当 Pod 使用多个卷时,调度可能会永久失败:一个卷可能已经在拓扑段中创建,而该卷又没有足够的容量来创建另一个卷, 要想从中恢复,必须要进行手动干预,比如通过增加存储容量或者删除已经创建的卷. 需要进一步工作来自动处理此问题.

** 开启存储容量跟踪
存储容量跟踪是一个 Beta 特性,从 Kubernetes 1.21 版本起在 Kubernetes 集群 中默认被启用.除了在集群中启用此功能特性之外,还要求 CSI 驱动支持此特性.

## 卷健康监测
特性状态: Kubernetes v1.21 [alpha]
CSI 卷健康监测支持 CSI 驱动从底层的存储系统着手, 探测异常的卷状态,并以事件的形式上报到 PVCs 或 Pods.

卷健康监测
Kubernetes 卷健康监测 是 Kubernetes 容器存储接口(CSI)实现的一部分. 卷健康监测特性由两个组件实现:外部健康监测控制器和 kubelet.

如果 CSI 驱动器通过控制器的方式支持卷健康监测特性,那么只要在 CSI 卷上监测到异常卷状态,就会在 PersistentVolumeClaim (PVC) 中上报一个事件.

外部健康监测控制器也会监测节点失效事件. 如果要启动节点失效监测功能,你可以设置标志 enable-node-watcher 为 true. 当外部健康监测器检测到节点失效事件,控制器会报送一个事件,该事件会在 PVC 上继续上报, 以表明使用此 PVC 的 Pod 正位于一个失效的节点上.

如果 CSI 驱动程序支持节点测的卷健康检测,那当在 CSI 卷上检测到异常卷时, 会在使用该 PVC 的每个Pod 上触发一个事件.

说明: 你需要启用 CSIVolumeHealth 特性门控, 才能在节点上使用此特性.

## 特定于节点的卷数限制
此页面描述了各个云供应商可关联至一个节点的最大卷数.

谷歌、亚马逊和微软等云供应商通常对可以关联到节点的卷数量进行限制. Kubernetes 需要尊重这些限制. 否则,在节点上调度的 Pod 可能会卡住去等待卷的关联.

详细
https://kubernetes.io/zh/docs/concepts/storage/storage-limits/


-------------------
# 配置
## 配置最佳实践
本文档重点介绍并整合了整个用户指南、入门文档和示例中介绍的配置最佳实践.

这是一份不断改进的文件. 如果你认为某些内容缺失但可能对其他人有用,请不要犹豫,提交 Issue 或提交 PR.


** 一般配置提示 
  * 定义配置时,请指定最新的稳定 API 版本.
  * 在推送到集群之前,配置文件应存储在版本控制中. 这允许你在必要时快速回滚配置更改. 它还有助于集群重新创建和恢复.
  * 使用 YAML 而不是 JSON 编写配置文件.虽然这些格式几乎可以在所有场景中互换使用,但 YAML 往往更加用户友好.
  * 只要有意义,就将相关对象分组到一个文件中. 一个文件通常比几个文件更容易管理. 请参阅 guestbook-all-in-one.yaml 文件作为此语法的示例.
  * 另请注意,可以在目录上调用许多kubectl命令. 例如,你可以在配置文件的目录中调用kubectl apply.
  * 除非必要,否则不指定默认值:简单的最小配置会降低错误的可能性.
  * 将对象描述放在注释中,以便更好地进行内省.


** "Naked" Pods 与 ReplicaSet,Deployment 和 Jobs
  * 如果可能,不要使用独立的 Pods(即,未绑定到 ReplicaSet 或 Deployment 的 Pod). 如果节点发生故障,将不会重新调度独立的 Pods.

Deployment 既可以创建一个 ReplicaSet 来确保预期个数的 Pod 始终可用,也可以指定替换 Pod 的策略(例如 RollingUpdate). 除了一些显式的 restartPolicy: Never 场景外,Deployment 通常比直接创建 Pod 要好得多.Job 也可能是合适的选择.


** 服务 
  * 在创建相应的后端工作负载(Deployment 或 ReplicaSet),以及在需要访问它的任何工作负载之前创建 服务. 当 Kubernetes 启动容器时,它提供指向启动容器时正在运行的所有服务的环境变量. 例如,如果存在名为 foo 的服务,则所有容器将在其初始环境中获得以下变量.

  FOO_SERVICE_HOST=<the host the Service is running on>
  FOO_SERVICE_PORT=<the port the Service is running on>

这确实意味着在顺序上的要求 - 必须在 Pod 本身被创建之前创建 Pod 想要访问的任何 Service, 否则将环境变量不会生效.DNS 没有此限制.

  * 一个可选(尽管强烈推荐)的集群插件 是 DNS 服务器.DNS 服务器为新的 Services 监视 Kubernetes API,并为每个创建一组 DNS 记录. 如果在整个集群中启用了 DNS,则所有 Pods 应该能够自动对 Services 进行名称解析.
  * 除非绝对必要,否则不要为 Pod 指定 hostPort. 将 Pod 绑定到hostPort时,它会限制 Pod 可以调度的位置数,因为每个 <hostIP, hostPort, protocol>组合必须是唯一的. 如果你没有明确指定 hostIP 和 protocol,Kubernetes 将使用 0.0.0.0 作为默认 hostIP 和 TCP 作为默认 protocol.
如果你只需要访问端口以进行调试,则可以使用 apiserver proxy或 kubectl port-forward.

如果你明确需要在节点上公开 Pod 的端口,请在使用 hostPort 之前考虑使用 NodePort 服务.

  * 避免使用 hostNetwork,原因与 hostPort 相同.
  * 当你不需要 kube-proxy 负载均衡时,使用 无头服务
(ClusterIP 被设置为 None)以便于服务发现.


** 使用标签 
  * 定义并使用标签来识别应用程序 或 Deployment 的 语义属性,例如{ app: myapp, tier: frontend, phase: test, deployment: v3 }. 你可以使用这些标签为其他资源选择合适的 Pod; 例如,一个选择所有 tier: frontend Pod 的服务,或者 app: myapp 的所有 phase: test 组件. 有关此方法的示例,请参阅 guestbook .

通过从选择器中省略特定发行版的标签,可以使服务跨越多个 Deployment. 当你需要不停机的情况下更新正在运行的服务,可以使用Deployment.

Deployment 描述了对象的期望状态,并且如果对该规范的更改被成功应用, 则 Deployment 控制器以受控速率将实际状态改变为期望状态.

  * 对于常见场景,应使用 Kubernetes 通用标签. 这些标准化的标签丰富了对象的元数据,使得包括 kubectl 和 仪表板(Dashboard) 这些工具能够以可互操作的方式工作.
  * 你可以操纵标签进行调试. 由于 Kubernetes 控制器(例如 ReplicaSet)和服务使用选择器标签来匹配 Pod, 从 Pod 中删除相关标签将阻止其被控制器考虑或由服务提供服务流量. 如果删除现有 Pod 的标签,其控制器将创建一个新的 Pod 来取代它. 这是在"隔离"环境中调试先前"活跃"的 Pod 的有用方法. 要以交互方式删除或添加标签,请使用 kubectl label.


** 使用 kubectl 
  * 使用 kubectl apply -f <directory>. 它在 <directory> 中的所有 .yaml、.yml 和 .json 文件中查找 Kubernetes 配置,并将其传递给 apply.
  * 使用标签选择器进行 get 和 delete 操作,而不是特定的对象名称.
  * 请参阅标签选择器和 有效使用标签部分.
  * 使用kubectl run和kubectl expose来快速创建单容器部署和服务. 有关示例,请参阅使用服务访问集群中的应用程序.



## ConfigMap
ConfigMap 是一种 API 对象,用来将非机密性的数据保存到键值对中.使用时, Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件.

ConfigMap 将您的环境配置信息和 容器镜像 解耦,便于应用配置的修改.

注意:
ConfigMap 并不提供保密或者加密功能. 如果你想存储的数据是机密的,请使用 Secret, 或者使用其他第三方工具来保证你的数据的私密性,而不是用 ConfigMap.


** 动机 
使用 ConfigMap 来将你的配置数据和应用程序代码分开.

比如,假设你正在开发一个应用,它可以在你自己的电脑上(用于开发)和在云上 (用于实际流量)运行. 你的代码里有一段是用于查看环境变量 DATABASE_HOST,在本地运行时, 你将这个变量设置为 localhost,在云上,你将其设置为引用 Kubernetes 集群中的 公开数据库组件的 服务.

这让你可以获取在云中运行的容器镜像,并且如果有需要的话,在本地调试完全相同的代码.

ConfigMap 在设计上不是用来保存大量数据的.在 ConfigMap 中保存的数据不可超过 1 MiB.如果你需要保存超出此尺寸限制的数据,你可能希望考虑挂载存储卷 或者使用独立的数据库或者文件服务.


** ConfigMap 对象
ConfigMap 是一个 API 对象, 让你可以存储其他对象所需要使用的配置. 和其他 Kubernetes 对象都有一个 spec 不同的是,ConfigMap 使用 data 和 binaryData 字段.这些字段能够接收键-值对作为其取值.data 和 binaryData 字段都是可选的.data 字段设计用来保存 UTF-8 字符串,而 binaryData 则被设计用来保存二进制数据作为 base64 编码的字串.

ConfigMap 的名字必须是一个合法的 DNS 子域名.

data 或 binaryData 字段下面的每个键的名称都必须由字母数字字符或者 -、_ 或 . 组成.在 data 下保存的键名不可以与在 binaryData 下出现的键名有重叠.

从 v1.19 开始,你可以添加一个 immutable 字段到 ConfigMap 定义中, 创建不可变更的 ConfigMap.


** ConfigMaps 和 Pods
你可以写一个引用 ConfigMap 的 Pod 的 spec,并根据 ConfigMap 中的数据在该 Pod 中配置容器.这个 Pod 和 ConfigMap 必须要在同一个 名字空间 中.

说明: 静态 Pod 中的 spec 字段不能引用 ConfigMap 或任何其他 API 对象.

这是一个 ConfigMap 的示例,它的一些键只有一个值,其他键的值看起来像是 配置的片段格式.

apiVersion: v1
kind: ConfigMap
metadata:
  name: game-demo
data:
  # 类属性键;每一个键都映射到一个简单的值
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"

  # 类文件键
  game.properties: |
    enemy.types=aliens,monsters
    player.maximum-lives=5    
  user-interface.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true    

你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器:

  1. 在容器命令和参数内
  2. 容器的环境变量
  3. 在只读卷里面添加一个文件,让应用来读取
  4. 编写代码在 Pod 中运行,使用 Kubernetes API 来读取 ConfigMap

这些不同的方法适用于不同的数据使用方式. 对前三个方法,kubelet 使用 ConfigMap 中的数据在 Pod 中启动容器.

第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据.然而, 由于你是直接使用 Kubernetes API,因此只要 ConfigMap 发生更改, 你的应用就能够通过订阅来获取更新,并且在这样的情况发生的时候做出反应. 通过直接进入 Kubernetes API,这个技术也可以让你能够获取到不同的名字空间里的 ConfigMap.

下面是一个 Pod 的示例,它通过使用 game-demo 中的值来配置一个 Pod:

apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      env:
        # 定义环境变量
        - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的
          valueFrom:
            configMapKeyRef:
              name: game-demo           # 这个值来自 ConfigMap
              key: player_initial_lives # 需要取值的键
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: ui_properties_file_name
      volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
  volumes:
    # 你可以在 Pod 级别设置卷,然后将其挂载到 Pod 内的容器中
    - name: config
      configMap:
        # 提供你想要挂载的 ConfigMap 的名字
        name: game-demo
        # 来自 ConfigMap 的一组键,将被创建为文件
        items:
        - key: "game.properties"
          path: "game.properties"
        - key: "user-interface.properties"
          path: "user-interface.properties"

ConfigMap 不会区分单行属性值和多行类似文件的值,重要的是 Pods 和其他对象如何使用这些值.

上面的例子定义了一个卷并将它作为 /config 文件夹挂载到 demo 容器内, 创建两个文件,/config/game.properties 和 /config/user-interface.properties, 尽管 ConfigMap 中包含了四个键. 这是因为 Pod 定义中在 volumes 节指定了一个 items 数组. 如果你完全忽略 items 数组,则 ConfigMap 中的每个键都会变成一个与该键同名的文件, 因此你会得到四个文件.


** 使用 ConfigMap 
ConfigMap 可以作为数据卷挂载.ConfigMap 也可被系统的其他组件使用, 而不一定直接暴露给 Pod.例如,ConfigMap 可以保存系统中其他组件要使用的配置数据.

ConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置. 你也可以单独使用 ConfigMap.

比如,你可能会遇到基于 ConfigMap 来调整其行为的 插件 或者 operator.

* 在 Pod 中将 ConfigMap 当做文件使用
要在一个 Pod 的存储卷中使用 ConfigMap:

  1. 创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象.多个 Pod 可以引用同一个 ConfigMap.
  2. 修改 Pod 定义,在 spec.volumes[] 下添加一个卷. 为该卷设置任意名称,之后将 spec.volumes[].configMap.name 字段设置为对你的 ConfigMap 对象的引用.
  3. 为每个需要该 ConfigMap 的容器添加一个 .spec.containers[].volumeMounts[]. 设置 .spec.containers[].volumeMounts[].readOnly=true 并将 .spec.containers[].volumeMounts[].mountPath 设置为一个未使用的目录名, ConfigMap 的内容将出现在该目录中.
  4. 更改你的镜像或者命令行,以便程序能够从该目录中查找文件.ConfigMap 中的每个 data 键会变成 mountPath 下面的一个文件名.

下面是一个将 ConfigMap 以卷的形式进行挂载的 Pod 示例:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap

你希望使用的每个 ConfigMap 都需要在 spec.volumes 中被引用到.

如果 Pod 中有多个容器,则每个容器都需要自己的 volumeMounts 块,但针对每个 ConfigMap,你只需要设置一个 spec.volumes 块.

被挂载的 ConfigMap 内容会被自动更新
当卷中使用的 ConfigMap 被更新时,所投射的键最终也会被更新. kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新. 不过,kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值. 高速缓存的类型可以通过 KubeletConfiguration 结构. 的 ConfigMapAndSecretChangeDetectionStrategy 字段来配置.

ConfigMap 既可以通过 watch 操作实现内容传播(默认形式),也可实现基于 TTL 的缓存,还可以直接经过所有请求重定向到 API 服务器. 因此,从 ConfigMap 被更新的那一刻算起,到新的主键被投射到 Pod 中去, 这一时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等. 这里的传播延迟取决于所选的高速缓存类型 (分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0).

以环境变量方式使用的 ConfigMap 数据不会被自动更新. 更新这些数据需要重新启动 Pod.

说明: 使用 ConfigMap 作为 subPath 卷挂载的容器将不会收到 ConfigMap 的更新.


** 不可变更的 ConfigMap 
特性状态: Kubernetes v1.21 [stable]

Kubernetes 特性 Immutable Secret 和 ConfigMaps 提供了一种将各个 Secret 和 ConfigMap 设置为不可变更的选项.对于大量使用 ConfigMap 的集群 (至少有数万个各不相同的 ConfigMap 给 Pod 挂载)而言,禁止更改 ConfigMap 的数据有以下好处:

  * 保护应用,使之免受意外(不想要的)更新所带来的负面影响.
  * 通过大幅降低对 kube-apiserver 的压力提升集群性能, 这是因为系统会关闭对已标记为不可变更的 ConfigMap 的监视操作.

此功能特性由 ImmutableEphemeralVolumes 特性门控来控制. 你可以通过将 immutable 字段设置为 true 创建不可变更的 ConfigMap. 例如:

apiVersion: v1
kind: ConfigMap
metadata:
  ...
data:
  ...
immutable: true

一旦某 ConfigMap 被标记为不可变更,则 无法 逆转这一变化,,也无法更改 data 或 binaryData 字段的内容.你只能删除并重建 ConfigMap. 因为现有的 Pod 会维护一个已被删除的 ConfigMap 的挂载点,建议重新创建这些 Pods.



## Secret
Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象. 这样的信息可能会被放在 Pod 规约中或者镜像中. 使用 Secret 意味着你不需要在应用程序代码中包含机密数据.

由于创建 Secret 可以独立于使用它们的 Pod, 因此在创建、查看和编辑 Pod 的工作流程中暴露 Secret(及其数据)的风险较小. Kubernetes 和在集群中运行的应用程序也可以对 Secret 采取额外的预防措施, 例如避免将机密数据写入非易失性存储.

Secret 类似于 ConfigMap 但专门用于保存机密数据.

注意:
默认情况下,Kubernetes Secret 未加密地存储在 API 服务器的底层数据存储(etcd)中. 任何拥有 API 访问权限的人都可以检索或修改 Secret,任何有权访问 etcd 的人也可以. 此外,任何有权限在命名空间中创建 Pod 的人都可以使用该访问权限读取该命名空间中的任何 Secret; 这包括间接访问,例如创建 Deployment 的能力.

为了安全地使用 Secret,请至少执行以下步骤:

为 Secret 启用静态加密;
启用或配置 RBAC 规则来限制读取和写入 Secret 的数据(包括通过间接方式).需要注意的是,被准许创建 Pod 的人也隐式地被授权获取 Secret 内容.
在适当的情况下,还可以使用 RBAC 等机制来限制允许哪些主体创建新 Secret 或替换现有 Secret.


** Secret 的使用
Pod 可以用三种方式之一来使用 Secret:

作为挂载到一个或多个容器上的卷 中的文件.
作为容器的环境变量.
由 kubelet 在为 Pod 拉取镜像时使用.
Kubernetes 控制面也使用 Secret; 例如,引导令牌 Secret 是一种帮助自动化节点注册的机制.

* Secret 的替代方案 
除了使用 Secret 来保护机密数据,你也可以选择一些替代方案.

下面是一些选项:

  * 如果你的云原生组件需要执行身份认证来访问你所知道的、在同一 Kubernetes 集群中运行的另一个应用, 你可以使用 ServiceAccount 及其令牌来标识你的客户端身份.
  * 你可以运行的第三方工具也有很多,这些工具可以运行在集群内或集群外,提供机密数据管理. 例如,这一工具可能是 Pod 通过 HTTPS 访问的一个服务,该服务在客户端能够正确地通过身份认证 (例如,通过 ServiceAccount 令牌)时,提供机密数据内容.
  * 就身份认证而言,你可以为 X.509 证书实现一个定制的签名者,并使用 CertificateSigningRequest 来让该签名者为需要证书的 Pod 发放证书.
你可以使用一个设备插件 来将节点本地的加密硬件暴露给特定的 Pod.例如,你可以将可信任的 Pod 调度到提供可信平台模块(Trusted Platform Module,TPM)的节点上. 这类节点是另行配置的.
  * 你还可以将如上选项的两种或多种进行组合,包括直接使用 Secret 对象本身也是一种选项.

例如:实现(或部署)一个 operator, 从外部服务取回生命期很短的会话令牌,之后基于这些生命期很短的会话令牌来创建 Secret. 运行在集群中的 Pod 可以使用这些会话令牌,而 Operator 则确保这些令牌是合法的. 这种责权分离意味着你可以运行那些不了解会话令牌如何发放与刷新的确切机制的 Pod.


** 使用 Secret 

* 创建 Secret 
  * 使用 kubectl 命令来创建 Secret
  * 基于配置文件来创建 Secret
  * 使用 kustomize 来创建 Secret

对 Secret 名称与数据的约束
Secret 对象的名称必须是合法的 DNS 子域名.

在为创建 Secret 编写配置文件时,你可以设置 data 与/或 stringData 字段. data 和 stringData 字段都是可选的.data 字段中所有键值都必须是 base64 编码的字符串.如果不希望执行这种 base64 字符串的转换操作,你可以选择设置 stringData 字段,其中可以使用任何字符串作为其取值.

data 和 stringData 中的键名只能包含字母、数字、-、_ 或 . 字符. stringData 字段中的所有键值对都会在内部被合并到 data 字段中. 如果某个主键同时出现在 data 和 stringData 字段中,stringData 所指定的键值具有高优先级.

尺寸限制 
每个 Secret 的尺寸最多为 1MiB.施加这一限制是为了避免用户创建非常大的 Secret, 进而导致 API 服务器和 kubelet 内存耗尽.不过创建很多小的 Secret 也可能耗尽内存. 你可以使用资源配额来约束每个名字空间中 Secret(或其他资源)的个数.

* 编辑 Secret 
你可以使用 kubectl 来编辑一个已有的 Secret:

kubectl edit secrets mysecret
这一命令会启动你的默认编辑器,允许你更新 data 字段中存放的 base64 编码的 Secret 值; 例如:

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file, it will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
kind: Secret
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: { ... }
  creationTimestamp: 2020-01-22T18:41:56Z
  name: mysecret
  namespace: default
  resourceVersion: "164619"
  uid: cfee02d6-c137-11e5-8d73-42010af00002
type: Opaque

这一示例清单定义了一个 Secret,其 data 字段中包含两个主键:username 和 password. 清单中的字段值是 Base64 字符串,不过,当你在 Pod 中使用 Secret 时,kubelet 为 Pod 及其中的容器提供的是解码后的数据.

你可以在一个 Secret 中打包多个主键和数值,也可以选择使用多个 Secret, 完全取决于哪种方式最方便.

* 使用 Secret 
Secret 可以以数据卷的形式挂载,也可以作为环境变量 暴露给 Pod 中的容器使用.Secret 也可用于系统中的其他部分,而不是一定要直接暴露给 Pod. 例如,Secret 也可以包含系统中其他部分在替你与外部系统交互时要使用的凭证数据.

Kubernetes 会检查 Secret 的卷数据源,确保所指定的对象引用确实指向类型为 Secret 的对象.因此,如果 Pod 依赖于某 Secret,该 Secret 必须先于 Pod 被创建.

如果 Secret 内容无法取回(可能因为 Secret 尚不存在或者临时性地出现 API 服务器网络连接问题),kubelet 会周期性地重试 Pod 运行操作.kubelet 也会为该 Pod 报告 Event 事件,给出读取 Secret 时遇到的问题细节.

可选的 Secret 
当你定义一个基于 Secret 的环境变量时,你可以将其标记为可选. 默认情况下,所引用的 Secret 都是必需的.

只有所有非可选的 Secret 都可用时,Pod 中的容器才能启动运行.

如果 Pod 引用了 Secret 中的特定主键,而虽然 Secret 本身存在,对应的主键不存在, Pod 启动也会失败.

* 在 Pod 中以文件形式使用 Secret 
如果你希望在 Pod 中访问 Secret 内的数据,一种方式是让 Kubernetes 将 Secret 以 Pod 中一个或多个容器的文件系统中的文件的形式呈现出来.

要配置这种行为,你需要:

  1. 创建一个 Secret 或者使用已有的 Secret.多个 Pod 可以引用同一个 Secret.
  2. 更改 Pod 定义,在 .spec.volumes[] 下添加一个卷.根据需要为卷设置其名称, 并将 .spec.volumes[].secret.secretName 字段设置为 Secret 对象的名称.
  3. 为每个需要该 Secret 的容器添加 .spec.containers[].volumeMounts[]. 并将 .spec.containers[].volumeMounts[].readyOnly 设置为 true, 将 .spec.containers[].volumeMounts[].mountPath 设置为希望 Secret 被放置的、目前尚未被使用的路径名.
  4. 更改你的镜像或命令行,以便程序读取所设置的目录下的文件.Secret 的 data 映射中的每个主键都成为 mountPath 下面的文件名.

下面是一个通过卷来挂载名为 mysecret 的 Secret 的 Pod 示例:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      optional: false # 默认设置,意味着 "mysecret" 必须已经存在

你要访问的每个 Secret 都需要通过 .spec.volumes 来引用.

如果 Pod 中包含多个容器,则每个容器需要自己的 volumeMounts 块, 不过针对每个 Secret 而言,只需要一份 .spec.volumes 设置.

说明:
Kubernetes v1.22 版本之前都会自动创建用来访问 Kubernetes API 的凭证. 这一老的机制是基于创建可被挂载到 Pod 中的令牌 Secret 来实现的. 在最近的版本中,包括 Kubernetes v1.23 中,API 凭据是直接通过 TokenRequest API 来获得的,这一凭据会使用投射卷 挂载到 Pod 中.使用这种方式获得的令牌有确定的生命期,并且在挂载它们的 Pod 被删除时自动作废.

你仍然可以手动创建 服务账号令牌.例如,当你需要一个永远都不过期的令牌时. 不过,仍然建议使用 TokenRequest 子资源来获得访问 API 服务器的令牌.

将 Secret 键投射到特定目录
你也可以控制 Secret 键所投射到的卷中的路径. 你可以使用 .spec.volumes[].secret.items 字段来更改每个主键的目标路径:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      items:
      - key: username
        path: my-group/my-username

将发生的事情如下:

  * mysecret 中的键 username 会出现在容器中的路径为 /etc/foo/my-group/my-username, 而不是 /etc/foo/username.
  * Secret 对象的 password 键不会被投射.

如果使用了 .spec.volumes[].secret.items,则只有 items 中指定了的主键会被投射. 如果要使用 Secret 中的所有主键,则需要将它们全部枚举到 items 字段中.

如果你显式地列举了主键,则所列举的主键都必须在对应的 Secret 中存在. 否则所在的卷不会被创建.

Secret 文件的访问权限
你可以为某个 Secret 主键设置 POSIX 文件访问权限位. 如果你不指定访问权限,默认会使用 0644. 你也可以为整个 Secret 卷设置默认的访问模式,然后再根据需要在主键层面重载.

例如,你可以像下面这样设置默认的模式:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
  volumes:
  - name: foo
    secret:
      secretName: mysecret
      defaultMode: 0400

该 Secret 被挂载在 /etc/foo 下,Secret 卷挂载所创建的所有文件的访问模式都是 0400.

说明:
如果你是使用 JSON 来定义 Pod 或 Pod 模板,需要注意 JSON 规范不支持八进制的记数方式. 你可以在 defaultMode 中设置十进制的值(例如,八进制中的 0400 在十进制中为 256). 如果你使用 YAML 来编写定义,你可以用八进制值来设置 defaultMode.

使用来自卷中的 Secret 值 
在挂载了 Secret 卷的容器内,Secret 的主键都呈现为文件. Secret 的取值都是 Base64 编码的,保存在这些文件中.

下面是在上例中的容器内执行命令的结果:

ls /etc/foo/

输出类似于:

username
password

cat /etc/foo/username

输出类似于:

admin

cat /etc/foo/password

输出类似于:

1f2d1e2e67df

容器中的程序要负责根据需要读取 Secret 数据.

挂载的 Secret 是被自动更新的 
当卷中包含来自 Secret 的数据,而对应的 Secret 被更新,Kubernetes 会跟踪到这一操作并更新卷中的数据.更新的方式是保证最终一致性.

说明:
对于以 subPath 形式挂载 Secret 卷的容器而言, 它们无法收到自动的 Secret 更新.

Kubelet 组件会维护一个缓存,在其中保存节点上 Pod 卷中使用的 Secret 的当前主键和取值. 你可以配置 kubelet 如何检测所缓存数值的变化. kubelet 配置中的 configMapAndSecretChangeDetectionStrategy 字段控制 kubelet 所采用的策略. 默认的策略是 Watch.

对 Secret 的更新操作既可以通过 API 的 watch 机制(默认)来传播, 基于设置了生命期的缓存获取,也可以通过 kubelet 的同步回路来从集群的 API 服务器上轮询获取.

因此,从 Secret 被更新到新的主键被投射到 Pod 中,中间存在一个延迟. 这一延迟的上限是 kubelet 的同步周期加上缓存的传播延迟, 其中缓存的传播延迟取决于所选择的缓存类型. 对应上一段中提到的几种传播机制,延迟时长为 watch 的传播延迟、所配置的缓存 TTL 或者对于直接轮询而言是零.

* 以环境变量的方式使用 Secret 
如果需要在 Pod 中以环境变量 的形式使用 Secret:

  1. 创建 Secret(或者使用现有 Secret).多个 Pod 可以引用同一个 Secret.
  2. 更改 Pod 定义,在要使用 Secret 键值的每个容器中添加与所使用的主键对应的环境变量. 读取 Secret 主键的环境变量应该在 env[].valueFrom.secretKeyRef 中填写 Secret 的名称和主键名称.
  3. 更改你的镜像或命令行,以便程序读取环境变量中保存的值.

下面是一个通过环境变量来使用 Secret 的示例 Pod:

apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: mycontainer
    image: redis
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: username
            optional: false # 此值为默认值;意味着 "mysecret"
                            # 必须存在且包含名为 "username" 的主键
      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: password
            optional: false # 此值为默认值;意味着 "mysecret"
                            # 必须存在且包含名为 "password" 的主键
  restartPolicy: Never

非法环境变量 
对于通过 envFrom 字段来填充环境变量的 Secret 而言, 如果其中包含的主键不能被当做合法的环境变量名,这些主键会被忽略掉. Pod 仍然可以启动.

如果你定义的 Pod 中包含非法的变量名称,则 Pod 可能启动失败, 会形成 reason 为 InvalidVariableNames 的事件,以及列举被略过的非法主键的消息. 下面的例子中展示了一个 Pod,引用的是名为 mysecret 的 Secret, 其中包含两个非法的主键:1badkey 和 2alsobad.

kubectl get events

输出类似于:

LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.

通过环境变量使用 Secret 值
在通过环境变量来使用 Secret 的容器中,Secret 主键展现为普通的环境变量. 这些变量的取值是 Secret 数据的 Base64 解码值.

下面是在前文示例中的容器内执行命令的结果:

echo "$SECRET_USERNAME"

输出类似于:

admin

echo "$SECRET_PASSWORD"

输出类似于:

1f2d1e2e67df

说明:
如果容器已经在通过环境变量来使用 Secret,Secret 更新在容器内是看不到的, 除非容器被重启.有一些第三方的解决方案,能够在 Secret 发生变化时触发容器重启.

* 容器镜像拉取 Secret 
如果你尝试从私有仓库拉取容器镜像,你需要一种方式让每个节点上的 kubelet 能够完成与镜像库的身份认证.你可以配置 镜像拉取 Secret 来实现这点. Secret 是在 Pod 层面来配置的.

Pod 的 imagePullSecrets 字段是一个对 Pod 所在的名字空间中的 Secret 的引用列表.你可以使用 imagePullSecrets 来将镜像仓库访问凭据传递给 kubelet. kubelet 使用这个信息来替你的 Pod 拉取私有镜像. 参阅 Pod API 参考 中的 PodSpec 进一步了解 imagePullSecrets 字段.

使用 imagePullSecrets
imagePullSecrets 字段是一个列表,包含对同一名字空间中 Secret 的引用. 你可以使用 imagePullSecrets 将包含 Docker(或其他)镜像仓库密码的 Secret 传递给 kubelet.kubelet 使用此信息来替 Pod 拉取私有镜像. 参阅 PodSpec API 进一步了解 imagePullSecrets 字段.

手动设定 imagePullSecret
你可以通过阅读容器镜像 文档了解如何设置 imagePullSecrets.

设置 imagePullSecrets 为自动挂载
你可以手动创建 imagePullSecret,并在一个 ServiceAccount 中引用它. 对使用该 ServiceAccount 创建的所有 Pod,或者默认使用该 ServiceAccount 创建的 Pod 而言,其 imagePullSecrets 字段都会设置为该服务账号. 请阅读向服务账号添加 ImagePullSecrets 来详细了解这一过程.

在静态 Pod 中使用 Secret 
你不可以在静态 Pod. 中使用 ConfigMap 或 Secret.


** 使用场景 

* 使用场景:作为容器环境变量

创建 Secret:

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  USER_NAME: YWRtaW4=
  PASSWORD: MWYyZDFlMmU2N2Rm
创建 Secret:

kubectl apply -f mysecret.yaml

使用 envFrom 来将 Secret 的所有数据定义为容器的环境变量. 来自 Secret 的主键成为 Pod 中的环境变量名称:

apiVersion: v1
kind: Pod
metadata:
  name: secret-test-pod
spec:
  containers:
    - name: test-container
      image: k8s.gcr.io/busybox
      command: [ "/bin/sh", "-c", "env" ]
      envFrom:
      - secretRef:
          name: mysecret
  restartPolicy: Never

* 使用场景:带 SSH 密钥的 Pod

创建包含一些 SSH 密钥的 Secret:

kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=/path/to/.ssh/id_rsa --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub

输出类似于:

secret "ssh-key-secret" created

你也可以创建一个 kustomization.yaml 文件,在其 secretGenerator 字段中包含 SSH 密钥.

注意:
在提供你自己的 SSH 密钥之前要仔细思考:集群的其他用户可能有权访问该 Secret.

你也可以创建一个 SSH 私钥,代表一个你希望与你共享 Kubernetes 集群的其他用户分享的服务标识. 当凭据信息被泄露时,你可以收回该访问权限.

现在你可以创建一个 Pod,在其中访问包含 SSH 密钥的 Secret,并通过卷的方式来使用它:

apiVersion: v1
kind: Pod
metadata:
  name: secret-test-pod
  labels:
    name: secret-test
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: ssh-key-secret
  containers:
  - name: ssh-test-container
    image: mySshImage
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"

容器命令执行时,秘钥的数据可以在下面的位置访问到:

/etc/secret-volume/ssh-publickey
/etc/secret-volume/ssh-privatekey

容器就可以随便使用 Secret 数据来建立 SSH 连接.

* 使用场景:带有生产、测试环境凭据的 Pod 
这一示例所展示的一个 Pod 会使用包含生产环境凭据的 Secret,另一个 Pod 使用包含测试环境凭据的 Secret.

你可以创建一个带有 secretGenerator 字段的 kustomization.yaml 文件或者运行 kubectl create secret 来创建 Secret.

kubectl create secret generic prod-db-secret --from-literal=username=produser --from-literal=password=Y4nys7f11

输出类似于:

secret "prod-db-secret" created

你也可以创建一个包含测试环境凭据的 Secret:

kubectl create secret generic test-db-secret --from-literal=username=testuser --from-literal=password=iluvtests

输出类似于:

secret "test-db-secret" created

说明:
特殊字符(例如 $、\、*、= 和 !)会被你的 Shell解释,因此需要转义.

在大多数 Shell 中,对密码进行转义的最简单方式是用单引号(')将其括起来. 例如,如果你的实际密码是 S!B\*d$zDsb,则应通过以下方式执行命令:

kubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\*d$zDsb='

你无需对文件中的密码(--from-file)中的特殊字符进行转义.

现在生成 Pod:

cat <<EOF > pod.yaml
apiVersion: v1
kind: List
items:
- kind: Pod
  apiVersion: v1
  metadata:
    name: prod-db-client-pod
    labels:
      name: prod-db-client
  spec:
    volumes:
    - name: secret-volume
      secret:
        secretName: prod-db-secret
    containers:
    - name: db-client-container
      image: myClientImage
      volumeMounts:
      - name: secret-volume
        readOnly: true
        mountPath: "/etc/secret-volume"
- kind: Pod
  apiVersion: v1
  metadata:
    name: test-db-client-pod
    labels:
      name: test-db-client
  spec:
    volumes:
    - name: secret-volume
      secret:
        secretName: test-db-secret
    containers:
    - name: db-client-container
      image: myClientImage
      volumeMounts:
      - name: secret-volume
        readOnly: true
        mountPath: "/etc/secret-volume"
EOF

将 Pod 添加到同一 kustomization.yaml 文件中:

cat <<EOF >> kustomization.yaml
resources:
- pod.yaml
EOF

通过下面的命令在 API 服务器上应用所有这些对象:

kubectl apply -k .

两个文件都会在其文件系统中出现下面面的文件,文件中内容是各个容器的环境值:

/etc/secret-volume/username
/etc/secret-volume/password

注意这两个 Pod 的规约中只有一个字段不同. 这便于基于相同的 Pod 模板生成具有不同能力的 Pod.

你可以通过使用两个服务账号来进一步简化这一基本的 Pod 规约:

prod-user 服务账号使用 prod-db-secret
test-user 服务账号使用 test-db-secret

Pod 规约简化为:

apiVersion: v1
kind: Pod
metadata:
  name: prod-db-client-pod
  labels:
    name: prod-db-client
spec:
  serviceAccount: prod-db-client
  containers:
  - name: db-client-container
    image: myClientImage

* 使用场景:在 Secret 卷中带句点的文件 
通过定义以句点(.)开头的主键,你可以"隐藏"你的数据. 这些主键代表的是以句点开头的文件或"隐藏"文件. 例如,当下面的 Secret 被挂载到 secret-volume 卷中时:

apiVersion: v1
kind: Secret
metadata:
  name: dotfile-secret
data:
  .secret-file: dmFsdWUtMg0KDQo=
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-dotfiles-pod
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: dotfile-secret
  containers:
  - name: dotfile-test-container
    image: k8s.gcr.io/busybox
    command:
    - ls
    - "-l"
    - "/etc/secret-volume"
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"

卷中会包含一个名为 .secret-file 的文件,并且容器 dotfile-test-container 中此文件位于路径 /etc/secret-volume/.secret-file 处.

说明:
以句点开头的文件会在 ls -l 的输出中被隐藏起来; 列举目录内容时你必须使用 ls -la 才能看到它们.

* 使用场景:仅对 Pod 中一个容器可见的 Secret
考虑一个需要处理 HTTP 请求,执行某些复杂的业务逻辑,之后使用 HMAC 来对某些消息进行签名的程序.因为这一程序的应用逻辑很复杂, 其中可能包含未被注意到的远程服务器文件读取漏洞, 这种漏洞可能会把私钥暴露给攻击者.

这一程序可以分隔成两个容器中的两个进程:前端容器要处理用户交互和业务逻辑, 但无法看到私钥;签名容器可以看到私钥,并对来自前端的简单签名请求作出响应 (例如,通过本地主机网络).

采用这种划分的方法,攻击者现在必须欺骗应用服务器来做一些其他操作, 而这些操作可能要比读取一个文件要复杂很多.


** Secret 的类型 
创建 Secret 时,你可以使用 Secret 资源的 type 字段,或者与其等价的 kubectl 命令行参数(如果有的话)为其设置类型. Secret 类型有助于对 Secret 数据进行编程处理.

Kubernetes 提供若干种内置的类型,用于一些常见的使用场景. 针对这些类型,Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同.

内置类型	                                                用法
Opaque	                                                    用户定义的任意数据
kubernetes.io/service-account-token	    服务账号令牌
kubernetes.io/dockercfg	~/.dockercfg   文件的序列化形式
kubernetes.io/dockerconfigjson	            ~/.docker/config.json 文件的序列化形式
kubernetes.io/basic-auth	                        用于基本身份认证的凭据
kubernetes.io/ssh-auth	                          用于 SSH 身份认证的凭据
kubernetes.io/tls	                                    用于 TLS 客户端或者服务器端的数据
bootstrap.kubernetes.io/token	              启动引导令牌数据

通过为 Secret 对象的 type 字段设置一个非空的字符串值,你也可以定义并使用自己 Secret 类型.如果 type 值为空字符串,则被视为 Opaque 类型.

Kubernetes 并不对类型的名称作任何限制.不过,如果你要使用内置类型之一, 则你必须满足为该类型所定义的所有要求.

如果你要定义一种公开使用的 Secret 类型,请遵守 Secret 类型的约定和结构, 在类型名签名添加域名,并用 / 隔开. 例如:cloud-hosting.example.net/cloud-api-credentials.

* Opaque Secret
当 Secret 配置文件中未作显式设定时,默认的 Secret 类型是 Opaque. 当你使用 kubectl 来创建一个 Secret 时,你会使用 generic 子命令来标明 要创建的是一个 Opaque 类型 Secret. 例如,下面的命令会创建一个空的 Opaque 类型 Secret 对象:

kubectl create secret generic empty-secret
kubectl get secret empty-secret

输出类似于

NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s

DATA 列显示 Secret 中保存的数据条目个数. 在这个例子种,0 意味着我们刚刚创建了一个空的 Secret.

* 服务账号令牌 Secret 
类型为 kubernetes.io/service-account-token 的 Secret 用来存放标识某 服务账号的令牌. 使用这种 Secret 类型时,你需要确保对象的注解 kubernetes.io/service-account-name 被设置为某个已有的服务账号名称.某个 Kubernetes 控制器会填写 Secret 的其它字段,例如 kubernetes.io/service-account.uid 注解以及 data 字段中的 token 键值,使之包含实际的令牌内容.

下面的配置实例声明了一个服务账号令牌 Secret:

apiVersion: v1
kind: Secret
metadata:
  name: secret-sa-sample
  annotations:
    kubernetes.io/service-account.name: "sa-name"
type: kubernetes.io/service-account-token
data:
  # 你可以像 Opaque Secret 一样在这里添加额外的键/值偶对
  extra: YmFyCg==

Kubernetes 在创建 Pod 时会自动创建一个服务账号 Secret 并自动修改你的 Pod 以使用该 Secret.该服务账号令牌 Secret 中包含了访问 Kubernetes API 所需要的凭据.

如果需要,可以禁止或者重载这种自动创建并使用 API 凭据的操作. 不过,如果你仅仅是希望能够安全地访问 API 服务器,这是建议的工作方式.

参考 ServiceAccount 文档了解服务账号的工作原理.你也可以查看 Pod 资源中的 automountServiceAccountToken 和 serviceAccountName 字段文档, 进一步了解从 Pod 中引用服务账号.

* Docker 配置 Secret 
你可以使用下面两种 type 值之一来创建 Secret,用以存放访问 Docker 仓库 来下载镜像的凭据.

  * kubernetes.io/dockercfg
  * kubernetes.io/dockerconfigjson

kubernetes.io/dockercfg 是一种保留类型,用来存放 ~/.dockercfg 文件的序列化形式. 该文件是配置 Docker 命令行的一种老旧形式.使用此 Secret 类型时,你需要确保 Secret 的 data 字段中包含名为 .dockercfg 的主键,其对应键值是用 base64 编码的某 ~/.dockercfg 文件的内容.

类型 kubernetes.io/dockerconfigjson 被设计用来保存 JSON 数据的序列化形式, 该 JSON 也遵从 ~/.docker/config.json 文件的格式规则,而后者是 ~/.dockercfg 的新版本格式.使用此 Secret 类型时,Secret 对象的 data 字段必须包含 .dockerconfigjson 键,其键值为 base64 编码的字符串包含 ~/.docker/config.json 文件的内容.

下面是一个 kubernetes.io/dockercfg 类型 Secret 的示例:

apiVersion: v1
kind: Secret
metadata:
  name: secret-dockercfg
type: kubernetes.io/dockercfg
data:
  .dockercfg: |
        "<base64 encoded ~/.dockercfg file>"

Note:
如果你不希望执行 base64 编码转换,可以使用 stringData 字段代替.

当你使用清单文件来创建这两类 Secret 时,API 服务器会检查 data 字段中是否 存在所期望的主键,并且验证其中所提供的键值是否是合法的 JSON 数据. 不过,API 服务器不会检查 JSON 数据本身是否是一个合法的 Docker 配置文件内容.

kubectl create secret docker-registry secret-tiger-docker \
  --docker-email=tiger@acme.example \
  --docker-username=tiger \
  --docker-password=pass113 \
  --docker-server=my-registry.example:5000

上面的命令创建一个类型为 kubernetes.io/dockerconfigjson 的 Secret. 如果你对 .data.dockerconfigjson 内容进行转储并执行 base64 解码:

{
  "auths": {
    "my-registry.example:5000": {
      "username": "tiger",
      "password": "pass113",
      "email": "tiger@acme.com",
      "auth": "dGlnZXI6cGFzczExMw=="
    }
  }
}

Note:
auths 值是 base64 编码的,其内容被屏蔽但未被加密. 任何能够读取该 Secret 的人都可以了解镜像库的访问令牌.

基本身份认证 Secret 
kubernetes.io/basic-auth 类型用来存放用于基本身份认证所需的凭据信息. 使用这种 Secret 类型时,Secret 的 data 字段必须包含以下两个键:

  * username: 用于身份认证的用户名;
  * password: 用于身份认证的密码或令牌.

以上两个键的键值都是 base64 编码的字符串. 当然你也可以在创建 Secret 时使用 stringData 字段来提供明文形式的内容. 下面的 YAML 是基本身份认证 Secret 的一个示例清单:

apiVersion: v1
kind: Secret
metadata:
  name: secret-basic-auth
type: kubernetes.io/basic-auth
stringData:
  username: admin      #  kubernetes.io/basic-auth 类型的必需字段
  password: t0p-Secret # kubernetes.io/basic-auth 类型的必需字段

提供基本身份认证类型的 Secret 仅仅是出于方便性考虑. 你也可以使用 Opaque 类型来保存用于基本身份认证的凭据. 不过,使用预定义的、公开的 Secret 类型(kubernetes.io/basic-auth) 有助于帮助其他用户理解 Secret 的目的,并且对其中存在的主键形成一种约定. API 服务器会检查 Secret 配置中是否提供了所需要的主键.

* SSH 身份认证 Secret
Kubernetes 所提供的内置类型 kubernetes.io/ssh-auth 用来存放 SSH 身份认证中 所需要的凭据.使用这种 Secret 类型时,你就必须在其 data (或 stringData) 字段中提供一个 ssh-privatekey 键值对,作为要使用的 SSH 凭据.

下面的清单是一个 SSH 公钥/私钥身份认证的 Secret 示例:

apiVersion: v1
kind: Secret
metadata:
  name: secret-ssh-auth
type: kubernetes.io/ssh-auth
data:
  # 此例中的实际数据被截断
  ssh-privatekey: |
          MIIEpQIBAAKCAQEAulqb/Y ...

提供 SSH 身份认证类型的 Secret 仅仅是出于用户方便性考虑. 你也可以使用 Opaque 类型来保存用于 SSH 身份认证的凭据. 不过,使用预定义的、公开的 Secret 类型(kubernetes.io/ssh-auth) 有助于其他人理解你的 Secret 的用途,也可以就其中包含的主键名形成约定. API 服务器确实会检查 Secret 配置中是否提供了所需要的主键.

Caution:
SSH 私钥自身无法建立 SSH 客户端与服务器端之间的可信连接. 需要其它方式来建立这种信任关系,以缓解"中间人(Man In The Middle)" 攻击,例如向 ConfigMap 中添加一个 known_hosts 文件.

* TLS Secret
Kubernetes 提供一种内置的 kubernetes.io/tls Secret 类型,用来存放 TLS 场合通常要使用的证书及其相关密钥. TLS Secret 的一种典型用法是为 Ingress 资源配置传输过程中的数据加密,不过也可以用于其他资源或者直接在负载中使用. 当使用此类型的 Secret 时,Secret 配置中的 data (或 stringData)字段必须包含 tls.key 和 tls.crt 主键,尽管 API 服务器实际上并不会对每个键的取值作进一步的合法性检查.

下面的 YAML 包含一个 TLS Secret 的配置示例:

apiVersion: v1
kind: Secret
metadata:
  name: secret-tls
type: kubernetes.io/tls
data:
  # 此例中的数据被截断
  tls.crt: |
        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...
  tls.key: |
        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...

提供 TLS 类型的 Secret 仅仅是出于用户方便性考虑. 你也可以使用 Opaque 类型来保存用于 TLS 服务器与/或客户端的凭据. 不过,使用内置的 Secret 类型的有助于对凭据格式进行归一化处理,并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键.

当使用 kubectl 来创建 TLS Secret 时,你可以像下面的例子一样使用 tls 子命令:

kubectl create secret tls my-tls-secret \
  --cert=path/to/cert/file \
  --key=path/to/key/file

这里的公钥/私钥对都必须事先已存在.用于 --cert 的公钥证书必须是 RFC 7468 中 5.1 节 中所规定的 DER 格式,且与 --key 所给定的私钥匹配. 私钥必须是 DER 格式的 PKCS #8 (参见 RFC 7468 第 11节).

Note:
类型为 kubernetes.io/tls 的 Secret 中包含密钥和证书的 DER 数据,以 Base64 格式编码. 如果你熟悉私钥和证书的 PEM 格式,base64 与该格式相同,只是你需要略过 PEM 数据中所包含的第一行和最后一行.

例如,对于证书而言,你 不要 包含 --------BEGIN CERTIFICATE----- 和 -------END CERTIFICATE---- 这两行.

* 启动引导令牌 Secret 
通过将 Secret 的 type 设置为 bootstrap.kubernetes.io/token 可以创建 启动引导令牌类型的 Secret.这种类型的 Secret 被设计用来支持节点的启动引导过程. 其中包含用来为周知的 ConfigMap 签名的令牌.

启动引导令牌 Secret 通常创建于 kube-system 名字空间内,并以 bootstrap-token-<令牌 ID> 的形式命名;其中 <令牌 ID> 是一个由 6 个字符组成 的字符串,用作令牌的标识.

以 Kubernetes 清单文件的形式,某启动引导令牌 Secret 可能看起来像下面这样:

apiVersion: v1
kind: Secret
metadata:
  name: bootstrap-token-5emitj
  namespace: kube-system
type: bootstrap.kubernetes.io/token
data:
  auth-extra-groups: c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=
  expiration: MjAyMC0wOS0xM1QwNDozOToxMFo=
  token-id: NWVtaXRq
  token-secret: a3E0Z2lodnN6emduMXAwcg==
  usage-bootstrap-authentication: dHJ1ZQ==
  usage-bootstrap-signing: dHJ1ZQ==

启动引导令牌类型的 Secret 会在 data 字段中包含如下主键:

  * token-id:由 6 个随机字符组成的字符串,作为令牌的标识符.必需.
  * token-secret:由 16 个随机字符组成的字符串,包含实际的令牌机密.必需.
  * description:供用户阅读的字符串,描述令牌的用途.可选.
  * expiration:一个使用 RFC3339 来编码的 UTC 绝对时间,给出令牌要过期的时间.可选.
  * usage-bootstrap-<usage>:布尔类型的标志,用来标明启动引导令牌的其他用途.
  * auth-extra-groups:用逗号分隔的组名列表,身份认证时除被认证为 system:bootstrappers 组之外,还会被添加到所列的用户组中.

上面的 YAML 文件可能看起来令人费解,因为其中的数值均为 base64 编码的字符串. 实际上,你完全可以使用下面的 YAML 来创建一个一模一样的 Secret:

apiVersion: v1
kind: Secret
metadata:
  # 注意 Secret 的命名方式
  name: bootstrap-token-5emitj
  # 启动引导令牌 Secret 通常位于 kube-system 名字空间
  namespace: kube-system
type: bootstrap.kubernetes.io/token
stringData:
  auth-extra-groups: "system:bootstrappers:kubeadm:default-node-token"
  expiration: "2020-09-13T04:39:10Z"
  # 此令牌 ID 被用于生成 Secret 名称
  token-id: "5emitj"
  token-secret: "kq4gihvszzgn1p0r"
  # 此令牌还可用于 authentication (身份认证)
  usage-bootstrap-authentication: "true"
  # 且可用于 signing (证书签名)
  usage-bootstrap-signing: "true"


** 不可更改的 Secret
FEATURE STATE: Kubernetes v1.21 [stable]

Kubernetes 允许你将特定的 Secret(和 ConfigMap)标记为 不可更改(Immutable). 禁止更改现有 Secret 的数据有下列好处:

  * 防止意外(或非预期的)更新导致应用程序中断
  *(对于大量使用 Secret 的集群而言,至少数万个不同的 Secret 供 Pod 挂载), 通过将 Secret 标记为不可变,可以极大降低 kube-apiserver 的负载,提升集群性能. kubelet 不需要监视那些被标记为不可更改的 Secret.

* 将 Secret 标记为不可更改 

你可以通过将 Secret 的 immutable 字段设置为 true 创建不可更改的 Secret. 例如:

apiVersion: v1
kind: Secret
metadata:
  ...
data:
  ...
immutable: true

你也可以更改现有的 Secret,令其不可更改.

Note:
一旦一个 Secret 或 ConfigMap 被标记为不可更改,撤销此操作或者更改 data 字段的内容都是 不 可能的. 只能删除并重新创建这个 Secret.现有的 Pod 将维持对已删除 Secret 的挂载点 -- 建议重新创建这些 Pod.


** Secret 的信息安全问题
尽管 ConfigMap 和 Secret 的工作方式类似,但 Kubernetes 对 Secret 有一些额外的保护.

Secret 通常保存重要性各异的数值,其中很多都可能会导致 Kubernetes 中 (例如,服务账号令牌)或对外部系统的特权提升. 即使某些个别应用能够推导它期望使用的 Secret 的能力, 同一名字空间中的其他应用可能会让这种假定不成立.

只有当某个节点上的 Pod 需要某 Secret 时,对应的 Secret 才会被发送到该节点上. 如果将 Secret 挂载到 Pod 中,kubelet 会将数据的副本保存在在 tmpfs 中, 这样机密的数据不会被写入到持久性存储中. 一旦依赖于该 Secret 的 Pod 被删除,kubelet 会删除来自于该 Secret 的机密数据的本地副本.

同一个 Pod 中可能包含多个容器.默认情况下,你所定义的容器只能访问默认 ServiceAccount 及其相关 Secret.你必须显式地定义环境变量或者将卷映射到容器中,才能为容器提供对其他 Secret 的访问.

针对同一节点上的多个 Pod 可能有多个 Secret.不过,只有某个 Pod 所请求的 Secret 才有可能对 Pod 中的容器可见.因此,一个 Pod 不会获得访问其他 Pod 的 Secret 的权限.

Warning:
节点上的所有特权容器都可能访问到该节点上使用的所有 Secret.

* 针对开发人员的安全性建议
  * 应用在从环境变量或卷中读取了机密信息内容之后仍要对其进行保护.例如, 你的应用应该避免用明文的方式将 Secret 数据写入日志,或者将其传递给不可信的第三方.
  * 如果你在一个 Pod 中定义了多个容器,而只有一个容器需要访问某 Secret, 定义卷挂载或环境变量配置时,应确保其他容器无法访问该 Secret.
  * 如果你通过清单来配置某 Secret, Secret 数据以 Base64 的形式编码,将此文件共享,或者将其检入到某源码仓库, 都意味着 Secret 对于任何可以读取清单的人都是可见的. Base64 编码 不是 一种加密方法,与明文相比没有任何安全性提升.
部署与 Secret API 交互的应用时,你应该使用 RBAC 这类鉴权策略来限制访问.
  * 在 Kubernetes API 中,名字空间内对 Secret 对象的 watch 和 list 请求是非常强大的能力. 在可能的时候应该避免授予这类访问权限,因为通过列举 Secret, 客户端能够查看对应名字空间内所有 Secret 的取值.

* 针对集群管理员的安全性建议

Caution:
能够创建使用 Secret 的 Pod 的用户也可以查看该 Secret 的取值. 即使集群策略不允许某用户直接读取 Secret 对象,这一用户仍然可以通过运行一个 Pod 来访问 Secret 的内容.

  * 保留(使用 Kubernetes API)对集群中所有 Secret 对象执行 watch 或 list 操作的能力, 这样只有特权级最高、系统级别的组件能够执行这类操作.
  * 在部署需要通过 Secret API 交互的应用时,你应该通过使用 RBAC 这类鉴权策略来限制访问.
在 API 服务器上,对象(包括 Secret)会被持久化到 etcd 中; 因此:

  * 只应准许集群管理员访问 etcd(包括只读访问);
  * 为 Secret 对象启用静态加密, 这样这些 Secret 的数据就不会以明文的形式保存到 etcd 中;
  * 当 etcd 的持久化存储不再被使用时,请考虑彻底擦除存储介质;
  * 如果存在多个 etcd 实例,请确保 etcd 使用 SSL/TLS 来完成其对等通信.



## 为 Pod 和容器管理资源
当你定义 Pod 时可以选择性地为每个 容器设定所需要的资源数量. 最常见的可设定资源是 CPU 和内存(RAM)大小；此外还有其他类型的资源.

当你为 Pod 中的 Container 指定了资源 请求 时, kube-scheduler 就利用该信息决定将 Pod 调度到哪个节点上. 当你还为 Container 指定了资源 约束 时,kubelet 就可以确保运行的容器不会使用超出所设约束的资源. kubelet 还会为容器预留所 请求 数量的系统资源,供其使用.


** 请求和约束 
如果 Pod 运行所在的节点具有足够的可用资源,容器可能(且可以)使用超出对应资源 request 属性所设置的资源量.不过,容器不可以使用超出其资源 limit 属性所设置的资源量.

例如,如果你将容器的 memory 的请求量设置为 256 MiB,而该容器所处的 Pod 被调度到一个具有 8 GiB 内存的节点上,并且该节点上没有其他 Pods 运行,那么该容器就可以尝试使用更多的内存.

如果你将某容器的 memory 约束设置为 4 GiB,kubelet (和 容器运行时) 就会确保该约束生效. 容器运行时会禁止容器使用超出所设置资源约束的资源. 例如:当容器中进程尝试使用超出所允许内存量的资源时,系统内核会将尝试申请内存的进程终止, 并引发内存不足(OOM)错误.

约束值可以以被动方式来实现(系统会在发现违例时进行干预),或者通过强制生效的方式实现 (系统会避免容器用量超出约束值).不同的容器运行时采用不同方式来实现相同的限制.

Note:
如果某容器设置了自己的内存限制但未设置内存请求,Kubernetes 自动为其设置与内存限制相匹配的请求值.类似的,如果某 Container 设置了 CPU 限制值但未设置 CPU 请求值,则 Kubernetes 自动为其设置 CPU 请求并使之与 CPU 限制值匹配.


** 资源类型 
CPU 和 内存 都是 资源类型.每种资源类型具有其基本单位. CPU 表达的是计算处理能力,其单位是 Kubernetes CPUs. 内存的单位是字节. 对于 Linux 负载,则可以指定巨页(Huge Page)资源. 巨页是 Linux 特有的功能,节点内核在其中分配的内存块比默认页大小大得多.

例如,在默认页面大小为 4KiB 的系统上,你可以指定约束 hugepages-2Mi: 80Mi. 如果容器尝试分配 40 个 2MiB 大小的巨页(总共 80 MiB ),则分配请求会失败.

Note:
你不能过量使用 hugepages- * 资源. 这与 memory 和 cpu 资源不同.

CPU 和内存统称为"计算资源",或简称为"资源". 计算资源的数量是可测量的,可以被请求、被分配、被消耗. 它们与 API 资源 不同. API 资源(如 Pod 和 Service)是可通过 Kubernetes API 服务器读取和修改的对象.


** Pod 和 容器的资源请求和约束
针对每个容器,你都可以指定其资源约束和请求,包括如下选项:

  * spec.containers[].resources.limits.cpu
  * spec.containers[].resources.limits.memory
  * spec.containers[].resources.limits.hugepages-<size>
  * spec.containers[].resources.requests.cpu
  * spec.containers[].resources.requests.memory
  * spec.containers[].resources.requests.hugepages-<size>

尽管你只能逐个容器地指定请求和限制值,考虑 Pod 的总体资源请求和约束也是有用的. 对特定资源而言,Pod 的资源请求/约束值是 Pod 中各容器对该类型资源的请求/约束值的总和.


** Kubernetes 中的资源单位 

* CPU 资源单位 
CPU 资源的约束和请求以 "cpu" 为单位. 在 Kubernetes 中,一个 CPU 等于1 个物理 CPU 核 或者 一个虚拟核, 取决于节点是一台物理主机还是运行在某物理主机上的虚拟机.

你也可以表达带小数 CPU 的请求. 当你定义一个容器,将其 spec.containers[].resources.requests.cpu 设置为 0.5 时, 你所请求的 CPU 是你请求 1.0 CPU 时的一半. 对于 CPU 资源单位,数量 表达式 0.1 等价于表达式 100m,可以看作 "100 millicpu". 有些人说成是"一百毫核",其实说的是同样的事情.

CPU 资源总是设置为资源的绝对数量而非相对数量值. 例如,无论容器运行在单核、双核或者 48-核的机器上,500m CPU 表示的是大约相同的计算能力.

Note:
Kubernetes 不允许设置精度小于 1m 的 CPU 资源. 因此,当 CPU 单位小于 1 或 1000m 时,使用毫核的形式是有用的； 例如 5m 而不是 0.005.


** 内存资源单位 
memory 的约束和请求以字节为单位. 你可以使用普通的证书,或者带有以下 数量后缀 的定点数字来表示内存:E、P、T、G、M、k. 你也可以使用对应的 2 的幂数:Ei、Pi、Ti、Gi、Mi、Ki. 例如,以下表达式所代表的是大致相同的值:

128974848、129e6、129M、128974848000m、123Mi
请注意后缀的大小写.如果你请求 400m 内存,实际上请求的是 0.4 字节. 如果有人这样设定资源请求或限制,可能他的实际想法是申请 400 兆字节(400Mi) 或者 400M 字节.


** 容器资源示例 
以下 Pod 有两个容器.每个容器的请求为 0.25 CPU 和 64MiB(226 字节)内存, 每个容器的资源约束为 0.5 CPU 和 128MiB 内存. 你可以认为该 Pod 的资源请求为 0.5 CPU 和 128 MiB 内存,资源限制为 1 CPU 和 256MiB 内存.

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"


** 带资源请求的 Pod 如何调度
当你创建一个 Pod 时,Kubernetes 调度程序将为 Pod 选择一个节点. 每个节点对每种资源类型都有一个容量上限:可为 Pod 提供的 CPU 和内存量. 调度程序确保对于每种资源类型,所调度的容器的资源请求的总和小于节点的容量. 请注意,尽管节点上的实际内存或 CPU 资源使用量非常低,如果容量检查失败, 调度程序仍会拒绝在该节点上放置 Pod. 当稍后节点上资源用量增加,例如到达请求率的每日峰值区间时,节点上也不会出现资源不足的问题.


** Kubernetes 应用资源请求与约束的方式
当 kubelet 启动 Pod 中的容器时,它会将容器的 CPU 和内存请求与约束信息传递给容器运行时.

在 Linux 系统上,容器运行时通常会配置内核 CGroups,负责应用并实施所定义的请求.

CPU 约束值定义的是容器可使用的 CPU 时间的硬性上限. 在每个调度周期(时间片)期间,Linux 内核检查是否已经超出该约束值； 内核会在允许该 cgroup 恢复执行之前会等待.
CPU 请求值定义的是一个权重值.如果若干不同的容器(CGroups)需要在一个共享的系统上竞争运行, CPU 请求值大的负载会获得比请求值小的负载更多的 CPU 时间.
内存请求值主要用于(Kubernetes)Pod 调度期间.在一个启用了 CGroup v2 的节点上, 容器运行时可能会使用内存请求值作为设置 memory.min 和 memory.low 的提示值.
内存约束值定义的是 CGroup 的内存约束.如果容器尝试分配的内存量超出约束值, 则 Linux 内核的内存不足处理子系统会被激活,并停止尝试分配内存的容器中的某个进程. 如果该进程在容器中 PID 为 1,而容器被标记为可重新启动,则 Kubernetes 会重新启动该容器.
Pod 或容器的内存约束值也适用于通过内存供应的卷,例如 emptyDir 卷. kubelet 会跟踪 tmpfs 形式的 emptyDir 卷用量,将其作为容器的内存用量, 而不是临时存储用量.
如果某容器内存用量超过其内存请求值并且所在节点内存不足时,容器所处的 Pod 可能被逐出.

每个容器可能被允许也可能不被允许使用超过其 CPU 约束的处理时间. 但是,容器运行时不会由于 CPU 使用率过高而杀死 Pod 或容器.


** 监控计算和内存资源用量 
kubelet 会将 Pod 的资源使用情况作为 Pod status 的一部分来报告的.

如果为集群配置了可选的监控工具, 则可以直接从指标 API 或者监控工具获得 Pod 的资源使用情况.


** 本地临时存储 
FEATURE STATE: Kubernetes v1.10 [beta]
节点通常还可以具有本地的临时性存储,由本地挂接的可写入设备或者有时也用 RAM 来提供支持. "临时(Ephemeral)"意味着对所存储的数据不提供长期可用性的保证.

Pods 通常可以使用临时性本地存储来实现缓冲区、保存日志等功能. kubelet 可以为使用本地临时存储的 Pods 提供这种存储空间,允许后者使用 emptyDir 类型的 卷将其挂载到容器中.

kubelet 也使用此类存储来保存 节点层面的容器日志, 容器镜像文件、以及运行中容器的可写入层.

Caution: 如果节点失效,存储在临时性存储中的数据会丢失. 你的应用不能对本地临时性存储的性能 SLA(例如磁盘 IOPS)作任何假定.
作为一种 beta 阶段功能特性,Kubernetes 允许你跟踪、预留和限制 Pod 可消耗的临时性本地存储数量.

* 本地临时性存储的配置 
Kubernetes 有两种方式支持节点上配置本地临时性存储:

单一文件系统
采用这种配置时,你会把所有类型的临时性本地数据(包括 emptyDir 卷、可写入容器层、容器镜像、日志等)放到同一个文件系统中. 作为最有效的 kubelet 配置方式,这意味着该文件系统是专门提供给 Kubernetes (kubelet)来保存数据的.

kubelet 也会生成 节点层面的容器日志, 并按临时性本地存储的方式对待之.

kubelet 会将日志写入到所配置的日志目录(默认为 /var/log)下的文件中； 还会针对其他本地存储的数据使用同一个基础目录(默认为 /var/lib/kubelet).

通常,/var/lib/kubelet 和 /var/log 都是在系统的根文件系统中.kubelet 的设计也考虑到这一点.

你的集群节点当然可以包含其他的、并非用于 Kubernetes 的很多文件系统.

双文件系统
你使用节点上的某个文件系统来保存运行 Pods 时产生的临时性数据:日志和 emptyDir 卷等.你可以使用这个文件系统来保存其他数据(例如:与 Kubernetes 无关的其他系统日志)；这个文件系统还可以是根文件系统.

kubelet 也将 节点层面的容器日志 写入到第一个文件系统中,并按临时性本地存储的方式对待之.

同时你使用另一个由不同逻辑存储设备支持的文件系统.在这种配置下,你会告诉 kubelet 将容器镜像层和可写层保存到这第二个文件系统上的某个目录中.

第一个文件系统中不包含任何镜像层和可写层数据.

当然,你的集群节点上还可以有很多其他与 Kubernetes 没有关联的文件系统.

kubelet 能够度量其本地存储的用量.实现度量机制的前提是:

LocalStorageCapacityIsolation 特性门控 被启用(默认状态),并且
  * 你已经对节点进行了配置,使之使用所支持的本地临时性储存配置方式之一
  * 如果你的节点配置不同于以上预期,kubelet 就无法对临时性本地存储的资源约束实施限制.

Note: kubelet 会将 tmpfs emptyDir 卷的用量当作容器内存用量,而不是本地临时性存储来统计.

*为本地临时性存储设置请求和约束值
你可以使用 ephemeral-storage 来管理本地临时性存储. Pod 中的每个容器可以设置以下属性:

spec.containers[].resources.limits.ephemeral-storage
spec.containers[].resources.requests.ephemeral-storage
ephemeral-storage 的请求和约束值是按量纲计量的.你可以使用一般整数或者定点数字 加上下面的后缀来表达存储量:E、P、T、G、M、K. 你也可以使用对应的 2 的幂级数来表达:Ei、Pi、Ti、Gi、Mi、Ki. 例如,下面的表达式所表达的大致是同一个值:

128974848
129e6
129M
123Mi
在下面的例子中,Pod 包含两个容器.每个容器请求 2 GiB 大小的本地临时性存储. 每个容器都设置了 4 GiB 作为其本地临时性存储的约束值. 因此,整个 Pod 的本地临时性存储请求是 4 GiB,且其本地临时性存储的约束为 8 GiB.

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  - name: log-aggregator
    image: images.my-company.example/log-aggregator:v6
    resources:
      requests:
        ephemeral-storage: "2Gi"
      limits:
        ephemeral-storage: "4Gi"
    volumeMounts:
    - name: ephemeral
      mountPath: "/tmp"
  volumes:
    - name: ephemeral
      emptyDir: {}

* 带临时性存储的 Pods 的调度行为
当你创建一个 Pod 时,Kubernetes 调度器会为 Pod 选择一个节点来运行之. 每个节点都有一个本地临时性存储的上限,是其可提供给 Pods 使用的总量. 

调度器会确保所调度的容器的资源请求总和不会超出节点的资源容量.

* 临时性存储消耗的管理
如果 kubelet 将本地临时性存储作为资源来管理,则 kubelet 会度量以下各处的存储用量:

emptyDir 卷,除了 tmpfs emptyDir 卷
保存节点层面日志的目录
可写入的容器镜像层
如果某 Pod 的临时存储用量超出了你所允许的范围,kubelet 会向其发出逐出(eviction)信号,触发该 Pod 被逐出所在节点.

就容器层面的隔离而言,如果某容器的可写入镜像层和日志用量超出其存储约束, kubelet 也会将所在的 Pod 标记为逐出候选.

就 Pod 层面的隔离而言,kubelet 会将 Pod 中所有容器的约束值相加,得到 Pod 存储约束的总值.如果所有容器的本地临时性存储用量总和加上 Pod 的 emptyDir 卷的用量超出 Pod 存储约束值,kubelet 也会将该 Pod 标记为逐出候选.

Caution:
如果 kubelet 没有度量本地临时性存储的用量,即使 Pod 的本地存储用量超出其约束值也不会被逐出.

不过,如果用于可写入容器镜像层、节点层面日志或者 emptyDir 卷的文件系统中可用空间太少, 节点会为自身设置本地存储不足的污点 标签. 这一污点会触发对那些无法容忍该污点的 Pods 的逐出操作.

kubelet 支持使用不同方式来度量 Pod 的存储用量:

周期性扫描
kubelet 按预定周期执行扫描操作,检查 emptyDir 卷、容器日志目录以及可写入容器镜像层.

这一扫描会度量存储空间用量.

Note:
在这种模式下,kubelet 并不检查已删除文件所对应的、仍处于打开状态的文件描述符.

如果你(或者容器)在 emptyDir 卷中创建了一个文件,写入一些内容之后再次打开 该文件并执行了删除操作,所删除文件对应的 inode 仍然存在,直到你关闭该文件为止. kubelet 不会将该文件所占用的空间视为已使用空间.

文件系统项目配额
FEATURE STATE: Kubernetes v1.15 [alpha]

项目配额(Project Quota)是一个操作系统层的功能特性,用来管理文件系统中的存储用量. 在 Kubernetes 中,你可以启用项目配额以监视存储用量. 你需要确保节点上为 emptyDir 提供存储的文件系统支持项目配额. 例如,XFS 和 ext4fs 文件系统都支持项目配额.

Note: 项目配额可以帮你监视存储用量,但无法对存储约束执行限制.
Kubernetes 所使用的项目 ID 始于 1048576. 所使用的 IDs 会注册在 /etc/projects 和 /etc/projid 文件中. 如果该范围中的项目 ID 已经在系统中被用于其他目的,则已占用的项目 IDs 也必须注册到 /etc/projects 和 /etc/projid 中,这样 Kubernetes 才不会使用它们.

配额方式与目录扫描方式相比速度更快,结果更精确.当某个目录被分配给某个项目时, 该目录下所创建的所有文件都属于该项目,内核只需要跟踪该项目中的文件所使用的存储块个数. 如果某文件被创建后又被删除,但对应文件描述符仍处于打开状态, 该文件会继续耗用存储空间.配额跟踪技术能够精确第记录对应存储空间的状态, 而目录扫描方式会忽略被删除文件所占用的空间.

如果你希望使用项目配额,你需要:

  * 在 kubelet 配置中使用 featureGates 字段 或者使用 --feature-gates 命令行参数 启用 LocalStorageCapacityIsolationFSQuotaMonitoring=true 特性门控 .
  * 确保根文件系统(或者可选的运行时文件系统)启用了项目配额.所有 XFS 文件系统都支持项目配额. 对 extf 文件系统而言,你需要在文件系统尚未被挂载时启用项目配额跟踪特性:

    # 对 ext4 而言,在 /dev/block-device 尚未被挂载时执行下面操作
    sudo tune2fs -O project -Q prjquota /dev/block-device

  * 确保根文件系统(或者可选的运行时文件系统)在挂载时项目配额特性是被启用了的. 对于 XFS 和 ext4fs 而言,对应的挂载选项称作 prjquota.


** 扩展资源(Extended Resources) 
扩展资源是 kubernetes.io 域名之外的标准资源名称. 它们使得集群管理员能够颁布非 Kubernetes 内置资源,而用户可以使用他们.

使用扩展资源需要两个步骤.首先,集群管理员必须颁布扩展资源. 其次,用户必须在 Pod 中请求扩展资源.

具体内容:
https://kubernetes.io/zh/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run


** PID 限制 
进程 ID(PID)限制允许对 kubelet 进行配置,以限制给定 Pod 可以消耗的 PID 数量.


** 疑难解答

* 我的 Pod 处于悬决状态且事件信息显示 FailedScheduling
如果调度器找不到该 Pod 可以匹配的任何节点,则该 Pod 将保持未被调度状态, 直到找到一个可以被调度到的位置.每当调度器找不到 Pod 可以调度的地方时, 会产生一个 Event. 你可以使用 kubectl 来查看 Pod 的事件；例如:

kubectl describe pod frontend | grep -A 9999999999 Events
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu

在上述示例中,由于节点上的 CPU 资源不足,名为 "frontend" 的 Pod 无法被调度. 由于内存不足(PodExceedsFreeMemory)而导致失败时,也有类似的错误消息. 一般来说,如果 Pod 处于悬决状态且有这种类型的消息时,你可以尝试如下几件事情:

  * 向集群添加更多节点.
  * 终止不需要的 Pod,为悬决的 Pod 腾出空间.
  * 检查 Pod 所需的资源是否超出所有节点的资源容量.例如,如果所有节点的容量都是cpu:1, 那么一个请求为 cpu: 1.1 的 Pod 永远不会被调度.
  * 检查节点上的污点设置.如果集群中节点上存在污点,而新的 Pod 不能容忍污点, 调度器只会考虑将 Pod 调度到不带有该污点的节点上.

你可以使用 kubectl describe nodes 命令检查节点容量和已分配的资源数量. 例如:

kubectl describe nodes e2e-test-node-pool-4lw4

Name:            e2e-test-node-pool-4lw4
[ ... 这里忽略了若干行以便阅读 ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... 这里忽略了若干行以便阅读 ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (12%)        1070Mi (14%)

在上面的输出中,你可以看到如果 Pod 请求超过 1120m CPU 或者 6.23Gi 内存,节点将无法满足.

通过查看 "Pods" 部分,你将看到哪些 Pod 占用了节点上的资源.

Pods 可用的资源量低于节点的资源总量,因为系统守护进程也会使用一部分可用资源. 在 Kubernetes API 中,每个 Node 都有一个 .status.allocatable 字段 .

字段 .status.allocatable 描述节点上可以用于 Pod 的资源总量(例如:15 个虚拟 CPU、7538 MiB 内存).

你可以配置资源配额功能特性以限制每个名字空间可以使用的资源总量. 当某名字空间中存在 ResourceQuota 时,Kubernetes 会在该名字空间中的对象强制实施配额. 例如,如果你为不同的团队分配名字空间,你可以为这些名字空间添加 ResourceQuota. 设置资源配额有助于防止一个团队占用太多资源,以至于这种占用会影响其他团队.

你还需要考虑为这些名字空间设置授权访问: 为名字空间提供 全部 的写权限时,具有合适权限的人可能删除所有资源, 包括所配置的 ResourceQuota.

* 我的容器被终止了
你的容器可能因为资源紧张而被终止.要查看容器是否因为遇到资源限制而被杀死, 请针对相关的 Pod 执行 kubectl describe pod:

kubectl describe pod simmemleak-hra99
输出类似于:

Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image "saadali/simmemleak:latest" already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod

在上面的例子中,Restart Count: 5 意味着 Pod 中的 simmemleak 容器被终止并且(到目前为止)重启了五次. 原因 OOMKilled 显示容器尝试使用超出其限制的内存量.

你接下来要做的或许是检查应用代码,看看是否存在内存泄露. 如果你发现应用的行为与你所预期的相同,则可以考虑为该容器设置一个更高的内存约束 (也可能需要设置请求值).



## 使用 kubeconfig 文件组织集群访问
使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息.kubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息,并与集群的 API 服务器进行通信.

Note: 用于配置集群访问的文件称为 kubeconfig 文件.这是引用配置文件的通用方法.这并不意味着有一个名为 kubeconfig 的文件
Warning: 只使用来源可靠的 kubeconfig 文件.使用特制的 kubeconfig 文件可能会导致恶意代码执行或文件暴露. 如果必须使用不受信任的 kubeconfig 文件,请首先像检查 shell 脚本一样仔细检查它.
默认情况下,kubectl 在 $HOME/.kube 目录下查找名为 config 的文件. 您可以通过设置 KUBECONFIG 环境变量或者设置 --kubeconfig参数来指定其他 kubeconfig 文件.


** 支持多集群、用户和身份认证机制
假设您有多个集群,并且您的用户和组件以多种方式进行身份认证.比如:

正在运行的 kubelet 可能使用证书在进行认证.
用户可能通过令牌进行认证.
管理员可能拥有多个证书集合提供给各用户.
使用 kubeconfig 文件,您可以组织集群、用户和命名空间.您还可以定义上下文,以便在集群和命名空间之间快速轻松地切换.


** 上下文(Context)
通过 kubeconfig 文件中的 context 元素,使用简便的名称来对访问参数进行分组.每个上下文都有三个参数:cluster、namespace 和 user.默认情况下,kubectl 命令行工具使用 当前上下文 中的参数与集群进行通信.

选择当前上下文

kubectl config use-context


** KUBECONFIG 环境变量
KUBECONFIG 环境变量包含一个 kubeconfig 文件列表. 对于 Linux 和 Mac,列表以冒号分隔.对于 Windows,列表以分号分隔. KUBECONFIG 环境变量不是必要的. 如果 KUBECONFIG 环境变量不存在,kubectl 使用默认的 kubeconfig 文件,$HOME/.kube/config.

如果 KUBECONFIG 环境变量存在,kubectl 使用 KUBECONFIG 环境变量中列举的文件合并后的有效配置.


** 合并 kubeconfig 文件
要查看配置,输入以下命令:

kubectl config view

如前所述,输出可能来自 kubeconfig 文件,也可能是合并多个 kubeconfig 文件的结果.

以下是 kubectl 在合并 kubeconfig 文件时使用的规则.

  1. 如果设置了 --kubeconfig 参数,则仅使用指定的文件.不进行合并.此参数只能使用一次.

      否则,如果设置了 KUBECONFIG 环境变量,将它用作应合并的文件列表.根据以下规则合并 KUBECONFIG 环境变量中列出的文件:

        * 忽略空文件名.
        * 对于内容无法反序列化的文件,产生错误信息.
        * 第一个设置特定值或者映射键的文件将生效.
        * 永远不会更改值或者映射键.示例:保留第一个文件的上下文以设置 current-context.示例:如果两个文件都指定了 red-user,则仅使用第一个文件的 red-user 中的值.即使第二个文件在 red-user 下有非冲突条目,也要丢弃它们.


   否则,使用默认的 kubeconfig 文件, $HOME/.kube/config,不进行合并.

    1. 根据此链中的第一个匹配确定要使用的上下文.
        1. 如果存在,使用 --context 命令行参数.
        2. 使用合并的 kubeconfig 文件中的 current-context.

    这种场景下允许空上下文.

    1. 确定集群和用户.此时,可能有也可能没有上下文.根据此链中的第一个匹配确定集群和用户,这将运行两次:一次用于用户,一次用于集群.
        1. 如果存在,使用命令行参数:--user 或者 --cluster.
        2. 如果上下文非空,从上下文中获取用户或集群.
    
    这种场景下用户和集群可以为空.

    1. 确定要使用的实际集群信息.此时,可能有也可能没有集群信息.基于此链构建每个集群信息；第一个匹配项会被采用:
        1. 如果存在:--server、--certificate-authority 和 --insecure-skip-tls-verify,使用命令行参数.
        2. 如果合并的 kubeconfig 文件中存在集群信息属性,则使用它们.
        3. 如果没有 server 配置,则配置无效.
    2. 确定要使用的实际用户信息.使用与集群信息相同的规则构建用户信息,但每个用户只允许一种身份认证技术:
        1. 如果存在:--client-certificate、--client-key、--username、--password 和 --token,使用命令行参数.
        2. 使用合并的 kubeconfig 文件中的 user 字段.
        3. 如果存在两种冲突技术,则配置无效.
    3. 对于仍然缺失的任何信息,使用其对应的默认值,并可能提示输入身份认证信息.


** 文件引用
kubeconfig 文件中的文件和路径引用是相对于 kubeconfig 文件的位置. 命令行上的文件引用是相对于当前工作目录的. 在 $HOME/.kube/config 中,相对路径按相对路径存储,绝对路径按绝对路径存储.


** 代理
你可以在 kubeconfig 文件中设置 proxy-url 来为 kubectl 使用代理,例如:

apiVersion: v1
kind: Config

proxy-url: https://proxy.host:3128

clusters:
- cluster:
  name: development

users:
- name: developer

contexts:
- context:
  name: development


-------------------
# 安全
## 云原生安全概述
本概述定义了一个模型,用于在 Cloud Native 安全性上下文中考虑 Kubernetes 安全性.

Warning: 此容器安全模型只提供建议,而不是经过验证的信息安全策略.
云原生安全的 4 个 C
你可以分层去考虑安全性,云原生安全的 4 个 C 分别是云(Cloud)、集群(Cluster)、容器(Container)和代码(Code).

Note: 这种分层方法增强了深度防护方法在安全性方面的 防御能力,该方法被广泛认为是保护软件系统的最佳实践.


** 云原生安全的 4C
云原生安全模型的每一层都是基于下一个最外层,代码层受益于强大的基础安全层(云、集群、容器).你无法通过在代码层解决安全问题来为基础层中糟糕的安全标准提供保护.


** 云
在许多方面,云(或者位于同一位置的服务器,或者是公司数据中心)是 Kubernetes 集群中的 可信计算基. 如果云层容易受到攻击(或者被配置成了易受攻击的方式),就不能保证在此基础之上构建的组件是安全的. 每个云提供商都会提出安全建议,以在其环境中安全地运行工作负载.

* 云提供商安全性
具体内容:
https://kubernetes.io/zh/docs/concepts/security/overview/

* 基础设施安全
关于在 Kubernetes 集群中保护你的基础设施的建议:

Kubetnetes  基础架构关注领域	          建议
通过网络访问 API 服务(控制平面)	    所有对 Kubernetes 控制平面的访问不允许在 Internet 上公开,同时应由网络访问控制列表控制,该列表包含管理                                                                         集群所需的 IP 地址集.
通过网络访问 Node(节点)	                  节点应配置为 仅能 从控制平面上通过指定端口来接受(通过网络访问控制列表)连接,以及接受 NodePort 和 Loa                                                                          dBalancer 类型的 Kubernetes 服务连接.如果可能的话,这些节点不应完全暴露在公共互联网上.
Kubernetes 访问云提供商的 API	        每个云提供商都需要向 Kubernetes 控制平面和节点授予不同的权限集.为集群提供云提供商访问权限时,最好遵                                                                         循对需要管理的资源的最小特权原则.Kops 文档提供有关 IAM 策略和角色的信息.
访问 etcd	                                             对 etcd(Kubernetes 的数据存储)的访问应仅限于控制平面.根据配置情况,你应该尝试通过 TLS 来使用 etcd.更                                                                              多信息可以在 etcd 文档中找到.
etcd 加密	                                             在所有可能的情况下,最好对所有驱动器进行静态数据加密,并且由于 etcd 拥有整个集群的状态(包括机密信息)                                                                            ,因此其磁盘更应该进行静态数据加密.


** 集群
保护 Kubernetes 有两个方面需要注意:
  * 保护可配置的集群组件
  * 保护在集群中运行的应用程序

* 集群组件
如果想要保护集群免受意外或恶意的访问,采取良好的信息管理实践,请阅读并遵循有关保护集群的建议.

* 集群中的组件(您的应用) --- 重要
根据您的应用程序的受攻击面,您可能需要关注安全性的特定面,比如: 如果您正在运行中的一个服务(A 服务)在其他资源链中很重要,并且所运行的另一工作负载(服务 B) 容易受到资源枯竭的攻击,则如果你不限制服务 B 的资源的话,损害服务 A 的风险就会很高.下表列出了安全性关注的领域和建议,用以保护 Kubernetes 中运行的工作负载:

工作负载安全性关注领域	                    建议
RBAC 授权(访问 Kubernetes API)	          https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/
认证方式	                                                https://kubernetes.io/zh/docs/concepts/security/controlling-access/
应用程序 Secret 管理 (并在 etcd           https://kubernetes.io/zh/docs/concepts/configuration/secret/
中对其进行静态数据加密)	                    https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/ 
确保 Pod 符合定义的 Pod 安全标准	    https://kubernetes.io/zh/docs/concepts/security/pod-security-standards/#policy-instantiation
服务质量(和集群资源管理)	                  https://kubernetes.io/zh/docs/tasks/configure-pod-container/quality-service-pod/
网络策略	                                                https://kubernetes.io/zh/docs/concepts/services-networking/network-policies/
Kubernetes Ingress 的 TLS 支持	            https://kubernetes.io/zh/docs/concepts/services-networking/ingress/#tls

^^^^^^^^^^^^^^^^^^^
## 使用 RBAC 鉴权
原文链接: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/
基于角色(Role)的访问控制(RBAC)是一种基于组织中用户的角色来调节控制对 计算机或网络资源的访问的方法.

RBAC 鉴权机制使用 rbac.authorization.k8s.io API 组 来驱动鉴权决定,允许你通过 Kubernetes API 动态配置策略.

要启用 RBAC,在启动 API 服务器 时将 --authorization-mode 参数设置为一个逗号分隔的列表并确保其中包含 RBAC.

kube-apiserver --authorization-mode=Example,RBAC --<其他选项> --<其他选项>


** API 对象 
RBAC API 声明了四种 Kubernetes 对象:Role、ClusterRole、RoleBinding 和 ClusterRoleBinding.你可以像使用其他 Kubernetes 对象一样, 通过类似 kubectl 这类工具 描述对象, 或修补对象.

Caution:
这些对象在设计时即实施了一些访问限制.如果你在学习过程中对集群做了更改,请参考 避免特权提升和引导 一节,以了解这些限制会以怎样的方式阻止你做出修改.

* Role 和 ClusterRole 
RBAC 的 Role 或 ClusterRole 中包含一组代表相关权限的规则.这些权限是纯粹累加的(不存在拒绝某操作的规则).

Role 总是用来在某个名字空间 内设置访问权限；在你创建 Role 时,你必须指定该 Role 所属的名字空间.

与之相对,ClusterRole 则是一个集群作用域的资源.这两种资源的名字不同(Role 和 ClusterRole)是因为 Kubernetes 对象要么是名字空间作用域的,要么是集群作用域的, 不可两者兼具.

ClusterRole 有若干用法.你可以用它来:

  1. 定义对某名字空间域对象的访问权限,并将在各个名字空间内完成授权；
  2. 为名字空间作用域的对象设置访问权限,并跨所有名字空间执行授权；
  3. 为集群作用域的资源定义访问权限.
  4. 如果你希望在名字空间内定义角色,应该使用 Role； 如果你希望定义集群范围的角色,应该使用 ClusterRole.

Role 示例
下面是一个位于 "default" 名字空间的 Role 的示例,可用来授予对 pods 的读访问权限:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" 标明 core API 组
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

* ClusterRole 示例
ClusterRole 可以和 Role 相同完成授权.因为 ClusterRole 属于集群范围,所以它也可以为以下资源授予访问权限:

  * 集群范围资源(比如 节点(Node))
  * 非资源端点(比如 /healthz)
  * 跨名字空间访问的名字空间作用域的资源(如 Pods)
     比如,你可以使用 ClusterRole 来允许某特定用户执行 kubectl get pods --all-namespaces

下面是一个 ClusterRole 的示例,可用来为任一特定名字空间中的 Secret 授予读访问权限, 或者跨名字空间的访问权限(取决于该角色是如何绑定的):

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  # "namespace" 被忽略,因为 ClusterRoles 不受名字空间限制
  name: secret-reader
rules:
- apiGroups: [""]
  # 在 HTTP 层面,用来访问 Secret 对象的资源的名称为 "secrets"
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]

Role 或 ClusterRole 对象的名称必须是合法的 路径区段名称.

* RoleBinding 和 ClusterRoleBinding 
角色绑定(Role Binding)是将角色中定义的权限赋予一个或者一组用户.它包含若干 主体(用户、组或服务账户)的列表和对这些主体所获得的角色的引用.RoleBinding 在指定的名字空间中执行授权,而 ClusterRoleBinding 在集群范围执行授权.

一个 RoleBinding 可以引用同一的名字空间中的任何 Role.或者,一个 RoleBinding 可以引用某 ClusterRole 并将该 ClusterRole 绑定到 RoleBinding 所在的名字空间.如果你希望将某 ClusterRole 绑定到集群中所有名字空间,你要使用 ClusterRoleBinding.

RoleBinding 或 ClusterRoleBinding 对象的名称必须是合法的 路径区段名称.

RoleBinding 示例 
下面的例子中的 RoleBinding 将 "pod-reader" Role 授予在 "default" 名字空间中的用户 "jane".这样,用户 "jane" 就具有了读取 "default" 名字空间中 pods 的权限.

apiVersion: rbac.authorization.k8s.io/v1
# 此角色绑定允许 "jane" 读取 "default" 名字空间中的 Pods
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# 你可以指定不止一个“subject(主体)”
- kind: User
  name: jane # "name" 是区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # "roleRef" 指定与某 Role 或 ClusterRole 的绑定关系
  kind: Role # 此字段必须是 Role 或 ClusterRole
  name: pod-reader     # 此字段必须与你要绑定的 Role 或 ClusterRole 的名称匹配
  apiGroup: rbac.authorization.k8s.io

RoleBinding 也可以引用 ClusterRole,以将对应 ClusterRole 中定义的访问权限授予 RoleBinding 所在名字空间的资源.这种引用使得你可以跨整个集群定义一组通用的角色, 之后在多个名字空间中复用.

例如,尽管下面的 RoleBinding 引用的是一个 ClusterRole,"dave"(这里的主体, 区分大小写)只能访问 "development" 名字空间中的 Secrets 对象,因为 RoleBinding 所在的名字空间(由其 metadata 决定)是 "development".

apiVersion: rbac.authorization.k8s.io/v1
# 此角色绑定使得用户 "dave" 能够读取 "development" 名字空间中的 Secrets
# 你需要一个名为 "secret-reader" 的 ClusterRole
kind: RoleBinding
metadata:
  name: read-secrets
  # RoleBinding 的名字空间决定了访问权限的授予范围.
  # 这里隐含授权仅在 "development" 名字空间内的访问权限.
  namespace: development
subjects:
- kind: User
  name: dave # 'name' 是区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io

ClusterRoleBinding 示例 
要跨整个集群完成访问权限的授予,你可以使用一个 ClusterRoleBinding.下面的 ClusterRoleBinding 允许 "manager" 组内的所有用户访问任何名字空间中的 Secrets.

apiVersion: rbac.authorization.k8s.io/v1
# 此集群角色绑定允许 “manager” 组中的任何人访问任何名字空间中的 secrets
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager # 'name' 是区分大小写的
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io

创建了绑定之后,你不能再修改绑定对象所引用的 Role 或 ClusterRole.试图改变绑定对象的 roleRef 将导致合法性检查错误.如果你想要改变现有绑定对象中 roleRef 字段的内容,必须删除重新创建绑定对象.

这种限制有两个主要原因:
  
  1. 针对不同角色的绑定是完全不一样的绑定.要求通过删除/重建绑定来更改 roleRef, 这样可以确保要赋予绑定的所有主体会被授予新的角色(而不是在允许或者不小心修改 了 roleRef 的情况下导致所有现有主体未经验证即被授予新角色对应的权限).
  2. 将 roleRef 设置为不可以改变,这使得可以为用户授予对现有绑定对象的 update 权限, 这样可以让他们管理主体列表,同时不能更改被授予这些主体的角色.

命令 kubectl auth reconcile 可以创建或者更新包含 RBAC 对象的清单文件, 并且在必要的情况下删除和重新创建绑定对象,以改变所引用的角色.

* 对资源的引用 
在 Kubernetes API 中,大多数资源都是使用对象名称的字符串表示来呈现与访问的.例如,对于 Pod 应使用 "pods".RBAC 使用对应 API 端点的 URL 中呈现的名字来引用资源.有一些 Kubernetes API 涉及 子资源(subresource),例如 Pod 的日志.对 Pod 日志的请求看起来像这样:

GET /api/v1/namespaces/{namespace}/pods/{name}/log

在这里,pods 对应名字空间作用域的 Pod 资源,而 log 是 pods 的子资源.在 RBAC 角色表达子资源时,使用斜线(/)来分隔资源和子资源.要允许某主体读取 pods 同时访问这些 Pod 的 log 子资源,你可以这么写:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-and-pod-logs-reader
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]

对于某些请求,也可以通过 resourceNames 列表按名称引用资源.在指定时,可以将请求限定为资源的单个实例.下面的例子中限制可以 "get" 和 "update" 一个名为 my-configmap 的 ConfigMap:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: configmap-updater
rules:
- apiGroups: [""]
  # 在 HTTP 层面,用来访问 ConfigMap 的资源的名称为 "configmaps"
  resources: ["configmaps"]
  resourceNames: ["my-configmap"]
  verbs: ["update", "get"]

Note:
你不能使用资源名字来限制 create 或者 deletecollection 请求.对于 create 请求而言,这是因为在鉴权时可能还不知道新对象的名字.如果你使用 resourceName 来限制 list 或者 watch 请求, 客户端必须在它们的 list 或者 watch 请求里包含一个与指定的 resourceName 匹配的 metadata.name 字段选择器.例如,kubectl get configmaps --field-selector=metadata.name=my-configmap

* 聚合的 ClusterRole 
你可以将若干 ClusterRole 聚合(Aggregate) 起来,形成一个复合的 ClusterRole.某个控制器作为集群控制面的一部分会监视带有 aggregationRule 的 ClusterRole 对象集合.aggregationRule 为控制器定义一个标签 选择算符供后者匹配 应该组合到当前 ClusterRole 的 roles 字段中的 ClusterRole 对象.

下面是一个聚合 ClusterRole 的示例:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring
aggregationRule:
  clusterRoleSelectors:
  - matchLabels:
      rbac.example.com/aggregate-to-monitoring: "true"
rules: [] # 控制面自动填充这里的规则

如果你创建一个与某个已存在的聚合 ClusterRole 的标签选择算符匹配的 ClusterRole, 这一变化会触发新的规则被添加到聚合 ClusterRole 的操作.下面的例子中,通过创建一个标签同样为 rbac.example.com/aggregate-to-monitoring: true 的 ClusterRole,新的规则可被添加到 "monitoring" ClusterRole 中.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-endpoints
  labels:
    rbac.example.com/aggregate-to-monitoring: "true"
# 当你创建 "monitoring-endpoints" ClusterRole 时,
# 下面的规则会被添加到 "monitoring" ClusterRole 中
rules:
- apiGroups: [""]
  resources: ["services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]

默认的面向用户的角色 使用 ClusterRole 聚合.这使得作为集群管理员的你可以为扩展默认规则,包括为定制资源设置规则, 比如通过 CustomResourceDefinitions 或聚合 API 服务器提供的定制资源.

例如,下面的 ClusterRoles 让默认角色 "admin" 和 "edit" 拥有管理自定义资源 "CronTabs" 的权限, "view" 角色对 CronTab 资源拥有读操作权限.你可以假定 CronTab 对象在 API 服务器所看到的 URL 中被命名为 "crontabs".

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aggregate-cron-tabs-edit
  labels:
    # 添加以下权限到默认角色 "admin" 和 "edit" 中
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
rules:
- apiGroups: ["stable.example.com"]
  resources: ["crontabs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: aggregate-cron-tabs-view
  labels:
    # 添加以下权限到 "view" 默认角色中
    rbac.authorization.k8s.io/aggregate-to-view: "true"
rules:
- apiGroups: ["stable.example.com"]
  resources: ["crontabs"]
  verbs: ["get", "list", "watch"]

Role 示例 
以下示例均为从 Role 或 ClusterRole 对象中截取出来,我们仅展示其 rules 部分.

允许读取在核心 API 组下的 "Pods":

rules:
- apiGroups: [""]
  # 在 HTTP 层面,用来访问 Pod 的资源的名称为 "pods"
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

允许读/写在 "extensions" 和 "apps" API 组中的 Deployment(在 HTTP 层面,对应 URL 中资源部分为 "deployments"):

rules:
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

允许读取核心 API 组中的 "pods" 和读/写 "batch" 或 "extensions" API 组中的 "jobs":

rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

允许读取名称为 "my-config" 的 ConfigMap(需要通过 RoleBinding 绑定以 限制为某名字空间中特定的 ConfigMap):

rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["my-config"]
  verbs: ["get"]

允许读取在核心组中的 "nodes" 资源(因为 Node 是集群作用域的,所以需要 ClusterRole 绑定到 ClusterRoleBinding 才生效):

rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]

允许针对非资源端点 /healthz 和其子路径上发起 GET 和 POST 请求 (必须在 ClusterRole 绑定 ClusterRoleBinding 才生效):

rules:
  - nonResourceURLs: ["/healthz", "/healthz/*"] # nonResourceURL 中的 '*' 是一个全局通配符
    verbs: ["get", "post"]

* 对主体的引用 
RoleBinding 或者 ClusterRoleBinding 可绑定角色到某 *主体(Subject)*上.主体可以是组,用户或者 服务账户.

Kubernetes 用字符串来表示用户名.用户名可以是普通的用户名,像 "alice"；或者是邮件风格的名称,如 "bob@example.com", 或者是以字符串形式表达的数字 ID.你作为 Kubernetes 管理员负责配置 身份认证模块 以便后者能够生成你所期望的格式的用户名.

Caution:
前缀 system: 是 Kubernetes 系统保留的,所以你要确保 所配置的用户名或者组名不能出现上述 system: 前缀.除了对前缀的限制之外,RBAC 鉴权系统不对用户名格式作任何要求.

在 Kubernetes 中,鉴权模块提供用户组信息.与用户名一样,用户组名也用字符串来表示,而且对该字符串没有格式要求, 只是不能使用保留的前缀 system:.

服务账户 的用户名前缀为 system:serviceaccount:,属于前缀为 system:serviceaccounts: 的用户组.

Note:
  * system:serviceaccount: (单数)是用于服务账户用户名的前缀；
  * system:serviceaccounts: (复数)是用于服务账户组名的前缀.

RoleBinding 示例 
下面示例是 RoleBinding 中的片段,仅展示其 subjects 的部分.

对于名称为 alice@example.com 的用户:

subjects:
- kind: User
  name: "alice@example.com"
  apiGroup: rbac.authorization.k8s.io

对于名称为 frontend-admins 的用户组:

subjects:
- kind: Group
  name: "frontend-admins"
  apiGroup: rbac.authorization.k8s.io

对于 kube-system 名字空间中的默认服务账户:

subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system

对于任何名称空间中的 "qa" 组中所有的服务账户:

subjects:
- kind: Group
  name: system:serviceaccounts:qa
  apiGroup: rbac.authorization.k8s.io

对于 "development" 名称空间中 "dev" 组中的所有服务帐户:

subjects:
- kind: Group
  name: system:serviceaccounts:dev
  apiGroup: rbac.authorization.k8s.io
  namespace: development

对于在任何名字空间中的服务账户:

subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io

对于所有已经过认证的用户:

subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io

对于所有未通过认证的用户:

subjects:
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io

对于所有用户:

subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io


** 默认 Roles 和 Role Bindings
API 服务器创建一组默认的 ClusterRole 和 ClusterRoleBinding 对象.这其中许多是以 system: 为前缀的,用以标识对应资源是直接由集群控制面管理的.所有的默认 ClusterRole 和 ClusterRoleBinding 都有 kubernetes.io/bootstrapping=rbac-defaults 标签.

Caution:
在修改名称包含 system: 前缀的 ClusterRole 和 ClusterRoleBinding 时要格外小心.对这些资源的更改可能导致集群无法继续工作.

* 自动协商 
在每次启动时,API 服务器都会更新默认 ClusterRole 以添加缺失的各种权限,并更新 默认的 ClusterRoleBinding 以增加缺失的各类主体.这种自动协商机制允许集群去修复一些不小心发生的修改,并且有助于保证角色和角色绑定 在新的发行版本中有权限或主体变更时仍然保持最新.

如果要禁止此功能,请将默认 ClusterRole 以及 ClusterRoleBinding 的 rbac.authorization.kubernetes.io/autoupdate 注解设置成 false.注意,缺少默认权限和角色绑定主体可能会导致集群无法正常工作.

如果基于 RBAC 的鉴权机制被启用,则自动协商功能默认是被启用的.

* API 发现角色 
无论是经过身份验证的还是未经过身份验证的用户,默认的角色绑定都授权他们读取被认为 是可安全地公开访问的 API( 包括 CustomResourceDefinitions).如果要禁用匿名的未经过身份验证的用户访问,请在 API 服务器配置中中添加 --anonymous-auth=false 的配置选项.

通过运行命令 kubectl 可以查看这些角色的配置信息:

kubectl get clusterroles system:discovery -o yaml

Note:
如果你编辑该 ClusterRole,你所作的变更会被 API 服务器在重启时自动覆盖,这是通过 自动协商机制完成的.要避免这类覆盖操作, 要么不要手动编辑这些角色,要么禁止自动协商机制.

默认 ClusterRole	                  默认 ClusterRoleBinding	        描述
system:basic-user                  system:authenticated 组	        允许用户以只读的方式去访问他们自己的基本信息.在 1.14 版本之前,这个角色在默认情况下                                                                                                        也绑定在 system:unauthenticated 上.
system:discovery	                  system:authenticated 组	        允许以只读方式访问 API 发现端点,这些端点用来发现和协商 API 级别.在 1.14 版本之前,                                                                                                                 这个角色在默认情况下绑定在 system:unauthenticated 上.
system:public-info-viewer	    system:authenticated 和 
                                                system:unauthenticated 组	    允许对集群的非敏感信息进行只读访问,它是在 1.14 版本中引入的.

* 面向用户的角色 
一些默认的 ClusterRole 不是以前缀 system: 开头的.这些是面向用户的角色.它们包括超级用户(Super-User)角色(cluster-admin)、 使用 ClusterRoleBinding 在集群范围内完成授权的角色(cluster-status)、 以及使用 RoleBinding 在特定名字空间中授予的角色(admin、edit、view).

面向用户的 ClusterRole 使用 ClusterRole 聚合以允许管理员在 这些 ClusterRole 上添加用于定制资源的规则.如果想要添加规则到 admin、edit 或者 view, 可以创建带有以下一个或多个标签的 ClusterRole:

metadata:
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
    rbac.authorization.k8s.io/aggregate-to-view: "true"

默认 ClusterRole	  默认 ClusterRoleBinding	    描述
cluster-admin	        system:masters 组	              允许超级用户在平台上的任何资源上执行所有操作.当在 ClusterRoleBinding 中使用时,可以授权对集群                                                                                        中以及所有名字空间中的全部资源进行完全控制.当在 RoleBinding 中使用时,可以授权控制 RoleBinding                                                                                       所在名字空间中的所有资源,包括名字空间本身.
admin	                      无	                                        允许管理员访问权限,旨在使用 RoleBinding 在名字空间内执行授权.
                                                                              如果在 RoleBinding 中使用,则可授予对名字空间中的大多数资源的读/写权限, 包括创建角色和角色绑定                                                                                      的能力.此角色不允许对资源配额或者名字空间本身进行写操作.此角色也不允许对 Kubernetes v1.22+                                                                                         创建的 Endpoints 进行写操作.更多信息参阅“Endpoints 写权限”小节.

edit	                        无	                                        允许对名字空间的大多数对象进行读/写操作.
                                                                              它不允许查看或者修改角色或者角色绑定.不过,此角色可以访问 Secret,以名字空间中任何 ServiceAcc                                                                                            ount 的身份运行 Pods, 所以可以用来了解名字空间内所有服务账户的 API 访问级别.此角色也不允许对                                                                                       Kubernetes v1.22+ 创建的 Endpoints 进行写操作.更多信息参阅“Endpoints 写操作”小节.

view	                        无	                                        允许对名字空间的大多数对象有只读权限.它不允许查看角色或角色绑定.
                                                                              此角色不允许查看 Secrets,因为读取 Secret 的内容意味着可以访问名字空间中 ServiceAccount 的凭据信息                                                                                 ,进而允许利用名字空间中任何 ServiceAccount 的 身份访问 API(这是一种特权提升).


* 核心组件角色 

默认 ClusterRole	                            默认 ClusterRoleBinding	                  描述
system:kube-scheduler	                  system:kube-scheduler                     用户	允许访问 scheduler 组件所需要的资源.
system:volume-scheduler	              system:kube-scheduler                     用户	允许访问 kube-scheduler 组件所需要的卷资源.
system:kube-controller-manager	  system:kube-controller-manager      用户	允许访问控制器管理器 组件所需要的资源.各个控制回路所需要的权限在控制                                                                                                                        器角色 详述.
system:node	                                    无	                                                      允许访问 kubelet 所需要的资源,包括对所有 Secret 的读操作和对所有 Pod 状态对象                                                                                                                       的写操作.
                                                                                                                      你应该使用 Node 鉴权组件 和 NodeRestriction 准入插件 而不是 system:node 角色.                                                                                                                           同时基于 kubelet 上调度执行的 Pod 来授权 kubelet 对 API 的访问.

                                                                                                                     system:node 角色的意义仅是为了与从 v1.8 之前版本升级而来的集群兼容.

system:node-proxier	                      system:kube-proxy 用户	                 允许访问 kube-proxy 组件所需要的资源.

* 其他组件角色 

默认 ClusterRole	                                      默认 ClusterRoleBinding	      描述
system:auth-delegator	                            无	                                          允许将身份认证和鉴权检查操作外包出去.这种角色通常用在插件式 API 服务器上,以                                                                                                                      实现统一的身份认证和鉴权.
system:heapster	                                        无	                                          为 Heapster 组件(已弃用)定义的角色.
system:kube-aggregator	                          无	                                          为 kube-aggregator 组件定义的角色.
system:kube-dns	                                      在 kube-system 名字空         为 kube-dns 组件定义的角色
                                                                    间中的 kube-dns 服务账户	.
system:kubelet-api-admin	                        无	                                          允许 kubelet API 的完全访问权限.
system:node-bootstrapper	                      无	                                          允许访问执行 kubelet TLS 启动引导 所需要的资源.
system:node-problem-detector	                无	                                          为 node-problem-detector 组件定义的角色.
system:persistent-volume-provisioner	    无	                                          允许访问大部分 动态卷驱动 所需要的资源.
system:monitoring	                                    system:monitoring 组	          允许对控制平面监控端点的读取访问(例如:kube-apiserver 存活和就绪端点(/healthz、/l                                                                                                                   ivez、/readyz), 各个健康检查端点(/healthz/*、/livez/*、/readyz/*)和 /metrics).请注意,                                                                                                                     各个运行状况检查端点和度量标准端点可能会公开敏感信息.

* 内置控制器的角色 
Kubernetes 控制器管理器 运行内建于 Kubernetes 控制面的控制器.当使用 --use-service-account-credentials 参数启动时, kube-controller-manager 使用单独的服务账户来启动每个控制器.每个内置控制器都有相应的、前缀为 system:controller: 的角色.如果控制管理器启动时未设置 --use-service-account-credentials, 它使用自己的身份凭据来运行所有的控制器,该身份必须被授予所有相关的角色.这些角色包括:

  * system:controller:attachdetach-controller
  * system:controller:certificate-controller
  * system:controller:clusterrole-aggregation-controller
  * system:controller:cronjob-controller
  * system:controller:daemon-set-controller
  * system:controller:deployment-controller
  * system:controller:disruption-controller
  * system:controller:endpoint-controller
  * system:controller:expand-controller
  * system:controller:generic-garbage-collector
  * system:controller:horizontal-pod-autoscaler
  * system:controller:job-controller
  * system:controller:namespace-controller
  * system:controller:node-controller
  * system:controller:persistent-volume-binder
  * system:controller:pod-garbage-collector
  * system:controller:pv-protection-controller
  * system:controller:pvc-protection-controller
  * system:controller:replicaset-controller
  * system:controller:replication-controller
  * system:controller:resourcequota-controller
  * system:controller:root-ca-cert-publisher
  * system:controller:route-controller
  * system:controller:service-account-controller
  * system:controller:service-controller
  * system:controller:statefulset-controller
  * system:controller:ttl-controller


** 初始化与预防权限提升
RBAC API 会阻止用户通过编辑角色或者角色绑定来提升权限. 由于这一点是在 API 级别实现的,所以在 RBAC 鉴权组件未启用的状态下依然可以正常工作.

* 对角色创建或更新的限制
只有在符合下列条件之一的情况下,你才能创建/更新角色:

  1. 你已经拥有角色中包含的所有权限,且其作用域与正被修改的对象作用域相同. (对 ClusterRole 而言意味着集群范围,对 Role 而言意味着相同名字空间或者集群范围).
  2. 你被显式授权在 rbac.authorization.k8s.io API 组中的 roles 或 clusterroles 资源 使用 escalate 动词.

例如,如果 user-1 没有列举集群范围所有 Secret 的权限,他将不能创建包含该权限的 ClusterRole. 若要允许用户创建/更新角色:

  1. 根据需要赋予他们一个角色,允许他们根据需要创建/更新 Role 或者 ClusterRole 对象.
  2. 授予他们在所创建/更新角色中包含特殊权限的权限:
    * 隐式地为他们授权(如果它们试图创建或者更改 Role 或 ClusterRole 的权限, 但自身没有被授予相应权限,API 请求将被禁止).
    * 通过允许他们在 Role 或 ClusterRole 资源上执行 escalate 动作显式完成授权. 这里的 roles 和 clusterroles 资源包含在 rbac.authorization.k8s.io API 组中.

* 对角色绑定创建或更新的限制
只有你已经具有了所引用的角色中包含的全部权限时,或者你被授权在所引用的角色上执行 bind 动词时,你才可以创建或更新角色绑定.这里的权限与角色绑定的作用域相同. 例如,如果用户 user-1 没有列举集群范围所有 Secret 的能力,则他不可以创建 ClusterRoleBinding 引用授予该许可权限的角色. 如要允许用户创建或更新角色绑定:

  1. 赋予他们一个角色,使得他们能够根据需要创建或更新 RoleBinding 或 ClusterRoleBinding 对象.
  2. 授予他们绑定某特定角色所需要的许可权限:
    * 隐式授权下,可以将角色中包含的许可权限授予他们；
    * 显式授权下,可以授权他们在特定 Role (或 ClusterRole)上执行 bind 动词的权限.

例如,下面的 ClusterRole 和 RoleBinding 将允许用户 user-1 把名字空间 user-1-namespace 中的 admin、edit 和 view 角色赋予其他用户:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: role-grantor
rules:
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings"]
  verbs: ["create"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["clusterroles"]
  verbs: ["bind"]
  # 忽略 resourceNames 意味着允许绑定任何 ClusterRole
  resourceNames: ["admin","edit","view"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: role-grantor-binding
  namespace: user-1-namespace
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: role-grantor
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user-1

当启动引导第一个角色和角色绑定时,需要为初始用户授予他们尚未拥有的权限. 对初始角色和角色绑定进行初始化时需要:

  * 使用用户组为 system:masters 的凭据,该用户组由默认绑定关联到 cluster-admin 这个超级用户角色.
  * 如果你的 API 服务器启动时启用了不安全端口(使用 --insecure-port), 你也可以通过 该端口调用 API ,这样的操作会绕过身份验证或鉴权.


** 一些命令行工具
* kubectl create role
创建 Role 对象,定义在某一名字空间中的权限.例如:

  * 创建名称为 "pod-reader" 的 Role 对象,允许用户对 Pods 执行 get、watch 和 list 操作:
  
     kubectl create role pod-reader --verb=get --verb=list --verb=watch --resource=pods
  
  * 创建名称为 "pod-reader" 的 Role 对象并指定 resourceNames:

     kubectl create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod
  
  * 创建名为 "foo" 的 Role 对象并指定 apiGroups:

     kubectl create role foo --verb=get,list,watch --resource=replicasets.apps
  
  * 创建名为 "foo" 的 Role 对象并指定子资源权限:

     kubectl create role foo --verb=get,list,watch --resource=pods,pods/status

  * 创建名为 "my-component-lease-holder" 的 Role 对象,使其具有对特定名称的 资源执行 get/update 的权限:

     kubectl create role my-component-lease-holder --verb=get,list,watch,update --resource=lease --resource-name=my-component

* kubectl create clusterrole
创建 ClusterRole 对象.例如:

  * 创建名称为 "pod-reader" 的 ClusterRole对象,允许用户对 Pods 对象执行 get、watch和list` 操作:

     kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods

  * 创建名为 "pod-reader" 的 ClusterRole 对象并指定 resourceNames:

     kubectl create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod

  * 创建名为 "foo" 的 ClusterRole 对象并指定 apiGroups:

     kubectl create clusterrole foo --verb=get,list,watch --resource=replicasets.apps

  * 创建名为 "foo" 的 ClusterRole 对象并指定子资源:

     kubectl create clusterrole foo --verb=get,list,watch --resource=pods,pods/status
  
  * 创建名为 "foo" 的 ClusterRole 对象并指定 nonResourceURL:

     kubectl create clusterrole "foo" --verb=get --non-resource-url=/logs/*

  * 创建名为 "monitoring" 的 ClusterRole 对象并指定 aggregationRule:

     kubectl create clusterrole monitoring --aggregation-rule="rbac.example.com/aggregate-to-monitoring=true"

* kubectl create rolebinding
在特定的名字空间中对 Role 或 ClusterRole 授权.例如:

  * 在名字空间 "acme" 中,将名为 admin 的 ClusterRole 中的权限授予名称 "bob" 的用户:

     kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme

  * 在名字空间 "acme" 中,将名为 view 的 ClusterRole 中的权限授予名字空间 "acme" 中名为 myapp 的服务账户:

     kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme

  * 在名字空间 "acme" 中,将名为 view 的 ClusterRole 对象中的权限授予名字空间 "myappnamespace" 中名称为 myapp 的服务账户:

     kubectl create rolebinding myappnamespace-myapp-view-binding --clusterrole=view --serviceaccount=myappnamespace:myapp --namespace=acme

* kubectl create clusterrolebinding
在整个集群(所有名字空间)中用 ClusterRole 授权.例如:

  * 在整个集群范围,将名为 cluster-admin 的 ClusterRole 中定义的权限授予名为 "root" 用户:

     kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root

  * 在整个集群范围内,将名为 system:node-proxier 的 ClusterRole 的权限授予名为 "system:kube-proxy" 的用户:

     kubectl create clusterrolebinding kube-proxy-binding --clusterrole=system:node-proxier --user=system:kube-proxy

  * 在整个集群范围内,将名为 view 的 ClusterRole 中定义的权限授予 "acme" 名字空间中 名为 "myapp" 的服务账户:

     kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp

* kubectl auth reconcile
使用清单文件来创建或者更新 rbac.authorization.k8s.io/v1 API 对象.

尚不存在的对象会被创建,如果对应的名字空间也不存在,必要的话也会被创建. 已经存在的角色会被更新,使之包含输入对象中所给的权限.如果指定了 --remove-extra-permissions,可以删除额外的权限.

已经存在的绑定也会被更新,使之包含输入对象中所给的主体.如果指定了 --remove-extra-permissions,则可以删除多余的主体.

例如:

  * 测试应用 RBAC 对象的清单文件,显示将要进行的更改:

     kubectl auth reconcile -f my-rbac-rules.yaml --dry-run           # --dry-run=client 参数来预览而不真正提交即将下发到集群的对象实例

  * 应用 RBAC 对象的清单文件,保留角色中的额外权限和绑定中的其他主体:

     kubectl auth reconcile -f my-rbac-rules.yaml

  * 应用 RBAC 对象的清单文件, 删除角色中的额外权限和绑定中的其他主体:

     kubectl auth reconcile -f my-rbac-rules.yaml --remove-extra-subjects --remove-extra-permissions


** 服务账户权限 
默认的 RBAC 策略为控制面组件、节点和控制器授予权限. 但是不会对 kube-system 名字空间之外的服务账户授予权限. (除了授予所有已认证用户的发现权限)

这使得你可以根据需要向特定服务账户授予特定权限. 细粒度的角色绑定可带来更好的安全性,但需要更多精力管理. 粗粒度的授权可能导致服务账户被授予不必要的 API 访问权限(甚至导致潜在的权限提升), 但更易于管理.

按从最安全到最不安全的顺序,存在以下方法:

  1. 为特定应用的服务账户授予角色(最佳实践)

      这要求应用在其 Pod 规约中指定 serviceAccountName, 并额外创建服务账户(包括通过 API、应用程序清单、kubectl create serviceaccount 等).

      例如,在名字空间 "my-namespace" 中授予服务账户 "my-sa" 只读权限:

      kubectl create rolebinding my-sa-view \
      --clusterrole=view \
      --serviceaccount=my-namespace:my-sa \
      --namespace=my-namespace

  2. 将角色授予某名字空间中的 "default" 服务账户

      如果某应用没有指定 serviceAccountName,那么它将使用 "default" 服务账户.

      Note: "default" 服务账户所具有的权限会被授予给名字空间中所有未指定 serviceAccountName 的 Pod.
      
      例如,在名字空间 "my-namespace" 中授予服务账户 "default" 只读权限:

      kubectl create rolebinding default-view \
      --clusterrole=view \
      --serviceaccount=my-namespace:default \
      --namespace=my-namespace

      许多插件组件 在 kube-system 名字空间以 "default" 服务账户运行. 要允许这些插件组件以超级用户权限运行,需要将集群的 cluster-admin 权限授               予kube-system 名字空间中的 "default" 服务账户.

      Note: 启用这一配置意味着在 kube-system 名字空间中包含以超级用户账号来访问 API 的 Secrets.

      kubectl create clusterrolebinding add-on-cluster-admin \
      --clusterrole=cluster-admin \
      --serviceaccount=kube-system:default

  3. 将角色授予名字空间中所有服务账户

      如果你想要名字空间中所有应用都具有某角色,无论它们使用的什么服务账户, 可以将角色授予该名字空间的服务账户组.

      例如,在名字空间 "my-namespace" 中的只读权限授予该名字空间中的所有服务账户:

      kubectl create rolebinding serviceaccounts-view \
      --clusterrole=view \
      --group=system:serviceaccounts:my-namespace \
      --namespace=my-namespace

  4. 在集群范围内为所有服务账户授予一个受限角色(不鼓励)

      如果你不想管理每一个名字空间的权限,你可以向所有的服务账户授予集群范围的角色.

      例如,为集群范围的所有服务账户授予跨所有名字空间的只读权限:

      kubectl create clusterrolebinding serviceaccounts-view \
      --clusterrole=view \
      --group=system:serviceaccounts

  5. 授予超级用户访问权限给集群范围内的所有服务帐户(强烈不鼓励)

      如果你不关心如何区分权限,你可以将超级用户访问权限授予所有服务账户.

      Warning: 这样做会允许所有应用都对你的集群拥有完全的访问权限,并将允许所有能够读取 Secret(或创建 Pod)的用户对你的集群有完全的访问权限.
      
      kubectl create clusterrolebinding serviceaccounts-cluster-admin \
      --clusterrole=cluster-admin \
      --group=system:serviceaccounts


** Endpoints 写权限
在 Kubernetes v1.22 之前版本创建的集群里, "edit" 和 "admin" 聚合角色包含对 Endpoints 的写权限.

升级到 Kubernetes v1.22 版本的现有集群不会包括此变化.

如果你希望在新集群的聚合角色里保留此访问权限,你可以创建下面的 ClusterRole:

access/endpoints-aggregated.yaml 

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    kubernetes.io/description: |-
      Add endpoints write permissions to the edit and admin roles. This was
      removed by default in 1.22 because of CVE-2021-25740. See
      https://issue.k8s.io/103675. This can allow writers to direct LoadBalancer
      or Ingress implementations to expose backend IPs that would not otherwise
      be accessible, and can circumvent network policies or security controls
      intended to prevent/isolate access to those backends.      
  labels:
    rbac.authorization.k8s.io/aggregate-to-edit: "true"
  name: custom:aggregate-to-edit:endpoints # you can change this if you wish
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["create", "delete", "deletecollection", "patch", "update"]


** 从 ABAC 升级
原来运行较老版本 Kubernetes 的集群通常会使用限制宽松的 ABAC 策略, 包括授予所有服务帐户全权访问 API 的能力.

默认的 RBAC 策略为控制面组件、节点和控制器等授予有限的权限,但不会为 kube-system 名字空间外的服务账户授权 (除了授予所有认证用户的发现权限之外).

这样做虽然安全得多,但可能会干扰期望自动获得 API 权限的现有工作负载. 这里有两种方法来完成这种转换:

* 并行鉴权 
同时运行 RBAC 和 ABAC 鉴权模式, 并指定包含 现有的 ABAC 策略 的策略文件:

--authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.json

关于命令行中的第一个选项:如果早期的鉴权组件,例如 Node,拒绝了某个请求,则 RBAC 鉴权组件尝试对该 API 请求鉴权.如果 RBAC 也拒绝了该 API 请求,则运行 ABAC 鉴权组件.这意味着被 RBAC 或 ABAC 策略所允许的任何请求都是被允许的请求.

如果 API 服务器启动时,RBAC 组件的日志级别为 5 或更高(--vmodule=rbac*=5 或 --v=5), 你可以在 API 服务器的日志中看到 RBAC 的细节 (前缀 RBAC:) 你可以使用这些信息来确定需要将哪些角色授予哪些用户、组或服务帐户.

一旦你将角色授予服务账户 ,工作负载运行时 在服务器日志中没有出现 RBAC 拒绝消息,就可以删除 ABAC 鉴权器.

* 宽松的 RBAC 权限 
你可以使用 RBAC 角色绑定在多个场合使用宽松的策略.

Warning:
下面的策略允许 所有 服务帐户充当集群管理员. 容器中运行的所有应用程序都会自动收到服务帐户的凭据,可以对 API 执行任何操作, 包括查看 Secrets 和修改权限.这一策略是不被推荐的.

kubectl create clusterrolebinding permissive-binding \
  --clusterrole=cluster-admin \
  --user=admin \
  --user=kubelet \
  --group=system:serviceaccounts

在你完成到 RBAC 的迁移后,应该调整集群的访问控制,确保相关的策略满足你的信息安全需求.

vvvvvvvvvvvvvvvvvvv


## Kubernetes API 访问控制
本页面概述了对 Kubernetes API 的访问控制.

用户使用 kubectl、客户端库或构造 REST 请求来访问 Kubernetes API. 人类用户和 Kubernetes 服务账户都可以被鉴权访问 API. 当请求到达 API 时,它会经历多个阶段,如下图所示:
                                     1                             2                          3                             4
Human User            -> Authentication -> Authorization -> Admission Control -> 对象
Pod(Kubernetes                 (认证)                     (鉴权)                  (准入控制)
Service Account)


** 传输安全
在典型的 Kubernetes 集群中,API 服务器在 443 端口上提供服务,受 TLS 保护. API 服务器出示证书. 该证书可以使用私有证书颁发机构(CA)签名,也可以基于链接到公认的 CA 的公钥基础架构签名.

如果你的集群使用私有证书颁发机构,你需要在客户端的 ~/.kube/config 文件中提供该 CA 证书的副本, 以便你可以信任该连接并确认该连接没有被拦截.

你的客户端可以在此阶段出示 TLS 客户端证书.


** 认证
如上图步骤 1 所示,建立 TLS 后, HTTP 请求将进入认证(Authentication)步骤. 集群创建脚本或者集群管理员配置 API 服务器,使之运行一个或多个身份认证组件. 身份认证组件在认证节中有更详细的描述.

认证步骤的输入整个 HTTP 请求；但是,通常组件只检查头部或/和客户端证书.

认证模块包含客户端证书、密码、普通令牌、引导令牌和 JSON Web 令牌(JWT,用于服务账户).

可以指定多个认证模块,在这种情况下,服务器依次尝试每个验证模块,直到其中一个成功.

如果请求认证不通过,服务器将以 HTTP 状态码 401 拒绝该请求. 反之,该用户被认证为特定的 username,并且该用户名可用于后续步骤以在其决策中使用. 部分验证器还提供用户的组成员身份,其他则不提供.


** 鉴权
如上图的步骤 2 所示,将请求验证为来自特定的用户后,请求必须被鉴权.

请求必须包含请求者的用户名、请求的行为以及受该操作影响的对象. 如果现有策略声明用户有权完成请求的操作,那么该请求被鉴权通过.

例如,如果 Bob 有以下策略,那么他只能在 projectCaribou 名称空间中读取 Pod.

{
    "apiVersion": "abac.authorization.kubernetes.io/v1beta1",
    "kind": "Policy",
    "spec": {
        "user": "bob",
        "namespace": "projectCaribou",
        "resource": "pods",
        "readonly": true
    }
}

如果 Bob 执行以下请求,那么请求会被鉴权,因为允许他读取 projectCaribou 名称空间中的对象.

{
  "apiVersion": "authorization.k8s.io/v1beta1",
  "kind": "SubjectAccessReview",
  "spec": {
    "resourceAttributes": {
      "namespace": "projectCaribou",
      "verb": "get",
      "group": "unicorn.example.org",
      "resource": "pods"
    }
  }
}

如果 Bob 在 projectCaribou 名字空间中请求写(create 或 update)对象,其鉴权请求将被拒绝. 如果 Bob 在诸如 projectFish 这类其它名字空间中请求读取(get)对象,其鉴权也会被拒绝.

Kubernetes 鉴权要求使用公共 REST 属性与现有的组织范围或云提供商范围的访问控制系统进行交互. 使用 REST 格式很重要,因为这些控制系统可能会与 Kubernetes API 之外的 API 交互.

Kubernetes 支持多种鉴权模块,例如 ABAC 模式、RBAC 模式和 Webhook 模式等. 管理员创建集群时,他们配置应在 API 服务器中使用的鉴权模块. 如果配置了多个鉴权模块,则 Kubernetes 会检查每个模块,任意一个模块鉴权该请求,请求即可继续； 如果所有模块拒绝了该请求,请求将会被拒绝(HTTP 状态码 403).


** 准入控制
准入控制模块是可以修改或拒绝请求的软件模块. 除鉴权模块可用的属性外,准入控制模块还可以访问正在创建或修改的对象的内容.

准入控制器对创建、修改、删除或(通过代理)连接对象的请求进行操作. 准入控制器不会对仅读取对象的请求起作用. 有多个准入控制器被配置时,服务器将依次调用它们.

这一操作如上图的步骤 3 所示.

与身份认证和鉴权模块不同,如果任何准入控制器模块拒绝某请求,则该请求将立即被拒绝.

除了拒绝对象之外,准入控制器还可以为字段设置复杂的默认值.

可用的准入控制模块在准入控制器中进行了描述.

请求通过所有准入控制器后,将使用检验例程检查对应的 API 对象,然后将其写入对象存储(如步骤 4 所示).


** 审计
Kubernetes 审计提供了一套与安全相关的、按时间顺序排列的记录,其中记录了集群中的操作序列. 集群对用户、使用 Kubernetes API 的应用程序以及控制平面本身产生的活动进行审计.


** API 服务器端口和 IP
前面的讨论适用于发送到 API 服务器的安全端口的请求(典型情况). API 服务器实际上可以在 2 个端口上提供服务:

默认情况下,Kubernetes API 服务器在 2 个端口上提供 HTTP 服务:

  1. localhost 端口:
      * 用于测试和引导,以及主控节点上的其他组件(调度器,控制器管理器)与 API 通信
      * 没有 TLS
      * 默认为端口 8080
      * 默认 IP 为 localhost,使用 --insecure-bind-address 进行更改
      * 请求 绕过 身份认证和鉴权模块
      * 由准入控制模块处理的请求
      * 受需要访问主机的保护
  2. “安全端口”:
      * 尽可能使用
      * 使用 TLS. 用 --tls-cert-file 设置证书,用 --tls-private-key-file 设置密钥
      * 默认端口 6443,使用 --secure-port 更改
      * 默认 IP 是第一个非本地网络接口,使用 --bind-address 更改
      * 请求须经身份认证和鉴权组件处理
      * 请求须经准入控制模块处理
      * 身份认证和鉴权模块运行

^^^^^^^^^^^^^^^^^^^
## 静态加密 Secret 数据
原文链接: https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/encrypt-data/
本文展示如何启用和配置静态 Secret 数据的加密

** Before you begin
你必须拥有一个 Kubernetes 的集群,同时你的 Kubernetes 集群必须带有 kubectl 命令行工具. 
  * Katacoda
  * 玩转 Kubernetes
     To check the version, enter kubectl version.
  * 需要 etcd v3 或者更高版本


** 配置并确定是否已启用静态数据加密
kube-apiserver 的参数 --experimental-encryption-provider-config 控制 API 数据在 etcd 中的加密方式. 下面提供一个配置示例.


** 理解静态数据加密

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aesgcm:
        keys:
        - name: key1
          secret: c2VjcmV0IGlzIHNlY3VyZQ==
        - name: key2
          secret: dGhpcyBpcyBwYXNzd29yZA==
    - aescbc:
        keys:
        - name: key1
          secret: c2VjcmV0IGlzIHNlY3VyZQ==
        - name: key2
          secret: dGhpcyBpcyBwYXNzd29yZA==
    - secretbox:
        keys:
        - name: key1
          secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=

每个 resources 数组项目是一个单独的完整的配置. resources.resources 字段是要加密的 Kubernetes 资源名称(resource 或 resource.group)的数组. providers 数组是可能的加密 provider 的有序列表. 每个条目只能指定一个 provider 类型(可以是 identity 或 aescbc,但不能在同一个项目中同时指定).

列表中的第一个 provider 用于加密进入存储的资源. 当从存储器读取资源时,与存储的数据匹配的所有 provider 将按顺序尝试解密数据. 如果由于格式或密钥不匹配而导致没有 provider 能够读取存储的数据,则会返回一个错误,以防止客户端访问该资源.

Caution: 重要: 如果通过加密配置无法读取资源(因为密钥已更改),唯一的方法是直接从底层 etcd 中删除该密钥. 任何尝试读取资源的调用将会失败,直到它被删除或提供有效的解密密钥.

* Providers:
名称	          加密类型	                              强度	                  速度	  密钥长度	                      其它事项
identity	      无	                                          N/A	                    N/A	    N/A	                                不加密写入的资源.当设置为第一个 provider 时,资源将在新值写                                                                                                                                                           入时被解密.
secretbox	  XSalsa20 和 Poly1305	        强	                      更快	  32字节	                          较新的标准,在需要高度评审的环境中可能不被接受.
aesgcm	      带有随机数的 AES-GCM	      必须每 200k      最快	  16, 24 或者 32字节	    建议不要使用,除非实施了自动密钥循环方案.
                                                                    写入一次	  
aescbc	        填充 PKCS#7 的 AES-CBC	    弱	                      快	       32字节	                          由于 CBC 容易受到密文填塞攻击(Padding Oracle Attack),不推荐使用
kms	            使用信封加密方案:数据    最强	                  快	       32字节	                          建议使用第三方工具进行密钥管理.为每个加密生成新的 DEK,并                           使用带有 PKCS#7 填充的                                                                                     由用户控制 KEK 轮换来简化密钥轮换.                                                  
                    AES-CBC 通过数据加密密
                    钥(DEK)加密,DEK 根据 Key 
                    Management Service(KMS)
                    中的配置通过密钥加密密
                    钥(Key Encryption Keys,KEK)
                    加密	

每个 provider 都支持多个密钥 - 在解密时会按顺序使用密钥,如果是第一个 provider,则第一个密钥用于加密.

在 EncryptionConfig 中保存原始的加密密钥与不加密相比只会略微地提升安全级别. 请使用 kms 驱动以获得更强的安全性. 默认情况下,identity 驱动被用来对 etcd 中的 Secret 提供保护, 而这个驱动不提供加密能力. EncryptionConfiguration 的引入是为了能够使用本地管理的密钥来在本地加密 Secret 数据.

使用本地管理的密钥来加密 Secret 能够保护数据免受 etcd 破坏的影响,不过无法针对 主机被侵入提供防护. 这是因为加密的密钥保存在主机上的 EncryptionConfig YAML 文件中,有经验的入侵者 仍能访问该文件并从中提取出加密密钥.

封套加密(Envelope Encryption)引入了对独立密钥的依赖,而这个密钥并不保存在 Kubernetes 中. 在这种情况下下,入侵者需要攻破 etcd、kube-apiserver 和第三方的 KMS 驱动才能获得明文数据,因而这种方案提供了比本地保存加密密钥更高的安全级别.

* 加密你的数据
创建一个新的加密配置文件:

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - aescbc:
        keys:
        - name: key1
          secret: <BASE 64 ENCODED SECRET>
    - identity: {}

遵循如下步骤来创建一个新的 secret:

  1. 生成一个 32 字节的随机密钥并进行 base64 编码.如果你在 Linux 或 Mac OS X 上,请运行以下命令:

       head -c 32 /dev/urandom | base64

  2. 将这个值放入到 secret 字段中.
  3. 设置 kube-apiserver 的 --experimental-encryption-provider-config 参数,将其指向 配置文件所在位置.
  4. 重启你的 API server.

Caution: 你的配置文件包含可以解密 etcd 内容的密钥,因此你必须正确限制主控节点的访问权限, 以便只有能运行 kube-apiserver 的用户才能读取它.

* 验证数据已被加密
数据在写入 etcd 时会被加密.重新启动你的 kube-apiserver 后,任何新创建或更新的密码在存储时都应该被加密. 如果想要检查,你可以使用 etcdctl 命令行程序来检索你的加密内容.

  1. 创建一个新的 secret,名称为 secret1,命名空间为 default:

      kubectl create secret generic secret1 -n default --from-literal=mykey=mydata
  
  2. 使用 etcdctl 命令行,从 etcd 中读取 secret:

      ETCDCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C

      这里的 [...] 是用来连接 etcd 服务的额外参数.

  3. 验证存储的密钥前缀是否为 k8s:enc:aescbc:v1:,这表明 aescbc provider 已加密结果数据.

  4. 通过 API 检索,验证 secret 是否被正确解密:

       kubectl describe secret secret1 -n default
  
       其输出应该是 mykey: bXlkYXRh,mydata 数据是被加密过的,请参阅 解密 Secret 了解如何完全解码 Secret 内容.

* 确保所有 Secret 都被加密
由于 Secret 是在写入时被加密,因此对 Secret 执行更新也会加密该内容.

kubectl get secrets --all-namespaces -o json | kubectl replace -f -

上面的命令读取所有 Secret,然后使用服务端加密来更新其内容.

Note: 如果由于冲突写入而发生错误,请重试该命令. 对于较大的集群,你可能希望通过命名空间或更新脚本来对 Secret 进行划分.

* 轮换解密密钥
在不发生停机的情况下更改 Secret 需要多步操作,特别是在有多个 kube-apiserver 进程正在运行的 高可用环境中.

  1. 生成一个新密钥并将其添加为所有服务器上当前提供程序的第二个密钥条目
  2. 重新启动所有 kube-apiserver 进程以确保每台服务器都可以使用新密钥进行解密
  3. 将新密钥设置为 keys 数组中的第一个条目,以便在配置中使用其进行加密
  4. 重新启动所有 kube-apiserver 进程以确保每个服务器现在都使用新密钥进行加密
  5. 运行 kubectl get secrets --all-namespaces -o json | kubectl replace -f - 以用新密钥加密所有现有的秘密
  6. 在使用新密钥备份 etcd 后,从配置中删除旧的解密密钥并更新所有密钥

如果只有一个 kube-apiserver,第 2 步可能可以忽略.

* 解密所有数据
要禁用 rest 加密,请将 identity provider 作为配置中的第一个条目:

apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
    - secrets
    providers:
    - identity: {}
    - aescbc:
        keys:
        - name: key1
          secret: <BASE 64 ENCODED SECRET>

并重新启动所有 kube-apiserver 进程.然后运行:

kubectl get secrets -all-namespaces -o json | kubectl replace -f -

以强制解密所有 secret.

################################################
##################### end ############################



## Pod 安全性标准
Pod 安全性标准定义了三种不同的 策略(Policy),以广泛覆盖安全应用场景. 这些策略是 渐进式的(Cumulative),安全级别从高度宽松至高度受限. 本指南概述了每个策略的要求.

Profile	描述
Privileged	不受限制的策略,提供最大可能范围的权限许可.此策略允许已知的特权提升.
Baseline	限制性最弱的策略,禁止已知的策略提升.允许使用默认的(规定最少)Pod 配置.
Restricted	限制性非常强的策略,遵循当前的保护 Pod 的最佳实践.

具体内容:
https://kubernetes.io/zh/docs/concepts/security/pod-security-standards/



## Pod 安全性准入
FEATURE STATE: Kubernetes v1.23 [beta]
Kubernetes Pod 安全性标准(Security Standards) 为 Pod 定义不同的隔离级别.这些标准能够让你以一种清晰、一致的方式定义如何限制 Pod 行为.

作为一项 Beta 功能特性,Kubernetes 提供一种内置的 Pod 安全性 准入控制器, 作为 PodSecurityPolicies 特性的后继演化版本.Pod 安全性限制是在 Pod 被创建时在 名字空间层面实施的.

Note:
PodSecurityPolicy API 已经被废弃,会在 Kubernetes v1.25 发行版中 移除.

具体内容:
https://kubernetes.io/zh/docs/concepts/security/pod-security-admission/

-------------------
# 策略
## 限制范围
默认情况下, Kubernetes 集群上的容器运行使用的计算资源没有限制. 使用资源配额,集群管理员可以以名字空间为单位,限制其资源的使用与创建. 在命名空间中,一个 Pod 或 Container 最多能够使用命名空间的资源配额所定义的 CPU 和内存用量. 有人担心,一个 Pod 或 Container 会垄断所有可用的资源. LimitRange 是在命名空间内限制资源分配(给多个 Pod 或 Container)的策略对象.

一个 LimitRange(限制范围) 对象提供的限制能够做到:

  * 在一个命名空间中实施对每个 Pod 或 Container 最小和最大的资源使用量的限制.
  * 在一个命名空间中实施对每个 PersistentVolumeClaim 能申请的最小和最大的存储空间大小的限制.
  * 在一个命名空间中实施对一种资源的申请值和限制值的比值的控制.
  * 设置一个命名空间中对计算资源的默认申请/限制值,并且自动的在运行时注入到多个 Container 中.


** 启用 LimitRange
对 LimitRange 的支持自 Kubernetes 1.10 版本默认启用.

LimitRange 支持在很多 Kubernetes 发行版本中也是默认启用的.

LimitRange 的名称必须是合法的 DNS 子域名.

* 限制范围总览
  * 管理员在一个命名空间内创建一个 LimitRange 对象.
  * 用户在命名空间内创建 Pod ,Container 和 PersistentVolumeClaim 等资源.
  * LimitRanger 准入控制器对所有没有设置计算资源需求的 Pod 和 Container 设置默认值与限制值, 并跟踪其使用量以保证没有超出命名空间中存在的任意 LimitRange 对象中的最小、最大资源使用量以及使用量比值.
  * 若创建或更新资源(Pod、 Container、PersistentVolumeClaim)违反了 LimitRange 的约束, 向 API 服务器的请求会失败,并返回 HTTP 状态码 403 FORBIDDEN 与描述哪一项约束被违反的消息.
  * 若命名空间中的 LimitRange 启用了对 cpu 和 memory 的限制, 用户必须指定这些值的需求使用量与限制使用量.否则,系统将会拒绝创建 Pod.
LimitRange 的验证仅在 Pod 准入阶段进行,不对正在运行的 Pod 进行验证.

能够使用限制范围创建的策略示例有:

  * 在一个有两个节点,8 GiB 内存与16个核的集群中,限制一个命名空间的 Pod 申请 100m 单位,最大 500m 单位的 CPU,以及申请 200Mi,最大 600Mi 的内存.
  * 为 spec 中没有 cpu 和内存需求值的 Container 定义默认 CPU 限制值与需求值 150m,内存默认需求值 300Mi.

在命名空间的总限制值小于 Pod 或 Container 的限制值的总和的情况下,可能会产生资源竞争. 在这种情况下,将不会创建 Container 或 Pod.

竞争和对 LimitRange 的改变都不会影响任何已经创建了的资源.



## 资源配额
当多个用户或团队共享具有固定节点数目的集群时,人们会担心有人使用超过其基于公平原则所分配到的资源量.

资源配额是帮助管理员解决这一问题的工具.

资源配额,通过 ResourceQuota 对象来定义,对每个命名空间的资源消耗总量提供限制. 它可以限制命名空间中某种类型的对象的总数目上限,也可以限制命令空间中的 Pod 可以使用的计算资源的总上限.

资源配额的工作方式如下:
  * 不同的团队可以在不同的命名空间下工作,目前这是非约束性的,在未来的版本中可能会通过 ACL (Access Control List 访问控制列表) 来实现强制性约束.
  * 集群管理员可以为每个命名空间创建一个或多个 ResourceQuota 对象.
  * 当用户在命名空间下创建资源(如 Pod、Service 等)时,Kubernetes 的配额系统会 跟踪集群的资源使用情况,以确保使用的资源用量不超过 ResourceQuota 中定义的硬性资源限额.
  * 如果资源创建或者更新请求违反了配额约束,那么该请求会报错(HTTP 403 FORBIDDEN), 并在消息中给出有可能违反的约束.
  * 如果命名空间下的计算资源 (如 cpu 和 memory)的配额被启用,则用户必须为 这些资源设定请求值(request)和约束值(limit),否则配额系统将拒绝 Pod 的创建. 提示: 可使用 LimitRanger 准入控制器来为没有设置计算资源需求的 Pod 设置默认值.

ResourceQuota 对象的名称必须是合法的 DNS 子域名.

下面是使用命名空间和配额构建策略的示例:

  * 在具有 32 GiB 内存和 16 核 CPU 资源的集群中,允许 A 团队使用 20 GiB 内存 和 10 核的 CPU 资源, 允许 B 团队使用 10 GiB 内存和 4 核的 CPU 资源,并且预留 2 GiB 内存和 2 核的 CPU 资源供将来分配.
  * 限制 "testing" 命名空间使用 1 核 CPU 资源和 1GiB 内存.允许 "production" 命名空间使用任意数量.

在集群容量小于各命名空间配额总和的情况下,可能存在资源竞争.资源竞争时,Kubernetes 系统会遵循先到先得的原则.

不管是资源竞争还是配额的修改,都不会影响已经创建的资源使用对象.


** 启用资源配额 
资源配额的支持在很多 Kubernetes 版本中是默认启用的. 当 API 服务器 的命令行标志 --enable-admission-plugins= 中包含 ResourceQuota 时, 资源配额会被启用.

当命名空间中存在一个 ResourceQuota 对象时,对于该命名空间而言,资源配额就是开启的.


** 计算资源配额
用户可以对给定命名空间下的可被请求的 计算资源 总量进行限制.

配额机制所支持的资源类型:

资源名称	描述
limits.cpu	                  所有非终止状态的 Pod,其 CPU 限额总量不能超过该值.
limits.memory	          所有非终止状态的 Pod,其内存限额总量不能超过该值.
requests.cpu	            所有非终止状态的 Pod,其 CPU 需求总量不能超过该值.
requests.memory	    所有非终止状态的 Pod,其内存需求总量不能超过该值.
hugepages-<size>	  对于所有非终止状态的 Pod,针对指定尺寸的巨页请求总数不能超过此值.
cpu	                            与 requests.cpu 相同.
memory	                    与 requests.memory 相同.

*扩展资源的资源配额
除上述资源外,在 Kubernetes 1.10 版本中,还添加了对 扩展资源 的支持.

由于扩展资源不可超量分配,因此没有必要在配额中为同一扩展资源同时指定 requests 和 limits. 对于扩展资源而言,目前仅允许使用前缀为 requests. 的配额项.

以 GPU 拓展资源为例,如果资源名称为 nvidia.com/gpu,并且要将命名空间中请求的 GPU 资源总数限制为 4,则可以如下定义配额:

  * requests.nvidia.com/gpu: 4


** 存储资源配额
用户可以对给定命名空间下的存储资源 总量进行限制.

此外,还可以根据相关的存储类(Storage Class)来限制存储资源的消耗.

资源名称	                                                                        描述
requests.storage	                                                            所有 PVC,存储资源的需求总量不能超过该值.
persistentvolumeclaims	                                                  在该命名空间中所允许的 PVC 总量.
<storage-class-name>.storageclass.storage.k8s.io/     在所有与 <storage-class-name> 相关的持久卷申领中,存储请求的总和不能超过该值.
requests.storage	                                                            
<storage-class-name>.storageclass.storage.k8s.io/     在与 storage-class-name 相关的所有持久卷申领中,命名空间中可以存在的持久卷申领总数
persistentvolumeclaims	                                                    

例如,如果一个操作人员针对 gold 存储类型与 bronze 存储类型设置配额, 操作人员可以定义如下配额:

  * gold.storageclass.storage.k8s.io/requests.storage: 500Gi
  * bronze.storageclass.storage.k8s.io/requests.storage: 100Gi

在 Kubernetes 1.8 版本中,本地临时存储的配额支持已经是 Alpha 功能:

资源名称	描述
requests.ephemeral-storage	    在命名空间的所有 Pod 中,本地临时存储请求的总和不能超过此值.
limits.ephemeral-storage	          在命名空间的所有 Pod 中,本地临时存储限制值的总和不能超过此值.
ephemeral-storage	                    与 requests.ephemeral-storage 相同.

Note:
如果所使用的是 CRI 容器运行时,容器日志会被计入临时存储配额. 这可能会导致存储配额耗尽的 Pods 被意外地驱逐出节点. 参考日志架构 了解详细信息.


** 对象数量配额
你可以使用以下语法对所有标准的、命名空间域的资源类型进行配额设置:

  * count/<resource>.<group>:用于非核心(core)组的资源
  * count/<resource>:用于核心组的资源

这是用户可能希望利用对象计数配额来管理的一组资源示例.

  * count/persistentvolumeclaims
  * count/services
  * count/secrets
  * count/configmaps
  * count/replicationcontrollers
  * count/deployments.apps
  * count/replicasets.apps
  * count/statefulsets.apps
  * count/jobs.batch
  * count/cronjobs.batch

相同语法也可用于自定义资源. 例如,要对 example.com API 组中的自定义资源 widgets 设置配额,请使用 count/widgets.example.com.

当使用 count/* 资源配额时,如果对象存在于服务器存储中,则会根据配额管理资源. 这些类型的配额有助于防止存储资源耗尽.例如,用户可能想根据服务器的存储能力来对服务器中 Secret 的数量进行配额限制. 集群中存在过多的 Secret 实际上会导致服务器和控制器无法启动. 用户可以选择对 Job 进行配额管理,以防止配置不当的 CronJob 在某命名空间中创建太多 Job 而导致集群拒绝服务.

对有限的一组资源上实施一般性的对象数量配额也是可能的. 此外,还可以进一步按资源的类型设置其配额.

支持以下类型:

资源名称	                        描述
configmaps	                      在该命名空间中允许存在的 ConfigMap 总数上限.
persistentvolumeclaims	  在该命名空间中允许存在的 PVC 的总数上限.
pods	                                在该命名空间中允许存在的非终止状态的 Pod 总数上限.Pod 终止状态等价于 Pod 的 .status.phase in (Failed, Succeeded) 为真.
replicationcontrollers	      在该命名空间中允许存在的 ReplicationController 总数上限.
resourcequotas	              在该命名空间中允许存在的 ResourceQuota 总数上限.
services	                            在该命名空间中允许存在的 Service 总数上限.
services.loadbalancers	    在该命名空间中允许存在的 LoadBalancer 类型的 Service 总数上限.
services.nodeports	          在该命名空间中允许存在的 NodePort 类型的 Service 总数上限.
secrets	                            在该命名空间中允许存在的 Secret 总数上限.

例如,pods 配额统计某个命名空间中所创建的、非终止状态的 Pod 个数并确保其不超过某上限值. 用户可能希望在某命名空间中设置 pods 配额,以避免有用户创建很多小的 Pod, 从而耗尽集群所能提供的 Pod IP 地址.


** 配额作用域 
每个配额都有一组相关的 scope(作用域),配额只会对作用域内的资源生效. 配额机制仅统计所列举的作用域的交集中的资源用量.

当一个作用域被添加到配额中后,它会对作用域相关的资源数量作限制. 如配额中指定了允许(作用域)集合之外的资源,会导致验证错误.

作用域	                                      描述
Terminating	                              匹配所有 spec.activeDeadlineSeconds 不小于 0 的 Pod.
NotTerminating	                        匹配所有 spec.activeDeadlineSeconds 是 nil 的 Pod.
BestEffort	                                  匹配所有 Qos 是 BestEffort 的 Pod.
NotBestEffort	                          匹配所有 Qos 不是 BestEffort 的 Pod.
PriorityClass	                              匹配所有引用了所指定的优先级类的 Pods.
CrossNamespacePodAffinity	    匹配那些设置了跨名字空间 (反)亲和性条件的 Pod.

BestEffort 作用域限制配额跟踪以下资源:
  * pods

Terminating、NotTerminating、NotBestEffort 和 PriorityClass 这些作用域限制配额跟踪以下资源:
  * pods
  * cpu
  * memory
  * requests.cpu
  * requests.memory
  * limits.cpu
  * limits.memory

需要注意的是,你不可以在同一个配额对象中同时设置 Terminating 和 NotTerminating 作用域,你也不可以在同一个配额中同时设置 BestEffort 和 NotBestEffort 作用域.

scopeSelector 支持在 operator 字段中使用以下值:
  * In
  * NotIn
  * Exists
  * DoesNotExist

定义 scopeSelector 时,如果使用以下值之一作为 scopeName 的值,则对应的 operator 只能是 Exists.
  * Terminating
  * NotTerminating
  * BestEffort
  * NotBestEffort

如果 operator 是 In 或 NotIn 之一,则 values 字段必须至少包含一个值. 例如:

  scopeSelector:
    matchExpressions:
      - scopeName: PriorityClass
        operator: In
        values:
          - middle

如果 operator 为 Exists 或 DoesNotExist,则不可以设置 values 字段.

* 基于优先级类(PriorityClass)来设置资源配额
FEATURE STATE: Kubernetes v1.17 [stable]

Pod 可以创建为特定的优先级. 通过使用配额规约中的 scopeSelector 字段,用户可以根据 Pod 的优先级控制其系统资源消耗.

仅当配额规范中的 scopeSelector 字段选择到某 Pod 时,配额机制才会匹配和计量 Pod 的资源消耗.

如果配额对象通过 scopeSelector 字段设置其作用域为优先级类,则配额对象只能 跟踪以下资源:

  * pods
  * cpu
  * memory
  * ephemeral-storage
  * limits.cpu
  * limits.memory
  * limits.ephemeral-storage
  * requests.cpu
  * requests.memory
  * requests.ephemeral-storage

本示例创建一个配额对象,并将其与具有特定优先级的 Pod 进行匹配. 该示例的工作方式如下:
  
  * 集群中的 Pod 可取三个优先级类之一,即 "low"、"medium"、"high".
  * 为每个优先级创建一个配额对象.

将以下 YAML 保存到文件 quota.yml 中.

apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-high
  spec:
    hard:
      cpu: "1000"
      memory: 200Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["high"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-medium
  spec:
    hard:
      cpu: "10"
      memory: 20Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["medium"]
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: pods-low
  spec:
    hard:
      cpu: "5"
      memory: 10Gi
      pods: "10"
    scopeSelector:
      matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["low"]

使用 kubectl create 命令运行以下操作.

kubectl create -f ./quota.yml

resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created

使用 kubectl describe quota 操作验证配额的 Used 值为 0.

kubectl describe quota

Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10

创建优先级为 "high" 的 Pod. 将以下 YAML 保存到文件 high-priority-pod.yml 中.

apiVersion: v1
kind: Pod
metadata:
  name: high-priority
spec:
  containers:
  - name: high-priority
    image: ubuntu
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo hello; sleep 10;done"]
    resources:
      requests:
        memory: "10Gi"
        cpu: "500m"
      limits:
        memory: "10Gi"
        cpu: "500m"
  priorityClassName: high

使用 kubectl create 运行以下操作.

kubectl create -f ./high-priority-pod.yml

确认 "high" 优先级配额 pods-high 的 "Used" 统计信息已更改,并且其他两个配额未更改.

kubectl describe quota
Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10


** 跨名字空间的 Pod 亲和性配额 
FEATURE STATE: Kubernetes v1.22 [beta]

集群运维人员可以使用 CrossNamespacePodAffinity 配额作用域来 限制哪个名字空间中可以存在包含跨名字空间亲和性规则的 Pod. 更为具体一点,此作用域用来配置哪些 Pod 可以在其 Pod 亲和性规则 中设置 namespaces 或 namespaceSelector 字段.

禁止用户使用跨名字空间的亲和性规则可能是一种被需要的能力,因为带有 反亲和性约束的 Pod 可能会阻止所有其他名字空间的 Pod 被调度到某失效域中.

使用此作用域操作符可以避免某些名字空间(例如下面例子中的 foo-ns)运行 特别的 Pod,这类 Pod 使用跨名字空间的 Pod 亲和性约束,在该名字空间中创建 了作用域为 CrossNamespaceAffinity 的、硬性约束为 0 的资源配额对象.

apiVersion: v1
kind: ResourceQuota
metadata:
  name: disable-cross-namespace-affinity
  namespace: foo-ns
spec:
  hard:
    pods: "0"
  scopeSelector:
    matchExpressions:
    - scopeName: CrossNamespaceAffinity

如果集群运维人员希望默认禁止使用 namespaces 和 namespaceSelector,而 仅仅允许在特定名字空间中这样做,他们可以将 CrossNamespaceAffinity 作为一个 被约束的资源.方法是为 kube-apiserver 设置标志 --admission-control-config-file,使之指向如下的配置文件:

apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: "ResourceQuota"
  configuration:
    apiVersion: apiserver.config.k8s.io/v1
    kind: ResourceQuotaConfiguration
    limitedResources:
    - resource: pods
      matchScopes:
      - scopeName: CrossNamespaceAffinity

基于上面的配置,只有名字空间中包含作用域为 CrossNamespaceAffinity 且 硬性约束大于或等于使用 namespaces 和 namespaceSelector 字段的 Pods 个数时,才可以在该名字空间中继续创建在其 Pod 亲和性规则中设置 namespaces 或 namespaceSelector 的新 Pod.

此功能特性处于 Beta 阶段,默认被禁用.你可以通过为 kube-apiserver 和 kube-scheduler 设置 特性门控 PodAffinityNamespaceSelector 来启用此特性.


** 请求与限制的比较 
分配计算资源时,每个容器可以为 CPU 或内存指定请求和约束. 配额可以针对二者之一进行设置.

如果配额中指定了 requests.cpu 或 requests.memory 的值,则它要求每个容器都显式给出对这些资源的请求. 同理,如果配额中指定了 limits.cpu 或 limits.memory 的值,那么它要求每个容器都显式设定对应资源的限制.


** 查看和设置配额
Kubectl 支持创建、更新和查看配额:

kubectl create namespace myspace

cat <<EOF > compute-resources.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    requests.nvidia.com/gpu: 4
EOF

kubectl create -f ./compute-resources.yaml --namespace=myspace

cat <<EOF > object-counts.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    pods: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
EOF

kubectl create -f ./object-counts.yaml --namespace=myspace

kubectl get quota --namespace=myspace
NAME                    AGE
compute-resources       30s
object-counts           32s

kubectl describe quota compute-resources --namespace=myspace

Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4

kubectl describe quota object-counts --namespace=myspace

Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2

kubectl 还使用语法 count/<resource>.<group> 支持所有标准的、命名空间域的资源的对象计数配额:

kubectl create namespace myspace
kubectl create quota test --hard=count/deployments.apps=2,count/replicasets.apps=4,count/pods=3,count/secrets=4 --namespace=myspace
kubectl create deployment nginx --image=nginx --namespace=myspace --replicas=2

kubectl describe quota --namespace=myspace
Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4


** 配额和集群容量 
ResourceQuota 与集群资源总量是完全独立的.它们通过绝对的单位来配置. 所以,为集群添加节点时,资源配额不会自动赋予每个命名空间消耗更多资源的能力.

有时可能需要资源配额支持更复杂的策略,比如:

  * 在几个团队中按比例划分总的集群资源.
  * 允许每个租户根据需要增加资源使用量,但要有足够的限制以防止资源意外耗尽.
  * 探测某个命名空间的需求,添加物理节点并扩大资源配额值.

这些策略可以通过将资源配额作为一个组成模块、手动编写一个控制器来监控资源使用情况, 并结合其他信号调整命名空间上的硬性资源配额来实现.

注意:资源配额对集群资源总体进行划分,但它对节点没有限制:来自不同命名空间的 Pod 可能在同一节点上运行.


** 默认情况下限制特定优先级的资源消耗
有时候可能希望当且仅当某名字空间中存在匹配的配额对象时,才可以创建特定优先级 (例如 "cluster-services")的 Pod.

通过这种机制,操作人员能够将限制某些高优先级类仅出现在有限数量的命名空间中, 而并非每个命名空间默认情况下都能够使用这些优先级类.

要实现此目的,应设置 kube-apiserver 的标志 --admission-control-config-file 指向如下配置文件:

apiVersion: apiserver.config.k8s.io/v1
kind: AdmissionConfiguration
plugins:
- name: "ResourceQuota"
  configuration:
    apiVersion: apiserver.config.k8s.io/v1
    kind: ResourceQuotaConfiguration
    limitedResources:
    - resource: pods
      matchScopes:
      - scopeName: PriorityClass
        operator: In
        values: ["cluster-services"]

现在在 kube-system 名字空间中创建一个资源配额对象:

policy/priority-class-resourcequota.yaml 

apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-cluster-services
spec:
  scopeSelector:
    matchExpressions:
      - operator : In
        scopeName: PriorityClass
        values: ["cluster-services"]

kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
resourcequota/pods-cluster-services created

在这里,当以下条件满足时可以创建 Pod:
  * Pod 未设置 priorityClassName
  * Pod 的 priorityClassName 设置值不是 cluster-services
  * Pod 的 priorityClassName 设置值为 cluster-services,它将被创建于 kube-system 名字空间中,并且它已经通过了资源配额检查.

如果 Pod 的 priorityClassName 设置为 cluster-services,但要被创建到 kube-system 之外的别的名字空间,则 Pod 创建请求也被拒绝.



## 进程 ID 约束与预留
FEATURE STATE: Kubernetes v1.20 [stable]
Kubernetes 允许你限制一个 Pod 中可以使用的 进程 ID(PID)数目.你也可以为每个 节点 预留一定数量的可分配的 PID,供操作系统和守护进程(而非 Pod)使用.

进程 ID(PID)是节点上的一种基础资源.很容易就会在尚未超出其它资源约束的时候就 已经触及任务个数上限,进而导致宿主机器不稳定.

集群管理员需要一定的机制来确保集群中运行的 Pod 不会导致 PID 资源枯竭,甚而 造成宿主机上的守护进程(例如 kubelet 或者 kube-proxy 乃至包括容器运行时本身)无法正常运行. 此外,确保 Pod 中 PID 的个数受限对于保证其不会影响到同一节点上其它负载也很重要.

Note:
在某些 Linux 安装环境中,操作系统会将 PID 约束设置为一个较低的默认值,例如 32768.这时可以考虑提升 /proc/sys/kernel/pid_max 的设置值.

你可以配置 kubelet 限制给定 Pod 能够使用的 PID 个数. 例如,如果你的节点上的宿主操作系统被设置为最多可使用 262144 个 PID,同时预期 节点上会运行的 Pod 个数不会超过 250,那么你可以为每个 Pod 设置 1000 个 PID 的预算,避免耗尽该节点上可用 PID 的总量. 如果管理员系统像 CPU 或内存那样允许对 PID 进行过量分配(Overcommit),他们也可以 这样做,只是会有一些额外的风险.不管怎样,任何一个 Pod 都不可以将整个机器的运行 状态破坏.这类资源限制有助于避免简单的派生炸弹(Fork Bomb)影响到整个集群的运行.

在 Pod 级别设置 PID 限制使得管理员能够保护 Pod 之间不会互相伤害,不过无法 确保所有调度到该宿主机器上的所有 Pod 都不会影响到节点整体. Pod 级别的限制也无法保护节点代理任务自身不会受到 PID 耗尽的影响.

你也可以预留一定量的 PID,作为节点的额外开销,与分配给 Pod 的 PID 集合独立. 这有点类似于在给操作系统和其它设施预留 CPU、内存或其它资源时所做的操作, 这些任务都在 Pod 及其所包含的容器之外运行.

PID 限制是与计算资源 请求和限制相辅相成的一种机制.不过,你需要用一种不同的方式来设置这一限制: 你需要将其设置到 kubelet 上而不是在 Pod 的 .spec 中为 Pod 设置资源限制. 目前还不支持在 Pod 级别设置 PID 限制.

Caution:
这意味着,施加在 Pod 之上的限制值可能因为 Pod 运行所在的节点不同而有差别. 为了简化系统,最简单的方法是为所有节点设置相同的 PID 资源限制和预留值.


** 节点级别 PID 限制 
Kubernetes 允许你为系统预留一定量的进程 ID.为了配置预留数量,你可以使用 kubelet 的 --system-reserved 和 --kube-reserved 命令行选项中的参数 pid=<number>.你所设置的参数值分别用来声明为整个系统和 Kubernetes 系统 守护进程所保留的进程 ID 数目.

Note:
在 Kubernetes 1.20 版本之前,在节点级别通过 PID 资源限制预留 PID 的能力 需要启用特性门控 SupportNodePidsLimit 才行.


** Pod 级别 PID 限制 
Kubernetes 允许你限制 Pod 中运行的进程个数.你可以在节点级别设置这一限制, 而不是为特定的 Pod 来将其设置为资源限制. 每个节点都可以有不同的 PID 限制设置. 要设置限制值,你可以设置 kubelet 的命令行参数 --pod-max-pids,或者 在 kubelet 的配置文件 中设置 PodPidsLimit.

Note:
在 Kubernetes 1.20 版本之前,为 Pod 设置 PID 资源限制的能力需要启用 特性门控 SupportNodePidsLimit 才行.


** 基于 PID 的驱逐 
你可以配置 kubelet 使之在 Pod 行为不正常或者消耗不正常数量资源的时候将其终止. 这一特性称作驱逐.你可以针对不同的驱逐信号 配置资源不足的处理. 使用 pid.available 驱逐信号来配置 Pod 使用的 PID 个数的阈值. 你可以设置硬性的和软性的驱逐策略.不过,即使使用硬性的驱逐策略, 如果 PID 个数增长过快,节点仍然可能因为触及节点 PID 限制而进入一种不稳定状态. 驱逐信号的取值是周期性计算的,而不是一直能够强制实施约束.

Pod 级别和节点级别的 PID 限制会设置硬性限制. 一旦触及限制值,工作负载会在尝试获得新的 PID 时开始遇到问题. 这可能会也可能不会导致 Pod 被重新调度,取决于工作负载如何应对这类失败 以及 Pod 的存活性和就绪态探测是如何配置的. 可是,如果限制值被正确设置,你可以确保其它 Pod 负载和系统进程不会因为某个 Pod 行为不正常而没有 PID 可用.



## 节点资源管理器
Kubernetes 提供了一组资源管理器,用于支持延迟敏感的、高吞吐量的工作负载. 资源管理器的目标是协调和优化节点资源,以支持对 CPU、设备和内存(巨页)等资源有特殊需求的 Pod.

主管理器,也叫拓扑管理器(Topology Manager),是一个 Kubelet 组件, 它通过策略, 协调全局的资源管理过程.

-------------------
# 调度、抢占和驱逐
## Kubernetes 调度器
在 Kubernetes 中,调度 是指将 Pod 放置到合适的 Node 上,然后对应 Node 上的 Kubelet 才能够运行这些 pod.


** 调度概览
调度器通过 kubernetes 的监测(Watch)机制来发现集群中新创建且尚未被调度到 Node 上的 Pod. 调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行. 调度器会依据下文的调度原则来做出调度选择.

如果你想要理解 Pod 为什么会被调度到特定的 Node 上,或者你想要尝试实现 一个自定义的调度器,这篇文章将帮助你了解调度.


** kube-scheduler
kube-scheduler 是 Kubernetes 集群的默认调度器,并且是集群 控制面 的一部分. 如果你真的希望或者有这方面的需求,kube-scheduler 在设计上是允许 你自己写一个调度组件并替换原有的 kube-scheduler.

对每一个新创建的 Pod 或者是未被调度的 Pod,kube-scheduler 会选择一个最优的 Node 去运行这个 Pod.然而,Pod 内的每一个容器对资源都有不同的需求,而且 Pod 本身也有不同的资源需求.因此,Pod 在被调度到 Node 上之前, 根据这些特定的资源调度需求,需要对集群中的 Node 进行一次过滤.

在一个集群中,满足一个 Pod 调度请求的所有 Node 称之为 可调度节点. 如果没有任何一个 Node 能满足 Pod 的资源请求,那么这个 Pod 将一直停留在 未调度状态直到调度器能够找到合适的 Node.

调度器先在集群中找到一个 Pod 的所有可调度节点,然后根据一系列函数对这些可调度节点打分, 选出其中得分最高的 Node 来运行 Pod.之后,调度器将这个调度决定通知给 kube-apiserver,这个过程叫做 绑定.

在做调度决定时需要考虑的因素包括:单独和整体的资源请求、硬件/软件/策略限制、 亲和以及反亲和要求、数据局域性、负载间的干扰等等.


** kube-scheduler 调度流程
kube-scheduler 给一个 pod 做调度选择包含两个步骤:

  1. 过滤
  2. 打分

过滤阶段会将所有满足 Pod 调度需求的 Node 选出来. 例如,PodFitsResources 过滤函数会检查候选 Node 的可用资源能否满足 Pod 的资源请求. 在过滤之后,得出一个 Node 列表,里面包含了所有可调度节点；通常情况下, 这个 Node 列表包含不止一个 Node.如果这个列表是空的,代表这个 Pod 不可调度.

在打分阶段,调度器会为 Pod 从所有可调度节点中选取一个最合适的 Node. 根据当前启用的打分规则,调度器会给每一个可调度节点进行打分.

最后,kube-scheduler 会将 Pod 调度到得分最高的 Node 上. 如果存在多个得分最高的 Node,kube-scheduler 会从中随机选取一个.

支持以下两种方式配置调度器的过滤和打分行为:

  1. 调度策略 允许你配置过滤的 断言(Predicates) 和打分的 优先级(Priorities) .
  2. 调度配置 允许你配置实现不同调度阶段的插件, 包括:QueueSort, Filter, Score, Bind, Reserve, Permit 等等. 你也可以配置 kube-scheduler 运行不同的配置文件.

调度策略
在 Kubernetes v1.23 版本之前,可以使用调度策略来指定 predicates 和 priorities 进程. 例如,可以通过运行 kube-scheduler --policy-config-file <filename> 或者 kube-scheduler --policy-configmap <ConfigMap> 设置调度策略.

但是从 Kubernetes v1.23 版本开始,不再支持这种调度策略. 同样地也不支持相关的 policy-config-file、 policy-configmap、 policy-configmap-namespace 以及 use-legacy-policy-config 标志. 你可以通过使用 调度配置来实现类似的行为.

调度配置具体内容
https://kubernetes.io/zh/docs/reference/scheduling/config/#profiles



## 将 Pod 指派给节点
你可以约束一个 Pod 只能在特定的节点上运行. 有几种方法可以实现这点,推荐的方法都是用 标签选择算符来进行选择. 通常这样的约束不是必须的,因为调度器将自动进行合理的放置(比如,将 Pod 分散到节点上, 而不是将 Pod 放置在可用资源不足的节点上等等).但在某些情况下,你可能需要进一步控制 Pod 被部署到的节点.例如,确保 Pod 最终落在连接了 SSD 的机器上, 或者将来自两个不同的服务且有大量通信的 Pods 被放置在同一个可用区.

你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度:

  * 与节点标签匹配的 nodeSelector
  * 亲和性与反亲和性
  * nodeName 字段


** 节点标签 
与很多其他 Kubernetes 对象类似,节点也有标签. 你可以手动地添加标签. Kubernetes 也会为集群中所有节点添加一些标准的标签. 参见常用的标签、注解和污点以了解常见的节点标签.

Note:
这些标签的取值是取决于云提供商的,并且是无法在可靠性上给出承诺的. 例如,kubernetes.io/hostname 的取值在某些环境中可能与节点名称相同, 而在其他环境中会取不同的值.


** 节点隔离/限制 
通过为节点添加标签,你可以准备让 Pod 调度到特定节点或节点组上. 你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性,安全性或监管属性的节点上.

如果使用标签来实现节点隔离,建议选择节点上的 kubelet 无法修改的标签键. 这可以防止受感染的节点在自身上设置这些标签,进而影响调度器将工作负载调度到受感染的节点.

NodeRestriction 准入插件防止 kubelet 使用 node-restriction.kubernetes.io/ 前缀设置或修改标签.

要使用该标签前缀进行节点隔离:

  1. 确保你在使用节点鉴权机制并且已经启用了 NodeRestriction 准入插件.
  2. 将带有 node-restriction.kubernetes.io/ 前缀的标签添加到 Node 对象, 然后在节点选择器中使用这些标签. 例如,example.com.node-restriction.kubernetes.io/fips=true 或 example.com.node-restriction.kubernetes.io/pci-dss=true.


** nodeSelector
nodeSelector 是节点选择约束的最简单推荐形式.你可以将 nodeSelector 字段添加到 Pod 的规约中设置你希望目标节点所具有的节点标签. Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上.


** 亲和性与反亲和性 
nodeSelector 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上. 亲和性和反亲和性扩展了你可以定义的约束类型.使用亲和性与反亲和性的一些好处有:
  * 亲和性、反亲和性语言的表达能力更强.nodeSelector 只能选择拥有所有指定标签的节点. 亲和性、反亲和性为你提供对选择逻辑的更强控制能力.
  * 你可以标明某规则是“软需求”或者“偏好”,这样调度器在无法找到匹配节点时仍然调度该 Pod.
  * 你可以使用节点上(或其他拓扑域中)运行的其他 Pod 的标签来实施调度约束, 而不是只能使用节点本身的标签.这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起.

* 节点亲和性 
节点亲和性概念上类似于 nodeSelector, 它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上. 节点亲和性有两种:
  * requiredDuringSchedulingIgnoredDuringExecution: 调度器只有在规则被满足的时候才能执行调度.此功能类似于 nodeSelector, 但其语法表达能力更强.
  * preferredDuringSchedulingIgnoredDuringExecution: 调度器会尝试寻找满足对应规则的节点.如果找不到匹配的节点,调度器仍然会调度该 Pod.

Note:
在上述类型中,IgnoredDuringExecution 意味着如果节点标签在 Kubernetes 调度 Pod 时发生了变更,Pod 仍将继续运行.

你可以使用 Pod 规约中的 .spec.affinity.nodeAffinity 字段来设置节点亲和性. 例如,考虑下面的 Pod 规约:

pods/pod-with-node-affinity.yaml 

kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0

在这一示例中,所应用的规则如下:

  * 节点必须包含键名为 kubernetes.io/os 的标签,并且其取值为 linux.
  * 节点 最好 具有键名为 another-node-label-key 且取值为 another-node-label-value 的标签.

你可以使用 operator 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符. 你可以使用 In、NotIn、Exists、DoesNotExist、Gt 和 Lt 之一作为操作符.

NotIn 和 DoesNotExist 可用来实现节点反亲和性行为. 你也可以使用节点污点 将 Pod 从特定节点上驱逐.

Note:
如果你同时指定了 nodeSelector 和 nodeAffinity,两者 必须都要满足, 才能将 Pod 调度到候选节点上.

如果你指定了多个与 nodeAffinity 类型关联的 nodeSelectorTerms, 只要其中一个 nodeSelectorTerms 满足的话,Pod 就可以被调度到节点上.

如果你指定了多个与同一 nodeSelectorTerms 关联的 matchExpressions, 则只有当所有 matchExpressions 都满足时 Pod 才可以被调度到节点上.

节点亲和性权重 
你可以为 preferredDuringSchedulingIgnoredDuringExecution 亲和性类型的每个实例设置 weight 字段,其取值范围是 1 到 100. 当调度器找到能够满足 Pod 的其他调度请求的节点时,调度器会遍历节点满足的所有的偏好性规则, 并将对应表达式的 weight 值加和.

最终的加和值会添加到该节点的其他优先级函数的评分之上. 在调度器为 Pod 作出调度决定时,总分最高的节点的优先级也最高.

例如,考虑下面的 Pod 规约:

pods/pod-with-affinity-anti-affinity.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: with-affinity-anti-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: label-1
            operator: In
            values:
            - key-1
      - weight: 50
        preference:
          matchExpressions:
          - key: label-2
            operator: In
            values:
            - key-2
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:2.0

如果存在两个候选节点,都满足 requiredDuringSchedulingIgnoredDuringExecution 规则, 其中一个节点具有标签 label-1:key-1,另一个节点具有标签 label-2:key-2, 调度器会考察各个节点的 weight 取值,并将该权重值添加到节点的其他得分值之上,

Note:
如果你希望 Kubernetes 能够成功地调度此例中的 Pod,你必须拥有打了 kubernetes.io/os=linux 标签的节点.

逐个调度方案中设置节点亲和性 
FEATURE STATE: Kubernetes v1.20 [beta]

在配置多个调度方案时, 你可以将某个方案与节点亲和性关联起来,如果某个调度方案仅适用于某组特殊的节点时, 这样做是很有用的. 要实现这点,可以在调度器配置中为 NodeAffinity 插件的 args 字段添加 addedAffinity.例如:

apiVersion: kubescheduler.config.k8s.io/v1beta3
kind: KubeSchedulerConfiguration

profiles:
  - schedulerName: default-scheduler
  - schedulerName: foo-scheduler
    pluginConfig:
      - name: NodeAffinity
        args:
          addedAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: scheduler-profile
                  operator: In
                  values:
                  - foo

这里的 addedAffinity 除遵从 Pod 规约中设置的节点亲和性之外,还 适用于将 .spec.schedulerName 设置为 foo-scheduler. 换言之,为了匹配 Pod,节点需要满足 addedAffinity 和 Pod 的 .spec.NodeAffinity.

由于 addedAffinity 对最终用户不可见,其行为可能对用户而言是出乎意料的. 应该使用与调度方案名称有明确关联的节点标签.

Note:
DaemonSet 控制器为 DaemonSet 创建 Pods, 但该控制器不理会调度方案. DaemonSet 控制器创建 Pod 时,默认的 Kubernetes 调度器负责放置 Pod, 并遵从 DaemonSet 控制器中奢侈的 nodeAffinity 规则.

* pod 间亲和性与反亲和性 
Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 Pod 的标签来约束 Pod 可以调度到的节点,而不是基于节点上的标签.

Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod, 则这个 Pod 应该(或者在反亲和性的情况下不应该)运行在 X 上”. 这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域, Y 则是 Kubernetes 尝试满足的规则.

你通过标签选择算符 的形式来表达规则(Y),并可根据需要指定选关联的名字空间列表. Pod 在 Kubernetes 中是名字空间作用域的对象,因此 Pod 的标签也隐式地具有名字空间属性. 针对 Pod 标签的所有标签选择算符都要指定名字空间,Kubernetes 会在指定的名字空间内寻找标签.

你会通过 topologyKey 来表达拓扑域(X)的概念,其取值是系统用来标示域的节点标签键. 

Note:
Pod 间亲和性和反亲和性都需要相当的计算量,因此会在大规模集群中显著降低调度速度. 我们不建议在包含数百个节点的集群中使用这类设置.

Note:
Pod 反亲和性需要节点上存在一致性的标签.换言之, 集群中每个节点都必须拥有与 topologyKey 匹配的标签. 如果某些或者所有节点上不存在所指定的 topologyKey 标签,调度行为可能与预期的不同.

Pod 间亲和性与反亲和性的类型
与节点亲和性类似,Pod 的亲和性与反亲和性也有两种类型:
  * requiredDuringSchedulingIgnoredDuringExecution
  * preferredDuringSchedulingIgnoredDuringExecution

例如,你可以使用 requiredDuringSchedulingIgnoredDuringExecution 亲和性来告诉调度器, 将两个服务的 Pod 放到同一个云提供商可用区内,因为它们彼此之间通信非常频繁. 类似地,你可以使用 preferredDuringSchedulingIgnoredDuringExecution 反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中.

要使用 Pod 间亲和性,可以使用 Pod 规约中的 .affinity.podAffinity 字段. 对于 Pod 间反亲和性,可以使用 Pod 规约中的 .affinity.podAntiAffinity 字段.

Pod 亲和性示例 
考虑下面的 Pod 规约:

pods/pod-with-pod-affinity.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: k8s.gcr.io/pause:2.0

本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则.Pod 亲和性规则配置为 requiredDuringSchedulingIgnoredDuringExecution,而 Pod 反亲和性配置为 preferredDuringSchedulingIgnoredDuringExecution.

亲和性规则表示,仅当节点和至少一个已运行且有 security=S1 的标签的 Pod 处于同一区域时,才可以将该 Pod 调度到节点上. 更确切的说,调度器必须将 Pod 调度到具有 topology.kubernetes.io/zone=V 标签的节点上,并且集群中至少有一个位于该可用区的节点上运行着带有 security=S1 标签的 Pod.

反亲和性规则表示,如果节点处于 Pod 所在的同一可用区且至少一个 Pod 具有 security=S2 标签,则该 Pod 不应被调度到该节点上. 更确切地说, 如果同一可用区中存在其他运行着带有 security=S2 标签的 Pod 节点, 并且节点具有标签 topology.kubernetes.io/zone=R,Pod 不能被调度到该节点上.

你可以针对 Pod 间亲和性与反亲和性为其 operator 字段使用 In、NotIn、Exists、 DoesNotExist 等值.

原则上,topologyKey 可以是任何合法的标签键.出于性能和安全原因,topologyKey 有一些限制:
  * 对于 Pod 亲和性而言,在 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 中,topologyKey 不允许为空.
  * 对于 requiredDuringSchedulingIgnoredDuringExecution 要求的 Pod 反亲和性, 准入控制器 LimitPodHardAntiAffinityTopology 要求 topologyKey 只能是 kubernetes.io/hostname.如果你希望使用其他定制拓扑逻辑, 你可以更改准入控制器或者禁用之.

除了 labelSelector 和 topologyKey,你也可以指定 labelSelector 要匹配的命名空间列表,方法是在 labelSelector 和 topologyKey 所在层同一层次上设置 namespaces. 如果 namespaces 被忽略或者为空,则默认为 Pod 亲和性/反亲和性的定义所在的命名空间.

名字空间选择算符 
FEATURE STATE: Kubernetes v1.22 [beta]

用户也可以使用 namespaceSelector 选择匹配的名字空间,namespaceSelector 是对名字空间集合进行标签查询的机制. 亲和性条件会应用到 namespaceSelector 所选择的名字空间和 namespaces 字段中 所列举的名字空间之上. 注意,空的 namespaceSelector({})会匹配所有名字空间,而 null 或者空的 namespaces 列表以及 null 值 namespaceSelector 意味着“当前 Pod 的名字空间”.

Note:
此功能特性是 Beta 版本的,默认是被启用的.你可以通过针对 kube-apiserver 和 kube-scheduler 设置特性门控 PodAffinityNamespaceSelector 来禁用此特性.

更实际的用例
Pod 间亲和性与反亲和性在与更高级别的集合(例如 ReplicaSet、StatefulSet、 Deployment 等)一起使用时,它们可能更加有用. 这些规则使得你可以配置一组工作负载,使其位于相同定义拓扑(例如,节点)中.

在下面的 Redis 缓存 Deployment 示例中,副本上设置了标签 app=store. podAntiAffinity 规则告诉调度器避免将多个带有 app=store 标签的副本部署到同一节点上. 因此,每个独立节点上会创建一个缓存实例.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:3.2-alpine

下面的 Deployment 用来提供 Web 服务器服务,会创建带有标签 app=web-store 的副本. Pod 亲和性规则告诉调度器将副本放到运行有标签包含 app=store Pod 的节点上. Pod 反亲和性规则告诉调度器不要在同一节点上放置多个 app=web-store 的服务器.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-store
  replicas: 3
  template:
    metadata:
      labels:
        app: web-store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-store
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:1.16-alpine

创建前面两个 Deployment 会产生如下的集群布局,每个 Web 服务器与一个缓存实例并置, 并分别运行在三个独立的节点上.

node-1	        node-2	        node-3
webserver-1	webserver-2	webserver-3

cache-1	        cache-2	        cache-3

参阅 ZooKeeper 教程 了解一个 StatefulSet 的示例,该 StatefulSet 配置了反亲和性以实现高可用, 所使用的是与此例相同的技术.

ZooKeeper 教程
https://kubernetes.io/zh/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure


** nodeName
nodeName 是比亲和性或者 nodeSelector 更为直接的形式.nodeName 是 Pod 规约中的一个字段.如果 nodeName 字段不为空,调度器会忽略该 Pod, 而指定节点上的 kubelet 会尝试将 Pod 放到该节点上. 使用 nodeName 规则的优先级会高于使用 nodeSelector 或亲和性与非亲和性的规则.

使用 nodeName 来选择节点的方式有一些局限性:
  * 如果所指代的节点不存在,则 Pod 无法运行,而且在某些情况下可能会被自动删除.
  * 如果所指代的节点无法提供用来运行 Pod 所需的资源,Pod 会失败, 而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行.
  * 在云环境中的节点名称并不总是可预测的,也不总是稳定的.

下面是一个使用 nodeName 字段的 Pod 规约示例:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx
  nodeName: kube-01

上面的 Pod 只能运行在节点 kube-01 之上.



## Pod 开销
FEATURE STATE: Kubernetes v1.18 [beta]

在节点上运行 Pod 时,Pod 本身占用大量系统资源.这些是运行 Pod 内容器所需资源之外的资源. POD 开销 是一个特性,用于计算 Pod 基础设施在容器请求和限制之上消耗的资源.

在 Kubernetes 中,Pod 的开销是根据与 Pod 的 RuntimeClass 相关联的开销在准入时设置的.

如果启用了 Pod Overhead,在调度 Pod 时,除了考虑容器资源请求的总和外,还要考虑 Pod 开销. 类似地,kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销.


** 启用 Pod 开销
你需要确保在集群中启用了 PodOverhead 特性门控 (在 1.18 默认是开启的),以及一个定义了 overhead 字段的 RuntimeClass.


** 使用示例
要使用 PodOverhead 特性,需要一个定义了 overhead 字段的 RuntimeClass. 作为例子,下面的 RuntimeClass 定义中包含一个虚拟化所用的容器运行时, RuntimeClass 如下,其中每个 Pod 大约使用 120MiB 用来运行虚拟机和寄宿操作系统:

---
kind: RuntimeClass
apiVersion: node.k8s.io/v1
metadata:
    name: kata-fc
handler: kata-fc
overhead:
    podFixed:
        memory: "120Mi"
        cpu: "250m"

通过指定 kata-fc RuntimeClass 处理程序创建的工作负载会将内存和 CPU 开销计入资源配额计算、节点调度以及 Pod cgroup 尺寸确定.

假设我们运行下面给出的工作负载示例 test-pod:

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  runtimeClassName: kata-fc
  containers:
  - name: busybox-ctr
    image: busybox:1.28
    stdin: true
    tty: true
    resources:
      limits:
        cpu: 500m
        memory: 100Mi
  - name: nginx-ctr
    image: nginx
    resources:
      limits:
        cpu: 1500m
        memory: 100Mi

在准入阶段 RuntimeClass 准入控制器 更新工作负载的 PodSpec 以包含 RuntimeClass 中定义的 overhead.如果 PodSpec 中已定义该字段,该 Pod 将会被拒绝. 在这个例子中,由于只指定了 RuntimeClass 名称,所以准入控制器更新了 Pod,使之包含 overhead.

在 RuntimeClass 准入控制器之后,可以检验一下已更新的 PodSpec:

kubectl get pod test-pod -o jsonpath='{.spec.overhead}'

输出:

map[cpu:250m memory:120Mi]

如果定义了 ResourceQuata, 则容器请求的总量以及 overhead 字段都将计算在内.

当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时,调度器会兼顾该 Pod 的 overhead 以及该 Pod 的容器请求总量.在这个示例中,调度器将资源请求和开销相加, 然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点.

一旦 Pod 被调度到了某个节点, 该节点上的 kubelet 将为该 Pod 新建一个 cgroup. 底层容器运行时将在这个 Pod 中创建容器.

如果该资源对每一个容器都定义了一个限制(定义了限制值的 Guaranteed QoS 或者 Burstable QoS),kubelet 会为与该资源(CPU 的 cpu.cfs_quota_us 以及内存的 memory.limit_in_bytes) 相关的 Pod cgroup 设定一个上限.该上限基于 PodSpec 中定义的容器限制总量与 overhead 之和.

对于 CPU,如果 Pod 的 QoS 是 Guaranteed 或者 Burstable,kubelet 会基于容器请求总量与 PodSpec 中定义的 overhead 之和设置 cpu.shares.

请看这个例子,验证工作负载的容器请求:

kubectl get pod test-pod -o jsonpath='{.spec.containers[*].resources.limits}'

容器请求总计 2000m CPU 和 200MiB 内存:

map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]

对照从节点观察到的情况来检查一下:

kubectl describe node | grep test-pod -B2

该输出显示请求了 2250m CPU 以及 320MiB 内存,包含了 PodOverhead 在内:

  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m


** 验证 Pod cgroup 限制
在工作负载所运行的节点上检查 Pod 的内存 cgroups.在接下来的例子中, 将在该节点上使用具备 CRI 兼容的容器运行时命令行工具 crictl. 这是一个显示 PodOverhead 行为的高级示例, 预计用户不需要直接在节点上检查 cgroups. 首先在特定的节点上确定该 Pod 的标识符:

# 在该 Pod 被调度到的节点上执行如下命令:
POD_ID="$(sudo crictl pods --name test-pod -q)"

可以依此判断该 Pod 的 cgroup 路径:

# 在该 Pod 被调度到的节点上执行如下命令:
sudo crictl inspectp -o=json $POD_ID | grep cgroupsPath

执行结果的 cgroup 路径中包含了该 Pod 的 pause 容器.Pod 级别的 cgroup 在即上一层目录.

        "cgroupsPath": "/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a"

在这个例子中,该 Pod 的 cgroup 路径是 kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2. 验证内存的 Pod 级别 cgroup 设置:

# 在该 Pod 被调度到的节点上执行这个命令.
# 另外,修改 cgroup 的名称以匹配为该 Pod 分配的 cgroup.
 cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes

和预期的一样,这一数值为 320 MiB.

335544320

* 可观察性
在 kube-state-metrics 中可以通过 kube_pod_overhead 指标来协助确定何时使用 PodOverhead 以及协助观察以一个既定开销运行的工作负载的稳定性. 该特性在 kube-state-metrics 的 1.9 发行版本中不可用,不过预计将在后续版本中发布.在此之前,用户需要从源代码构建 kube-state-metrics.



## 污点和容忍度
节点亲和性 是 Pod 的一种属性,它使 Pod 被吸引到一类特定的节点 (这可能出于一种偏好,也可能是硬性要求). 污点(Taint)则相反——它使节点能够排斥一类特定的 Pod.

容忍度(Toleration)是应用于 Pod 上的,允许(但并不要求)Pod 调度到带有与之匹配的污点的节点上.

污点和容忍度(Toleration)相互配合,可以用来避免 Pod 被分配到不合适的节点上. 每个节点上都可以应用一个或多个污点,这表示对于那些不能容忍这些污点的 Pod,是不会被该节点接受的.


** 概念
您可以使用命令 kubectl taint 给节点增加一个污点.比如,

kubectl taint nodes node1 key1=value1:NoSchedule

给节点 node1 增加一个污点,它的键名是 key1,键值是 value1,效果是 NoSchedule. 这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 node1 这个节点.

若要移除上述命令所添加的污点,你可以执行:

kubectl taint nodes node1 key1=value1:NoSchedule-

您可以在 PodSpec 中定义 Pod 的容忍度. 下面两个容忍度均与上面例子中使用 kubectl taint 命令创建的污点相匹配, 因此如果一个 Pod 拥有其中的任何一个容忍度都能够被分配到 node1 :

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
tolerations:
- key: "key1"
  operator: "Exists"
  effect: "NoSchedule"

这里是一个使用了容忍度的 Pod:

pods/pod-with-toleration.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - key: "example-key"
    operator: "Exists"
    effect: "NoSchedule"

operator 的默认值是 Equal.

一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果,并且:

  * 如果 operator 是 Exists (此时容忍度不能指定 value),或者
  * 如果 operator 是 Equal ,则它们的 value 应该相等

Note:
存在两种特殊情况:

如果一个容忍度的 key 为空且 operator 为 Exists, 表示这个容忍度与任意的 key 、value 和 effect 都匹配,即这个容忍度能容忍任意 taint.

如果 effect 为空,则可以与所有键名 key1 的效果相匹配.

上述例子中 effect 使用的值为 NoSchedule,您也可以使用另外一个值 PreferNoSchedule. 这是“优化”或“软”版本的 NoSchedule —— 系统会 尽量 避免将 Pod 调度到存在其不能容忍污点的节点上, 但这不是强制的.effect 的值还可以设置为 NoExecute,下文会详细描述这个值.

您可以给一个节点添加多个污点,也可以给一个 Pod 添加多个容忍度设置. Kubernetes 处理多个污点和容忍度的过程就像一个过滤器:从一个节点的所有污点开始遍历, 过滤掉那些 Pod 中存在与之相匹配的容忍度的污点.余下未被过滤的污点的 effect 值决定了 Pod 是否会被分配到该节点,特别是以下情况:

  * 如果未被过滤的污点中存在至少一个 effect 值为 NoSchedule 的污点, 则 Kubernetes 不会将 Pod 分配到该节点.
  * 如果未被过滤的污点中不存在 effect 值为 NoSchedule 的污点, 但是存在 effect 值为 PreferNoSchedule 的污点, 则 Kubernetes 会 尝试 不将 Pod 分配到该节点.
  * 如果未被过滤的污点中存在至少一个 effect 值为 NoExecute 的污点, 则 Kubernetes 不会将 Pod 分配到该节点(如果 Pod 还未在节点上运行), 或者将 Pod 从该节点驱逐(如果 Pod 已经在节点上运行).

例如,假设您给一个节点添加了如下污点

  kubectl taint nodes node1 key1=value1:NoSchedule
  kubectl taint nodes node1 key1=value1:NoExecute
  kubectl taint nodes node1 key2=value2:NoSchedule

假定有一个 Pod,它有两个容忍度:

  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"

在这种情况下,上述 Pod 不会被分配到上述节点,因为其没有容忍度和第三个污点相匹配. 但是如果在给节点添加上述污点之前,该 Pod 已经在上述节点运行, 那么它还可以继续运行在该节点上,因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的.

通常情况下,如果给一个节点添加了一个 effect 值为 NoExecute 的污点, 则任何不能忍受这个污点的 Pod 都会马上被驱逐, 任何可以忍受这个污点的 Pod 都不会被驱逐. 但是,如果 Pod 存在一个 effect 值为 NoExecute 的容忍度指定了可选属性 tolerationSeconds 的值,则表示在给节点添加了上述污点之后, Pod 还能继续在节点上运行的时间.例如,

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600

这表示如果这个 Pod 正在运行,同时一个匹配的污点被添加到其所在的节点, 那么 Pod 还将继续在节点上运行 3600 秒,然后被驱逐. 如果在此之前上述污点被删除了,则 Pod 不会被驱逐.


** 使用例子
通过污点和容忍度,可以灵活地让 Pod 避开 某些节点或者将 Pod 从某些节点驱逐.下面是几个使用例子:

  * 专用节点:如果您想将某些节点专门分配给特定的一组用户使用,您可以给这些节点添加一个污点(即, kubectl taint nodes nodename dedicated=groupName:NoSchedule), 然后给这组用户的 Pod 添加一个相对应的 toleration(通过编写一个自定义的 准入控制器,很容易就能做到). 拥有上述容忍度的 Pod 就能够被分配到上述专用节点,同时也能够被分配到集群中的其它节点. 如果您希望这些 Pod 只能被分配到上述专用节点,那么您还需要给这些专用节点另外添加一个和上述 污点类似的 label (例如:dedicated=groupName),同时 还要在上述准入控制器中给 Pod 增加节点亲和性要求上述 Pod 只能被分配到添加了 dedicated=groupName 标签的节点上.
  * 配备了特殊硬件的节点:在部分节点配备了特殊硬件(比如 GPU)的集群中, 我们希望不需要这类硬件的 Pod 不要被分配到这些特殊节点,以便为后继需要这类硬件的 Pod 保留资源. 要达到这个目的,可以先给配备了特殊硬件的节点添加 taint (例如 kubectl taint nodes nodename special=true:NoSchedule 或 kubectl taint nodes nodename special=true:PreferNoSchedule), 然后给使用了这类特殊硬件的 Pod 添加一个相匹配的 toleration. 和专用节点的例子类似,添加这个容忍度的最简单的方法是使用自定义 准入控制器. 比如,我们推荐使用扩展资源 来表示特殊硬件,给配置了特殊硬件的节点添加污点时包含扩展资源名称, 然后运行一个 ExtendedResourceToleration 准入控制器.此时,因为节点已经被设置污点了,没有对应容忍度的 Pod 不会被调度到这些节点.但当你创建一个使用了扩展资源的 Pod 时, ExtendedResourceToleration 准入控制器会自动给 Pod 加上正确的容忍度, 这样 Pod 就会被自动调度到这些配置了特殊硬件件的节点上. 这样就能够确保这些配置了特殊硬件的节点专门用于运行需要使用这些硬件的 Pod, 并且您无需手动给这些 Pod 添加容忍度.
  * 基于污点的驱逐: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为,接下来的章节会描述这个特性.


** 基于污点的驱逐 
FEATURE STATE: Kubernetes v1.18 [stable]

前文提到过污点的 effect 值 NoExecute会影响已经在节点上运行的 Pod

  * 如果 Pod 不能忍受 effect 值为 NoExecute 的污点,那么 Pod 将马上被驱逐
  * 如果 Pod 能够忍受 effect 值为 NoExecute 的污点,但是在容忍度定义中没有指定 tolerationSeconds,则 Pod 还会一直在这个节点上运行.
  * 如果 Pod 能够忍受 effect 值为 NoExecute 的污点,而且指定了 tolerationSeconds, 则 Pod 还能在这个节点上继续运行这个指定的时间长度.

当某种条件为真时,节点控制器会自动给节点添加一个污点.当前内置的污点包括:

  * node.kubernetes.io/not-ready:节点未准备好.这相当于节点状态 Ready 的值为 "False".
  * node.kubernetes.io/unreachable:节点控制器访问不到节点. 这相当于节点状态 Ready 的值为 "Unknown".
  * node.kubernetes.io/memory-pressure:节点存在内存压力.
  * node.kubernetes.io/disk-pressure:节点存在磁盘压力.
  * node.kubernetes.io/pid-pressure: 节点的 PID 压力.
  * node.kubernetes.io/network-unavailable:节点网络不可用.
  * node.kubernetes.io/unschedulable: 节点不可调度.
  * node.cloudprovider.kubernetes.io/uninitialized:如果 kubelet 启动时指定了一个 "外部" 云平台驱动, 它将给当前节点添加一个污点将其标志为不可用.在 cloud-controller-manager 的一个控制器初始化这个节点后,kubelet 将删除这个污点.

在节点被驱逐时,节点控制器或者 kubelet 会添加带有 NoExecute 效应的相关污点. 如果异常状态恢复正常,kubelet 或节点控制器能够移除相关的污点.

Note: 为了保证由于节点问题引起的 Pod 驱逐 速率限制行为正常, 系统实际上会以限定速率的方式添加污点.在像主控节点与工作节点间通信中断等场景下, 这样做可以避免 Pod 被大量驱逐.

使用这个功能特性,结合 tolerationSeconds,Pod 就可以指定当节点出现一个 或全部上述问题时还将在这个节点上运行多长的时间.

比如,一个使用了很多本地状态的应用程序在网络断开时,仍然希望停留在当前节点上运行一段较长的时间, 愿意等待网络恢复以避免被驱逐.在这种情况下,Pod 的容忍度可能是下面这样的:

tolerations:
- key: "node.kubernetes.io/unreachable"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000

Note:
Kubernetes 会自动给 Pod 添加一个 key 为 node.kubernetes.io/not-ready 的容忍度 并配置 tolerationSeconds=300,除非用户提供的 Pod 配置中已经已存在了 key 为 node.kubernetes.io/not-ready 的容忍度.

同样,Kubernetes 会给 Pod 添加一个 key 为 node.kubernetes.io/unreachable 的容忍度 并配置 tolerationSeconds=300,除非用户提供的 Pod 配置中已经已存在了 key 为 node.kubernetes.io/unreachable 的容忍度.

这种自动添加的容忍度意味着在其中一种问题被检测到时 Pod 默认能够继续停留在当前节点运行 5 分钟.

DaemonSet 中的 Pod 被创建时, 针对以下污点自动添加的 NoExecute 的容忍度将不会指定 tolerationSeconds:

  * node.kubernetes.io/unreachable
  * node.kubernetes.io/not-ready

这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐.


** 基于节点状态添加污点
控制平面使用节点控制器自动创建 与节点状况对应的带有 NoSchedule 效应的污点.

调度器在进行调度时检查污点,而不是检查节点状况.这确保节点状况不会直接影响调度. 例如,如果 DiskPressure 节点状况处于活跃状态,则控制平面 添加 node.kubernetes.io/disk-pressure 污点并且不会调度新的 pod 到受影响的节点.如果 MemoryPressure 节点状况处于活跃状态,则 控制平面添加 node.kubernetes.io/memory-pressure 污点.

对于新创建的 Pod,可以通过添加相应的 Pod 容忍度来忽略节点状况. 控制平面还在具有除 BestEffort 之外的 QoS 类的 pod 上 添加 node.kubernetes.io/memory-pressure 容忍度. 这是因为 Kubernetes 将 Guaranteed 或 Burstable QoS 类中的 Pod(甚至没有设置内存请求的 Pod) 视为能够应对内存压力,而新创建的 BestEffort Pod 不会被调度到受影响的节点上.

DaemonSet 控制器自动为所有守护进程添加如下 NoSchedule 容忍度以防 DaemonSet 崩溃:

  * node.kubernetes.io/memory-pressure
  * node.kubernetes.io/disk-pressure
  * node.kubernetes.io/pid-pressure (1.14 或更高版本)
  * node.kubernetes.io/unschedulable (1.10 或更高版本)
  * node.kubernetes.io/network-unavailable (只适合主机网络配置)

添加上述容忍度确保了向后兼容,您也可以选择自由向 DaemonSet 添加容忍度.



## Pod 优先级和抢占
FEATURE STATE: Kubernetes v1.14 [stable]

Pod 可以有 优先级.优先级表示一个 Pod 相对于其他 Pod 的重要性.如果一个 Pod 无法被调度,调度程序会尝试抢占(驱逐)较低优先级的 Pod, 以使悬决 Pod 可以被调度.

Warning:
在一个并非所有用户都是可信的集群中,恶意用户可能以最高优先级创建 Pod, 导致其他 Pod 被驱逐或者无法被调度.管理员可以使用 ResourceQuota 来阻止用户创建高优先级的 Pod.参见默认限制优先级消费.


** 如何使用优先级和抢占
要使用优先级和抢占:

  1. 新增一个或多个 PriorityClass.
  2. 创建 Pod,并将其 priorityClassName 设置为新增的 PriorityClass.当然你不需要直接创建 Pod；通常,你将会添加 priorityClassName 到集合对象(如 Deployment) 的 Pod 模板中.

继续阅读以获取有关这些步骤的更多信息.

Note:
Kubernetes 已经提供了 2 个 PriorityClass: system-cluster-critical 和 system-node-critical.这些是常见的类,用于确保始终优先调度关键组件.


** PriorityClass
PriorityClass 是一个无名称空间对象,它定义了从优先级类名称到优先级整数值的映射.名称在 PriorityClass 对象元数据的 name 字段中指定.值在必填的 value 字段中指定.值越大,优先级越高.PriorityClass 对象的名称必须是有效的 DNS 子域名, 并且它不能以 system- 为前缀.

PriorityClass 对象可以设置任何小于或等于 10 亿的 32 位整数值.较大的数字是为通常不应被抢占或驱逐的关键的系统 Pod 所保留的.集群管理员应该为这类映射分别创建独立的 PriorityClass 对象.

PriorityClass 还有两个可选字段:globalDefault 和 description.globalDefault 字段表示这个 PriorityClass 的值应该用于没有 priorityClassName 的 Pod.系统中只能存在一个 globalDefault 设置为 true 的 PriorityClass.如果不存在设置了 globalDefault 的 PriorityClass, 则没有 priorityClassName 的 Pod 的优先级为零.

description 字段是一个任意字符串.它用来告诉集群用户何时应该使用此 PriorityClass.

* 关于 PodPriority 和现有集群的注意事项 
  * 如果你升级一个已经存在的但尚未使用此特性的集群,该集群中已经存在的 Pod 的优先级等效于零.
  * 添加一个将 globalDefault 设置为 true 的 PriorityClass 不会改变现有 Pod 的优先级.此类 PriorityClass 的值仅用于添加 PriorityClass 后创建的 Pod.
  * 如果你删除了某个 PriorityClass 对象,则使用被删除的 PriorityClass 名称的现有 Pod 保持不变, 但是你不能再创建使用已删除的 PriorityClass 名称的 Pod.

* PriorityClass 示例

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "此优先级类应仅用于 XYZ 服务 Pod."


** 非抢占式 PriorityClass
FEATURE STATE: Kubernetes v1.19 [beta]

配置了 preemptionPolicy: Never 的 Pod 将被放置在调度队列中较低优先级 Pod 之前, 但它们不能抢占其他 Pod.等待调度的非抢占式 Pod 将留在调度队列中,直到有足够的可用资源, 它才可以被调度.非抢占式 Pod,像其他 Pod 一样,受调度程序回退的影响.这意味着如果调度程序尝试这些 Pod 并且无法调度它们,它们将以更低的频率被重试, 从而允许其他优先级较低的 Pod 排在它们之前.

非抢占式 Pod 仍可能被其他高优先级 Pod 抢占.

preemptionPolicy 默认为 PreemptLowerPriority, 这将允许该 PriorityClass 的 Pod 抢占较低优先级的 Pod(现有默认行为也是如此).如果 preemptionPolicy 设置为 Never,则该 PriorityClass 中的 Pod 将是非抢占式的.

数据科学工作负载是一个示例用例.用户可以提交他们希望优先于其他工作负载的作业, 但不希望因为抢占运行中的 Pod 而导致现有工作被丢弃.设置为 preemptionPolicy: Never 的高优先级作业将在其他排队的 Pod 之前被调度, 只要足够的集群资源“自然地”变得可用.

* 非抢占式 PriorityClass 示例

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority-nonpreempting
value: 1000000
preemptionPolicy: Never
globalDefault: false
description: "This priority class will not cause other pods to be preempted."


** Pod 优先级
在你拥有一个或多个 PriorityClass 对象之后, 你可以创建在其规约中指定这些 PriorityClass 名称之一的 Pod.优先级准入控制器使用 priorityClassName 字段并填充优先级的整数值.如果未找到所指定的优先级类,则拒绝 Pod.

以下 YAML 是 Pod 配置的示例,它使用在前面的示例中创建的 PriorityClass.优先级准入控制器检查 Pod 规约并将其优先级解析为 1000000.

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority

* Pod 优先级对调度顺序的影响
当启用 Pod 优先级时,调度程序会按优先级对悬决 Pod 进行排序, 并且每个悬决的 Pod 会被放置在调度队列中其他优先级较低的悬决 Pod 之前.因此,如果满足调度要求,较高优先级的 Pod 可能会比具有较低优先级的 Pod 更早调度.如果无法调度此类 Pod,调度程序将继续并尝试调度其他较低优先级的 Pod.


** 抢占 
Pod 被创建后会进入队列等待调度.调度器从队列中挑选一个 Pod 并尝试将它调度到某个节点上.如果没有找到满足 Pod 的所指定的所有要求的节点,则触发对悬决 Pod 的抢占逻辑.让我们将悬决 Pod 称为 P.抢占逻辑试图找到一个节点, 在该节点中删除一个或多个优先级低于 P 的 Pod,则可以将 P 调度到该节点上.如果找到这样的节点,一个或多个优先级较低的 Pod 会被从节点中驱逐.被驱逐的 Pod 消失后,P 可以被调度到该节点上.

* 用户暴露的信息
当 Pod P 抢占节点 N 上的一个或多个 Pod 时, Pod P 状态的 nominatedNodeName 字段被设置为节点 N 的名称.该字段帮助调度程序跟踪为 Pod P 保留的资源,并为用户提供有关其集群中抢占的信息.

请注意,Pod P 不一定会调度到“被提名的节点(Nominated Node)”.在 Pod 因抢占而牺牲时,它们将获得体面终止期.如果调度程序正在等待牺牲者 Pod 终止时另一个节点变得可用, 则调度程序将使用另一个节点来调度 Pod P.因此,Pod 规约中的 nominatedNodeName 和 nodeName 并不总是相同.此外,如果调度程序抢占节点 N 上的 Pod,但随后比 Pod P 更高优先级的 Pod 到达, 则调度程序可能会将节点 N 分配给新的更高优先级的 Pod.在这种情况下,调度程序会清除 Pod P 的 nominatedNodeName.通过这样做,调度程序使 Pod P 有资格抢占另一个节点上的 Pod.

* 抢占的限制
被抢占牺牲者的体面终止
当 Pod 被抢占时,牺牲者会得到他们的 体面终止期.它们可以在体面终止期内完成工作并退出.如果它们不这样做就会被杀死.这个体面终止期在调度程序抢占 Pod 的时间点和待处理的 Pod (P) 可以在节点 (N) 上调度的时间点之间划分出了一个时间跨度.同时,调度器会继续调度其他待处理的 Pod.当牺牲者退出或被终止时, 调度程序会尝试在待处理队列中调度 Pod.因此,调度器抢占牺牲者的时间点与 Pod P 被调度的时间点之间通常存在时间间隔.为了最小化这个差距,可以将低优先级 Pod 的体面终止时间设置为零或一个小数字.

支持 PodDisruptionBudget,但不保证
PodDisruptionBudget (PDB) 允许多副本应用程序的所有者限制因自愿性质的干扰而同时终止的 Pod 数量.Kubernetes 在抢占 Pod 时支持 PDB,但对 PDB 的支持是基于尽力而为原则的.调度器会尝试寻找不会因被抢占而违反 PDB 的牺牲者,但如果没有找到这样的牺牲者, 抢占仍然会发生,并且即使违反了 PDB 约束也会删除优先级较低的 Pod.

与低优先级 Pod 之间的 Pod 间亲和性
只有当这个问题的答案是肯定的时,才考虑在一个节点上执行抢占操作: “如果从此节点上删除优先级低于悬决 Pod 的所有 Pod,悬决 Pod 是否可以在该节点上调度?”

Note: 抢占并不一定会删除所有较低优先级的 Pod.如果悬决 Pod 可以通过删除少于所有较低优先级的 Pod 来调度, 那么只有一部分较低优先级的 Pod 会被删除.即便如此,上述问题的答案必须是肯定的.如果答案是否定的,则不考虑在该节点上执行抢占.

如果悬决 Pod 与节点上的一个或多个较低优先级 Pod 具有 Pod 间亲和性, 则在没有这些较低优先级 Pod 的情况下,无法满足 Pod 间亲和性规则.在这种情况下,调度程序不会抢占节点上的任何 Pod.相反,它寻找另一个节点.调度程序可能会找到合适的节点, 也可能不会.无法保证悬决 Pod 可以被调度.

我们针对此问题推荐的解决方案是仅针对同等或更高优先级的 Pod 设置 Pod 间亲和性.

跨节点抢占
假设正在考虑在一个节点 N 上执行抢占,以便可以在 N 上调度待处理的 Pod P.只有当另一个节点上的 Pod 被抢占时,P 才可能在 N 上变得可行.下面是一个例子:
  * 正在考虑将 Pod P 调度到节点 N 上.
  * Pod Q 正在与节点 N 位于同一区域的另一个节点上运行.
  * Pod P 与 Pod Q 具有 Zone 维度的反亲和(topologyKey:topology.kubernetes.io/zone).# P 和 Q 不能在同一个 zone 内
  * Pod P 与 Zone 中的其他 Pod 之间没有其他反亲和性设置.
  * 为了在节点 N 上调度 Pod P,可以抢占 Pod Q,但调度器不会进行跨节点抢占.因此,Pod P 将被视为在节点 N 上不可调度.

如果将 Pod Q 从所在节点中移除,则不会违反 Pod 间反亲和性约束, 并且 Pod P 可能会被调度到节点 N 上.

如果有足够的需求,并且如果我们找到性能合理的算法, 我们可能会考虑在未来版本中添加跨节点抢占.


** 故障排除
Pod 优先级和抢占可能会产生不必要的副作用.以下是一些潜在问题的示例以及处理这些问题的方法.

* Pod 被不必要地抢占
抢占在资源压​​力较大时从集群中删除现有 Pod,为更高优先级的悬决 Pod 腾出空间.如果你错误地为某些 Pod 设置了高优先级,这些无意的高优先级 Pod 可能会导致集群中出现抢占行为.Pod 优先级是通过设置 Pod 规约中的 priorityClassName 字段来指定的.优先级的整数值然后被解析并填充到 podSpec 的 priority 字段.

为了解决这个问题,你可以将这些 Pod 的 priorityClassName 更改为使用较低优先级的类, 或者将该字段留空.默认情况下,空的 priorityClassName 解析为零.

当 Pod 被抢占时,集群会为被抢占的 Pod 记录事件.只有当集群没有足够的资源用于 Pod 时, 才会发生抢占.在这种情况下,只有当悬决 Pod(抢占者)的优先级高于受害 Pod 时才会发生抢占.当没有悬决 Pod,或者悬决 Pod 的优先级等于或低于牺牲者时,不得发生抢占.如果在这种情况下发生抢占,请提出问题.

* 有 Pod 被抢占,但抢占者并没有被调度
当 Pod 被抢占时,它们会收到请求的体面终止期,默认为 30 秒.如果受害 Pod 在此期限内没有终止,它们将被强制终止.一旦所有牺牲者都离开,就可以调度抢占者 Pod.

在抢占者 Pod 等待牺牲者离开的同时,可能某个适合同一个节点的更高优先级的 Pod 被创建.在这种情况下,调度器将调度优先级更高的 Pod 而不是抢占者.

这是预期的行为:具有较高优先级的 Pod 应该取代具有较低优先级的 Pod.

* 优先级较高的 Pod 在优先级较低的 Pod 之前被抢占
调度程序尝试查找可以运行悬决 Pod 的节点.如果没有找到这样的节点, 调度程序会尝试从任意节点中删除优先级较低的 Pod,以便为悬决 Pod 腾出空间.如果具有低优先级 Pod 的节点无法运行悬决 Pod, 调度器可能会选择另一个具有更高优先级 Pod 的节点(与其他节点上的 Pod 相比)进行抢占.牺牲者的优先级必须仍然低于抢占者 Pod.

当有多个节点可供执行抢占操作时,调度器会尝试选择具有一组优先级最低的 Pod 的节点.但是,如果此类 Pod 具有 PodDisruptionBudget,当它们被抢占时, 则会违反 PodDisruptionBudget,那么调度程序可能会选择另一个具有更高优先级 Pod 的节点.

当存在多个节点抢占且上述场景均不适用时,调度器会选择优先级最低的节点.


** Pod 优先级和服务质量之间的相互作用
Pod 优先级和 QoS 类 是两个正交特征,交互很少,并且对基于 QoS 类设置 Pod 的优先级没有默认限制.调度器的抢占逻辑在选择抢占目标时不考虑 QoS.抢占会考虑 Pod 优先级并尝试选择一组优先级最低的目标.仅当移除优先级最低的 Pod 不足以让调度程序调度抢占式 Pod, 或者最低优先级的 Pod 受 PodDisruptionBudget 保护时,才会考虑优先级较高的 Pod.

kubelet 使用优先级来确定 节点压力驱逐 Pod 的顺序.你可以使用 QoS 类来估计 Pod 最有可能被驱逐的顺序.kubelet 根据以下因素对 Pod 进行驱逐排名:

  1. 对紧俏资源的使用是否超过请求值
  2. Pod 优先级
  3. 相对于请求的资源使用量

有关更多详细信息,请参阅 kubelet 驱逐时 Pod 的选择.

当某 Pod 的资源用量未超过其请求时,kubelet 节点压力驱逐不会驱逐该 Pod.如果优先级较低的 Pod 没有超过其请求,则不会被驱逐.另一个优先级高于其请求的 Pod 可能会被驱逐.



## 节点压力驱逐
节点压力驱逐是 kubelet 主动终止 Pod 以回收节点上资源的过程.

kubelet 监控集群节点的 CPU、内存、磁盘空间和文件系统的 inode 等资源. 当这些资源中的一个或者多个达到特定的消耗水平, kubelet 可以主动地使节点上一个或者多个 Pod 失效,以回收资源防止饥饿.

在节点压力驱逐期间,kubelet 将所选 Pod 的 PodPhase 设置为 Failed.这将终止 Pod.

节点压力驱逐不同于 API 发起的驱逐.

kubelet 并不理会你配置的 PodDisruptionBudget 或者是 Pod 的 terminationGracePeriodSeconds. 如果你使用了软驱逐条件,kubelet 会考虑你所配置的 eviction-max-pod-grace-period. 如果你使用了硬驱逐条件,它使用 0s 宽限期来终止 Pod.

如果 Pod 是由替换失败 Pod 的工作负载资源 (例如 StatefulSet 或者 Deployment)管理, 则控制平面或 kube-controller-manager 会创建新的 Pod 来代替被驱逐的 Pod.

Note:
kubelet 在终止最终用户 Pod 之前会尝试回收节点级资源. 例如,它会在磁盘资源不足时删除未使用的容器镜像.

kubelet 使用各种参数来做出驱逐决定,如下所示:
  * 驱逐信号
  * 驱逐条件
  * 监控间隔


** 驱逐信号
驱逐信号是特定资源在特定时间点的当前状态. kubelet 使用驱逐信号,通过将信号与驱逐条件进行比较来做出驱逐决定, 驱逐条件是节点上应该可用资源的最小量.

kubelet 使用以下驱逐信号:

驱逐信号	描述
memory.available	      memory.available := node.status.capacity[memory] - node.stats.memory.workingSet
nodefs.available	        nodefs.available := node.stats.fs.available
nodefs.inodesFree	    nodefs.inodesFree := node.stats.fs.inodesFree
imagefs.available	      imagefs.available := node.stats.runtime.imagefs.available
imagefs.inodesFree	  imagefs.inodesFree := node.stats.runtime.imagefs.inodesFree
pid.available	              pid.available := node.stats.rlimit.maxpid - node.stats.rlimit.curproc

在上表中,描述列显示了 kubelet 如何获取信号的值.每个信号支持百分比值或者是字面值. kubelet 计算相对于与信号有关的总量的百分比值.

memory.available 的值来自 cgroupfs,而不是像 free -m 这样的工具. 这很重要,因为 free -m 在容器中不起作用,如果用户使用 节点可分配资源 这一功能特性,资源不足的判定是基于 CGroup 层次结构中的用户 Pod 所处的局部及 CGroup 根节点作出的. 这个脚本 重现了 kubelet 为计算 memory.available 而执行的相同步骤. kubelet 在其计算中排除了 inactive_file(即非活动 LRU 列表上基于文件来虚拟的内存的字节数), 因为它假定在压力下内存是可回收的.

kubelet 支持以下文件系统分区:

  1. nodefs:节点的主要文件系统,用于本地磁盘卷、emptyDir、日志存储等. 例如,nodefs 包含 /var/lib/kubelet/.
  2. imagefs:可选文件系统,供容器运行时存储容器镜像和容器可写层.

kubelet 会自动发现这些文件系统并忽略其他文件系统.kubelet 不支持其他配置.

Note:
一些 kubelet 垃圾收集功能已被弃用,以支持驱逐. 有关已弃用功能的列表,请参阅 kubelet 垃圾收集弃用.


** 驱逐条件
你可以为 kubelet 指定自定义驱逐条件,以便在作出驱逐决定时使用.

驱逐条件的形式为 [eviction-signal][operator][quantity],其中:
  * eviction-signal 是要使用的驱逐信号.
  * operator 是你想要的关系运算符, 比如 <(小于).
  * quantity 是驱逐条件数量,例如 1Gi. quantity 的值必须与 Kubernetes 使用的数量表示相匹配. 你可以使用文字值或百分比(%).

例如,如果一个节点的总内存为 10Gi 并且你希望在可用内存低于 1Gi 时触发驱逐, 则可以将驱逐条件定义为 memory.available<10% 或 memory.available< 1G. 你不能同时使用二者.

你可以配置软和硬驱逐条件.

* 软驱逐条件 
软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对. 在超过宽限期之前,kubelet 不会驱逐 Pod. 如果没有指定的宽限期,kubelet 会在启动时返回错误.

你可以既指定软驱逐条件宽限期,又指定 Pod 终止宽限期的上限,,给 kubelet 在驱逐期间使用. 如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件,则 kubelet 将使用两个宽限期中的较小者. 如果你没有指定宽限期上限,kubelet 会立即杀死被驱逐的 Pod,不允许其体面终止.

你可以使用以下标志来配置软驱逐条件:
  * eviction-soft:一组驱逐条件,如 memory.available<1.5Gi, 如果驱逐条件持续时长超过指定的宽限期,可以触发 Pod 驱逐.
  * eviction-soft-grace-period:一组驱逐宽限期, 如 memory.available=1m30s,定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间.
  * eviction-max-pod-grace-period:在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期(以秒为单位).

* 硬驱逐条件
硬驱逐条件没有宽限期.当达到硬驱逐条件时, kubelet 会立即杀死 pod,而不会正常终止以回收紧缺的资源.

你可以使用 eviction-hard 标志来配置一组硬驱逐条件, 例如 memory.available<1Gi.

kubelet 具有以下默认硬驱逐条件:
  * memory.available<100Mi
  * nodefs.available<10%
  * imagefs.available<15%
  * nodefs.inodesFree<5%(Linux 节点)


** 驱逐监测间隔 
kubelet 根据其配置的 housekeeping-interval(默认为 10s)评估驱逐条件.


** 节点条件 
kubelet 报告节点状况以反映节点处于压力之下,因为满足硬或软驱逐条件,与配置的宽限期无关.

kubelet 根据下表将驱逐信号映射为节点状况:

节点条件	             驱逐信号	                  描述
MemoryPressure	  memory.available	      节点上的可用内存已满足驱逐条件
DiskPressure	        nodefs.available、     节点的根文件系统或映像文件系统上的可用磁盘空间和 inode 已满足驱逐条件
                              nodefs.inodesFree、
                              imagefs.available 
                              或 imagefs.inodesFree	
PIDPressure	        pid.available	              (Linux) 节点上的可用进程标识符已低于驱逐条件

kubelet 根据配置的 --node-status-update-frequency 更新节点条件,默认为 10s.

* 节点条件振荡
在某些情况下,节点在软驱逐条件上下振荡,而没有保持定义的宽限期. 这会导致报告的节点条件在 true 和 false 之间不断切换,从而导致错误的驱逐决策.

为了防止振荡,你可以使用 eviction-pressure-transition-period 标志, 该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间. 过渡期的默认值为 5m.


** 回收节点级资源
kubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源.

当报告 DiskPressure 节点状况时,kubelet 会根据节点上的文件系统回收节点级资源.

* 有 imagefs
如果节点有一个专用的 imagefs 文件系统供容器运行时使用,kubelet 会执行以下操作:
  * 如果 nodefs 文件系统满足驱逐条件,kubelet 垃圾收集死亡 Pod 和容器.
  * 如果 imagefs 文件系统满足驱逐条件,kubelet 将删除所有未使用的镜像.

* 没有 imagefs 
如果节点只有一个满足驱逐条件的 nodefs 文件系统, kubelet 按以下顺序释放磁盘空间:
  * 对死亡的 Pod 和容器进行垃圾收集
  * 删除未使用的镜像


** kubelet 驱逐时 Pod 的选择
如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件, 则 kubelet 开始驱逐最终用户 Pod.

kubelet 使用以下参数来确定 Pod 驱逐顺序:
  1. Pod 的资源使用是否超过其请求
  2. Pod 优先级
  3. Pod 相对于请求的资源使用情况

因此,kubelet 按以下顺序排列和驱逐 Pod:
  1. 首先考虑资源使用量超过其请求的 BestEffort 或 Burstable Pod. 这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出.
  2. 资源使用量少于请求量的 Guaranteed Pod 和 Burstable Pod 根据其优先级被最后驱逐.

Note:
kubelet 不使用 Pod 的 QoS 类来确定驱逐顺序. 在回收内存等资源时,你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序. QoS 不适用于临时存储(EphemeralStorage)请求, 因此如果节点在 DiskPressure 下,则上述场景将不适用.

仅当 Guaranteed Pod 中所有容器都被指定了请求和限制并且二者相等时,才保证 Pod 不被驱逐. 这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐. 如果系统守护进程(例如 kubelet 和 journald) 消耗的资源比通过 system-reserved 或 kube-reserved 分配保留的资源多, 并且该节点只有 Guaranteed 或 Burstable Pod 使用的资源少于其上剩余的请求, 那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响. 在这种情况下,它会选择首先驱逐最低优先级的 Pod.

当 kubelet 因 inode 或 PID 不足而驱逐 pod 时, 它使用优先级来确定驱逐顺序,因为 inode 和 PID 没有请求.

kubelet 根据节点是否具有专用的 imagefs 文件系统对 Pod 进行不同的排序:

* 有 imagefs
如果 nodefs 触发驱逐, kubelet 会根据 nodefs 使用情况(本地卷 + 所有容器的日志)对 Pod 进行排序.

如果 imagefs 触发驱逐,kubelet 会根据所有容器的可写层使用情况对 Pod 进行排序.

* 没有 imagefs
如果 nodefs 触发驱逐, kubelet 会根据磁盘总用量(本地卷 + 日志和所有容器的可写层)对 Pod 进行排序.


** 最小驱逐回收 
在某些情况下,驱逐 Pod 只会回收少量的紧俏资源. 这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐.

你可以使用 --eviction-minimum-reclaim 标志或 kubelet 配置文件 为每个资源配置最小回收量. 当 kubelet 注意到某个资源耗尽时,它会继续回收该资源,直到回收到你所指定的数量为止.

例如,以下配置设置最小回收量:

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
evictionHard:
  memory.available: "500Mi"
  nodefs.available: "1Gi"
  imagefs.available: "100Gi"
evictionMinimumReclaim:
  memory.available: "0Mi"
  nodefs.available: "500Mi"
  imagefs.available: "2Gi"
在这个例子中,如果 nodefs.available 信号满足驱逐条件, kubelet 会回收资源,直到信号达到 1Gi 的条件, 然后继续回收至少 500Mi 直到信号达到 1.5Gi.

类似地,kubelet 会回收 imagefs 资源,直到 imagefs.available 信号达到 102Gi.

对于所有资源,默认的 eviction-minimum-reclaim 为 0.


** 节点内存不足行为 
如果节点在 kubelet 能够回收内存之前遇到内存不足(OOM)事件, 则节点依赖 oom_killer 来响应.

kubelet 根据 Pod 的服务质量(QoS)为每个容器设置一个 oom_score_adj 值.

服务质量	    oom_score_adj
Guaranteed	  -997
BestEffort	    1000
Burstable	    min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)

Note:
kubelet 还将具有 system-node-critical 优先级 的 Pod 中的容器 oom_score_adj 值设为 -997.

如果 kubelet 在节点遇到 OOM 之前无法回收内存, 则 oom_killer 根据它在节点上使用的内存百分比计算 oom_score, 然后加上 oom_score_adj 得到每个容器有效的 oom_score. 然后它会杀死得分最高的容器.

这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器,将首先被杀死.

与 Pod 驱逐不同,如果容器被 OOM 杀死, kubelet 可以根据其 RestartPolicy 重新启动它.


** 最佳实践
以下部分描述了驱逐配置的最佳实践.

可调度的资源和驱逐策略
当你为 kubelet 配置驱逐策略时, 你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度,因为这类 Pod 会立即引起内存压力.

考虑以下场景:

节点内存容量:10Gi
操作员希望为系统守护进程(内核、kubelet 等)保留 10% 的内存容量
操作员希望驱逐内存利用率为 95% 的Pod,以减少系统 OOM 的概率.
为此,kubelet 启动设置如下:

--eviction-hard=memory.available<500Mi
--system-reserved=memory=1.5Gi
在此配置中,--system-reserved 标志为系统预留了 1.5Gi 的内存, 即 总内存的 10% + 驱逐条件量.

如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 1Gi, 则节点可以达到驱逐条件,这使得 memory.available 信号低于 500Mi 并触发条件.


** DaemonSet 
Pod 优先级是做出驱逐决定的主要因素. 如果你不希望 kubelet 驱逐属于 DaemonSet 的 Pod, 请在 Pod 规约中为这些 Pod 提供足够高的 priorityClass. 你还可以使用优先级较低的 priorityClass 或默认配置, 仅在有足够资源时才运行 DaemonSet Pod.


** 已知问题
以下部分描述了与资源不足处理相关的已知问题.

* kubelet 可能不会立即观察到内存压力
默认情况下,kubelet 轮询 cAdvisor 以定期收集内存使用情况统计信息. 如果该轮询时间窗口内内存使用量迅速增加,kubelet 可能无法足够快地观察到 MemoryPressure, 但是 OOMKiller 仍将被调用.

你可以使用 --kernel-memcg-notification 标志在 kubelet 上启用 memcg 通知 API,以便在超过条件时立即收到通知.

如果你不是追求极端利用率,而是要采取合理的过量使用措施, 则解决此问题的可行方法是使用 --kube-reserved 和 --system-reserved 标志为系统分配内存.

* active_file 内存未被视为可用内存
在 Linux 上,内核跟踪活动 LRU 列表上的基于文件所虚拟的内存字节数作为 active_file 统计信息. kubelet 将 active_file 内存区域视为不可回收. 对于大量使用块设备形式的本地存储(包括临时本地存储)的工作负载, 文件和块数据的内核级缓存意味着许多最近访问的缓存页面可能被计为 active_file. 如果这些内核块缓冲区中在活动 LRU 列表上有足够多, kubelet 很容易将其视为资源用量过量并为节点设置内存压力污点,从而触发 Pod 驱逐.

你可以通过为可能执行 I/O 密集型活动的容器设置相同的内存限制和内存请求来应对该行为. 你将需要估计或测量该容器的最佳内存限制值.



## API 发起的驱逐
API 发起的驱逐是一个先调用 Eviction API 创建 Eviction 对象,再由该对象体面地中止 Pod 的过程.

你可以通过直接调用 Eviction API 发起驱逐,也可以通过编程的方式使用 API 服务器的客户端来发起驱逐, 比如 kubectl drain 命令. 此操作创建一个 Eviction 对象,该对象再驱动 API 服务器终止选定的 Pod.

API 发起的驱逐将遵从你的 PodDisruptionBudgets 和 terminationGracePeriodSeconds 配置.

使用 API 创建 Eviction 对象,就像对 Pod 执行策略控制的 DELETE 操作

** 调用 Eviction API
你可以使用 Kubernetes 语言客户端 来访问 Kubernetes API 并创建 Eviction 对象. 要执行此操作,你应该用 POST 发出要尝试的请求,类似于下面的示例:

policy/v1

Note:
policy/v1 版本的 Eviction 在 v1.22 以及更高的版本中可用,之前的发行版本使用 policy/v1beta1 版本.

{
  "apiVersion": "policy/v1",
  "kind": "Eviction",
  "metadata": {
    "name": "quux",
    "namespace": "default"
  }
}

或者,你可以通过使用 curl 或者 wget 来访问 API 以尝试驱逐操作,类似于以下示例:

curl -v -H 'Content-type: application/json' https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json


** API 发起驱逐的工作原理
当你使用 API 来请求驱逐时,API 服务器将执行准入检查,并通过以下方式之一做出响应:
  * 200 OK:允许驱逐,子资源 Eviction 被创建,并且 Pod 被删除, 类似于发送一个 DELETE 请求到 Pod 地址.
  * 429 Too Many Requests:当前不允许驱逐,因为配置了 PodDisruptionBudget. 你可以稍后再尝试驱逐.你也可能因为 API 速率限制而看到这种响应.
  * 500 Internal Server Error:不允许驱逐,因为存在配置错误, 例如存在多个 PodDisruptionBudgets 引用同一个 Pod.
如果你想驱逐的 Pod 不属于有 PodDisruptionBudget 的工作负载, API 服务器总是返回 200 OK 并且允许驱逐.

如果 API 服务器允许驱逐,Pod 按照如下方式删除:
  1. API 服务器中的 Pod 资源会更新上删除时间戳,之后 API 服务器会认为此 Pod 资源将被终止. 此 Pod 资源还会标记上配置的宽限期.
  2. 本地运行状态的 Pod 所处的节点上的 kubelet 注意到 Pod 资源被标记为终止,并开始优雅停止本地 Pod.
  3. 当 kubelet 停止 Pod 时,控制面从 Endpoint 和 EndpointSlice 对象中移除该 Pod.因此,控制器不再将此 Pod 视为有用对象.
  4. Pod 的宽限期到期后,kubelet 强制终止本地 Pod.
  5. kubelet 告诉 API 服务器删除 Pod 资源.
  6. API 服务器删除 Pod 资源.


** 解决驱逐被卡住的问题
在某些情况下,你的应用可能进入中断状态, 在你干预之前,驱逐 API 总是返回 429 或 500. 例如,如果 ReplicaSet 为你的应用程序创建了 Pod, 但新的 Pod 没有进入 Ready 状态,就会发生这种情况. 在最后一个被驱逐的 Pod 有很长的终止宽限期的情况下,你可能也会注意到这种行为.

如果你注意到驱逐被卡住,请尝试以下解决方案之一:
  * 终止或暂停导致问题的自动化操作,重新启动操作之前,请检查被卡住的应用程序.
  * 等待一段时间后,直接从集群控制平面删除 Pod,而不是使用 Eviction API.



## 扩展资源的资源装箱
详细介绍
https://kubernetes.io/zh/docs/concepts/scheduling-eviction/resource-bin-packing/



## 调度框架
FEATURE STATE: Kubernetes 1.19 [stable]
调度框架是面向 Kubernetes 调度器的一种插件架构, 它为现有的调度器添加了一组新的“插件” API.插件会被编译到调度器之中. 这些 API 允许大多数调度功能以插件的形式实现,同时使调度“核心”保持简单且可维护. 请参考调度框架的设计提案 获取框架设计的更多技术信息.

框架工作流程
调度框架定义了一些扩展点.调度器插件注册后在一个或多个扩展点处被调用. 这些插件中的一些可以改变调度决策,而另一些仅用于提供信息.

每次调度一个 Pod 的尝试都分为两个阶段,即 调度周期 和 绑定周期.

详细介绍
https://kubernetes.io/zh/docs/concepts/scheduling-eviction/scheduling-framework/



## 调度器性能调优
FEATURE STATE: Kubernetes 1.14 [beta]
作为 kubernetes 集群的默认调度器, kube-scheduler 主要负责将 Pod 调度到集群的 Node 上.

在一个集群中,满足一个 Pod 调度请求的所有 Node 称之为 可调度 Node. 调度器先在集群中找到一个 Pod 的可调度 Node,然后根据一系列函数对这些可调度 Node 打分, 之后选出其中得分最高的 Node 来运行 Pod. 最后,调度器将这个调度决定告知 kube-apiserver,这个过程叫做 绑定(Binding).

这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式.

在大规模集群中,你可以调节调度器的表现来平衡调度的延迟(新 Pod 快速就位) 和精度(调度器很少做出糟糕的放置决策).

你可以通过设置 kube-scheduler 的 percentageOfNodesToScore 来配置这个调优设置. 这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值.

* 设置阈值
percentageOfNodesToScore 选项接受从 0 到 100 之间的整数值. 0 值比较特殊,表示 kube-scheduler 应该使用其编译后的默认值. 如果你设置 percentageOfNodesToScore 的值超过了 100, kube-scheduler 的表现等价于设置值为 100.

要修改这个值,先编辑 kube-scheduler 的配置文件 然后重启调度器. 大多数情况下,这个配置文件是 /etc/kubernetes/config/kube-scheduler.yaml.

修改完成后,你可以执行

kubectl get pods -n kube-system | grep kube-scheduler

来检查该 kube-scheduler 组件是否健康.


** 节点打分阈值
要提升调度性能,kube-scheduler 可以在找到足够的可调度节点之后停止查找. 在大规模集群中,比起考虑每个节点的简单方法相比可以节省时间.

你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够. kube-scheduler 会将它转换为节点数的整数值.在调度期间,如果 kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量, kube-scheduler 将停止继续查找可调度节点并继续进行 打分阶段.

调度器如何遍历节点 详细介绍了这个过程.

* 默认阈值
如果你不指定阈值,Kubernetes 使用线性公式计算出一个比例,在 100-节点集群 下取 50%,在 5000-节点的集群下取 10%.这个自动设置的参数的最低值是 5%.

这意味着,调度器至少会对集群中 5% 的节点进行打分,除非用户将该参数设置的低于 5.

如果你想让调度器对集群内所有节点进行打分,则将 percentageOfNodesToScore 设置为 100.


** 示例
下面就是一个将 percentageOfNodesToScore 参数设置为 50% 的例子.

apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
algorithmSource:
  provider: DefaultProvider

...

percentageOfNodesToScore: 50

* 调节 percentageOfNodesToScore 参数
percentageOfNodesToScore 的值必须在 1 到 100 之间,而且其默认值是通过集群的规模计算得来的. 另外,还有一个 50 个 Node 的最小值是硬编码在程序中.

值得注意的是,该参数设置后可能会导致只有集群中少数节点被选为可调度节点, 很多节点都没有进入到打分阶段.这样就会造成一种后果, 一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段.

由于这个原因,这个参数不应该被设置成一个很低的值. 通常的做法是不会将这个参数的值设置的低于 10. 很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用. 换句话说,只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时, 才使用很低的参数设置.

* 调度器做调度选择的时候如何覆盖所有的 Node
如果你想要理解这一个特性的内部细节,那么请仔细阅读这一章节.

在将 Pod 调度到节点上时,为了让集群中所有节点都有公平的机会去运行这些 Pod, 调度器将会以轮询的方式覆盖全部的 Node. 你可以将 Node 列表想象成一个数组.调度器从数组的头部开始筛选可调度节点, 依次向后直到可调度节点的数量达到 percentageOfNodesToScore 参数的要求. 在对下一个 Pod 进行调度的时候,前一个 Pod 调度筛选停止的 Node 列表的位置, 将会来作为这次调度筛选 Node 开始的位置.

如果集群中的 Node 在多个区域,那么调度器将从不同的区域中轮询 Node, 来确保不同区域的 Node 接受可调度性检查.如下例,考虑两个区域中的六个节点:

Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
调度器将会按照如下的顺序去评估 Node 的可调度性:

Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
在评估完所有 Node 后,将会返回到 Node 1,从头开始.



## 集群管理 --- 重要
集群管理概述面向任何创建和管理 Kubernetes 集群的读者人群. 我们假设你大概了解一些核心的 Kubernetes 概念.


** 规划集群 
查阅安装中的指导,获取如何规划、建立以及配置 Kubernetes 集群的示例.本文所列的文章称为发行版 .

Note: 并非所有发行版都是被积极维护的. 请选择使用最近 Kubernetes 版本测试过的发行版.

在选择一个指南前,有一些因素需要考虑：

  * 你是打算在你的计算机上尝试 Kubernetes,还是要构建一个高可用的多节点集群? 请选择最适合你需求的发行版.
  * 你正在使用类似 Google Kubernetes Engine 这样的被托管的 Kubernetes 集群, 还是管理你自己的集群?
  * 你的集群是在本地还是云(IaaS) 上?Kubernetes 不能直接支持混合集群. 作为代替,你可以建立多个集群.
  * 如果你在本地配置 Kubernetes,需要考虑哪种 网络模型(https://kubernetes.io/zh/docs/concepts/cluster-administration/networking/)最适合.
  * 你的 Kubernetes 在裸金属硬件上还是虚拟机(VMs)上运行?
  * 你是想运行一个集群,还是打算参与开发 Kubernetes 项目代码? 如果是后者,请选择一个处于开发状态的发行版. 某些发行版只提供二进制发布版,但提供更多的选择.
  * 让你自己熟悉运行一个集群所需的组件(https://kubernetes.io/zh/docs/concepts/overview/components/).


** 管理集群 
  * 学习如何管理节点(https://kubernetes.io/zh/docs/concepts/architecture/nodes/).
  * 学习如何设定和管理集群共享的资源配额(https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/).


** 保护集群 
  * 生成证书(https://kubernetes.io/zh/docs/tasks/administer-cluster/certificates/) 节描述了使用不同的工具链生成证书的步骤.
  * Kubernetes 容器环境(https://kubernetes.io/zh/docs/concepts/containers/container-environment/) 描述了 Kubernetes 节点上由 Kubelet 管理的容器的环境.
  * 控制到 Kubernetes API 的访问(https://kubernetes.io/zh/docs/concepts/security/controlling-access/) 描述了如何为用户和 service accounts 建立权限许可.
  * 身份认证(https://kubernetes.io/zh/docs/reference/access-authn-authz/authentication/) 节阐述了 Kubernetes 中的身份认证功能,包括许多认证选项.
  * 鉴权(https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/) 与身份认证不同,用于控制如何处理 HTTP 请求.
  * 使用准入控制器(https://kubernetes.io/zh/docs/reference/access-authn-authz/admission-controllers/) 阐述了在认证和授权之后拦截到 Kubernetes API 服务的请求的插件.
  * 在 Kubernetes 集群中使用 Sysctls(https://kubernetes.io/zh/docs/tasks/administer-cluster/sysctl-cluster/) 描述了管理员如何使用 sysctl 命令行工具来设置内核参数.
  * 审计(https://kubernetes.io/zh/docs/tasks/debug/debug-cluster/audit/) 描述了如何与 Kubernetes 的审计日志交互.

* 保护 kubelet 
  * 主控节点通信(https://kubernetes.io/zh/docs/concepts/architecture/control-plane-node-communication/)
  * TLS 引导(https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/)
  * Kubelet 认证/授权(https://kubernetes.io/zh/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/)


** 可选集群服务 
  * DNS 集成(https://kubernetes.io/zh/docs/concepts/services-networking/dns-pod-service/) 描述了如何将一个 DNS 名解析到一个 Kubernetes service.
  * 记录和监控集群活动(https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/) 阐述了 Kubernetes 的日志如何工作以及怎样实现.








 





















