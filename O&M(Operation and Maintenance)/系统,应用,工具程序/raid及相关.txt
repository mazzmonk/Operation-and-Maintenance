raid 0 最少2块硬盘，平衡I/0负载,没有冗余
raid 1 同时写2块硬盘，可以同时从2块硬盘中读取，可以提高速度，但是由于同时写2块硬盘，因此影响写速度
raid 0+1 对raid 0做镜像，是2组硬盘先做raid 0，组成2个大容量逻辑硬盘，再互为镜像，写入时，磁盘会同时写入2个大的逻辑硬盘，磁盘利用效率只有50%，同raid 1.
raid 1+0 先建立2个独立的raid1,然后把这两个raid1组成raid0
raid 10上是上述2种的统称，至少需要4块硬盘
        raid 1
raid 0          raid 0
d0 d1           d0 d1
d2 d3 		d2 d3
d4 d5		d4 d5
d6 d7		d6 d7

raid 5最少3块盘，磁盘利用率是n-1，并且最多只能有1块坏掉,raid 5的读写与raid 0类似，数据会分成3份写入不同磁盘
raid 6是raid 5的升级版，多使用一块磁盘做奇偶校验，与raid 5的校验采用不同的算法，因此安全性更高，但是多余的这块硬盘不能做实际存储使用，同时，写入性能比raid 5还差，至少需要4块硬盘，可以坏2块硬盘



子网掩码计算
1.根据要划分的子网数目转换为2的m次方。如要分8个子网，8=2的3次方，如果不是恰好的2的多少次方，则取大原则，如要划分6个，则同样需要考虑2的3次方。

2.先将子网数转换为二进制数，然后取得位数，比如为N，用该IP地址的类子网掩码，确定的最终的掩码，如果是C类，则为255.255.255.224;如果是B类，则掩码为255.255.224.0;如果是A类，则掩码为255.224.0.0

举例说明，如果我们用的网络号为192.9.200，则该C类网的主机IP地址就是192.9.200.1-192.9.200.254，将网络划分为4个子网，步骤如下：
4的二进制为0100,有效为是3位，N=3,如果是C类地址的掩码是255.255.255.0的主机地址前3位置1,得到11111111 11111111 11111111 11100000十进制为225.255.255.224

3.通过主机数来确定掩码
举例说明：将B类IP地址为168.195.0.0的网络划分为若干子网，要求每个子网有主机数700台，则掩码计算如下
将700转换为二进制，得到1010111100
得到二进制位数为10
将255.255.255.255从后往前的10位全部置0,得到二进制数为11111111.11111111.11111100.00000000，转换成十进制为255.255.252.0

4.IP地址和掩码位数对照
		1占据的位代表网络地址	
255.255.255.128 255.255.255.10000000 /25  
255.255.255.192	255.255.255.11000000 /26
255.255.255.224 255.255.255.11100000 /27
255.255.255.240 255.255.255.11110000 /28
255.255.255.248 255.255.255.11111000 /29
255.255.255.252 255.255.255.11111100 /30

对于c类网络可以这样划分，B，A类可划分的余地更大
子网个数=2的X次，x为1的个数
主机个数=2的Y次-2,y为0的个数,同样可以用256-128=128,可以用的地址是128-2=126


IP 地址 10.0.10.32 255.255.255.224 ，这个地址是个什么地址呢？
第一先看这个地址为c类地址
256-224=32，那个就可以得出，这个子网划分的每个块里面的地址为32个，
我们知道IP地址从零算起，俺么0-31为一个网段，之中0为网络地址（也就是我们常说的网络ID）;那么31就为广播地址了，那么以此类推32-63，有又是一个网段。同样的32就为网络ID也就是网络地址。

132.119.100.200 255.255.255.240那么这个IP地址的广播地址为多少了？属于那个网段

256-240=16,可确定每个网段为16个IP地址，200/16是12余8,就是说这个段的起始地址是16x12=192,则他的网络地址就是192，广播地址192+15=207,因此这个地址的广播地址是132.119.100.207


10.0.10.32 255.255.255.224掩码是多少位
掩码头三段是255,有至少是3X8=24个1,就是说超过24位，最后一段是224,用224/2得到多少个1,加上前边的24个，就是掩码位数
224的二进制数是11100000，最后就是24+3=27位

10.0.10.32 255.255.224.0
同样的少于24位，多余16位,224最高三位是1,所以就是16+3=19位


NFS server
nfs协议有v4,v3,v2协议版本
v2最大只支持32bit的文件大小（4G），v3增加了64bit的文件大小
v2最多只能只能用8k作为文件传输尺寸，v3没有限定传输尺寸，可以使用-rsize and -wsize来进行设定
v3增加和完善了许多错误和成功信息返回
v2只提供了对UDP协议支持，v3增加了对tcp协议支持
异步写入（v3新增）能否使用异步写入，这是可选择的一种特性。NFS V3客户端发发送一个异步写入请求到服务器，在给客户端答复之前服务器并不是必须要将数据写入到存储器中（稳定的）。服务器能确定何时去写入数据或者将多个写入请求聚合到一起并加以处理，然后写入。。客户端能保持一个数据的copy以防万一服务器不能完整的将数据写入。当客户端希望释放这个copy的时候，它会向服务器通过这个操作过程，以确保每个操作步骤的完整。

NFS本身是没有提供信息传输的协议和功能的，但NFS却能让我们通过网络进行资料的分享，这是因为NFS使用了一些其它的传输协议。而这些传输协议勇士用到这个RPC功能的。可以说NFS本身就是使用RPC的一个程序。或者说NFS也是一个RPC SERVER.所以只要用到NFS的地方都要启动RPC服务，不论是NFS SERVER或者NFS CLIENT。这样SERVER和CLIENT才能通过RPC来实现PROGRAM PORT的对应。可以这么理解RPC和NFS的关系：NFS是一个文件系统，而RPC是负责负责信息的传输。

NFS需要启动的DAEMONS 
pc.nfsd:主要复杂登陆权限检测等。 
rpc.mountd：负责NFS的档案系统，当CLIENT端通过rpc.nfsd登陆SERVER后，对clinet存取server的文件进行一系列的管理 
NFS SERVER在REDHAT LINUX平台下一共需要两个套件：nfs-utils和PORTMAP 
nfs-utils：提供rpc.nfsd 及 rpc.mountd这两个NFS DAEMONS的套件 
portmap:NFS其实可以被看作是一个RPC SERVER PROGRAM,而要启动一个RPC SERVER PROGRAM，都要做好PORT的对应工作，而且这样的任务就是由PORTMAP来完成的。通俗的说PortMap就是用来做PORT的mapping的。

服务器端的设定（以LINUX为例） 
服务器端的设定都是在/etc/exports这个文件中进行设定的
可以设定的参数主要有以下这些： 
rw：可读写的权限； 
ro：只读的权限； 
no_root_squash：登入到NFS主机的用户如果是ROOT用户，他就拥有ROOT的权限，此参数很不安全，建议不要使用。
root_squash: 在登入NFS主机使用分享之目录的使用者如果是root时，那么这个使用者的权限将被压缩成为匿名使用者，通常他的UID与GID都会变成nobody那个系统账号的身份；
all_squash：不管登陆NFS主机的用户是什么都会被重新设定为nobody。 
anonuid：将登入NFS主机的用户都设定成指定的user id,此ID必须存在于/etc/passwd中。 
anongid：同anonuid，但是组设置成group ID就是了
sync：资料同步写入存储器中。 
async：资料会先暂时存放在内存中，不会直接写入硬盘。 
insecure 允许从这台机器过来的非授权访问。 

 例如可以编辑/etc/exports为： 
 /tmp　　　　　*(rw,no_root_squash) 
 /home/public　192.168.0.*(rw)　　 *(ro) 
 /home/test　　192.168.0.100(rw) 
 /home/linux　 *.the9.com(rw,all_squash,anonuid=40,anongid=40) 
 设定好后可以使用以下命令启动NFS: 
 /etc/rc.d/init.d/portmap start (在REDHAT中PORTMAP是默认启动的） 
 /etc/rc.d/init.d/nfs start 

 exportfs命令： 
 如果我们在启动了NFS之后又修改了/etc/exports，是不是还要重新启动nfs呢？这个时候我们就可以用exportfs命令来使改动立刻生效，该命令格式如下： 
exportfs [-aruv] 
-a ：全部mount或者unmount /etc/exports中的内容 
-r ：重新mount /etc/exports中分享出来的目录 
-u ：umount 目录 
-v ：在 export 的?r候，将详细的信息输出到屏幕上

 HARD mount和SOFT MOUNT： 
 HARD: NFS CLIENT会不断的尝试与SERVER的连接（在后台，不会给出任何提示信息,在LINUX下有的版本仍然会给出一些提示），直到MOUNT上。 
 SOFT:会在前台尝试与SERVER的连接，是默认的连接方式。当收到错误信息后终止mount尝试，并给出相关信息。 
 例如：mount -F nfs -o hard 192.168.0.10:/nfs /nfs 

与NFS性能有关的问题有很多，通常可以要考虑的有以下这些选择： 
 
 WSIZE,RSIZE参数来优化NFS的执行效能 
 WSIZE、RSIZE对于NFS的效能有很大的影响。 
 wsize和rsize设定了SERVER和CLIENT之间往来数据块的大小，这两个参数的合理设定与很多方面有关，不仅是软件方面也有硬件方面的因素会影响这两个参数的设定（例如LINUX KERNEL、网卡，交换机等等）。 
 下面这个命令可以测试NFS的执行效能，读和写的效能可以分别测试，分别找到合适的参数。对于要测试分散的大量的数据的读写可以通过编写脚本来进行测试。在每次测试的时候最好能重复的执行一次MOUNT和unmount。 
 time dd if=/dev/zero of=/mnt/home/testfile bs=16k count=16384 
 用于测试的WSIZE,RSIZE最好是1024的倍数，对于NFS V2来说8192是RSIZE和WSIZE的最大数值，如果使用的是NFS V3则可以尝试的最大数值是32768。 
 如果设置的值比较大的时候，应该最好在CLIENT上进入mount上的目录中，进行一些常规操作（LS,VI等等），看看有没有错误信息出现。有可能出现的典型问题有LS的时候文件不能完整的列出或者是出现错误信息，不同的操作系统有不同的最佳数值，所以对于不同的操作系统都要进行测试。

 设定最佳的NFSD的COPY数目。 
 linux中的NFSD的COPY数目是在/etc/rc.d/init.d/nfs这个启动文件中设置的，默认是8个NFSD,对于这个参数的设置一般是要根据可能的CLIENT数目来进行设定的，和WSIZE、RSIZE一样也是要通过测试来找到最近的数值。 
 
 UDP and TCP 
 可以手动进行设置，也可以自动进行选择。 
 mount -t nfs -o sync,tcp,noatime,rsize=1024,wsize=1024 EXPORT_MACHINE:/EXPORTED_DIR /DIR 
 UDP有着传输速度快，非连接传输的便捷特性，但是UDP在传输上没有TCP来的稳定，当网络不稳定或者黑客入侵的时候很容易使NFS的 Performance 大幅降低甚至使网络瘫痪。所以对于不同情况的网络要有针对的选择传输协议。nfs over tcp比较稳定，nfs over udp速度较快。在机器较少网络状况较好的情况下使用UDP协议能带来较好的性能，当机器较多，网络情况复杂时推荐使用TCP协议（V2只支持UDP协议）。在局域网中使用UDP协议较好，因为局域网有比较稳定的网络保证，使用UDP可以带来更好的性能，在广域网中推荐使用TCP协议，TCP协议能让NFS在复杂的网络环境中保持最好的传输稳定性
 
版本的选择 LINUX通过mount option的nfsvers=n进行选择。 
mountd 进程是一个远程过程调用 (RPC) ，其作用是对客户端要求安装（mount）文件系统的申请作出响应。mountd进程通过查找 /etc/xtab文件来获知哪些文件系统可以被远程客户端使用。另外，通过mountd进程，用户可以知道目前有哪些文件系统已被远程文件系统装配，并得知远程客户端的列表。查看mountd是否正常启动起来可以使用命令rpcinfo进行查看，在正常情况下在输出的列表中应该象这样的行： 
 100005 1 udp 1039 mountd 
 100005 1 tcp 1113 mountd 
 100005 2 udp 1039 mountd 
 100005 2 tcp 1113 mountd 
 100005 3 udp 1039 mountd 
 100005 3 tcp 1113 mountd 

而NFS默认是使用111端口，所以我们先要检测是否打开了这个端口




LVS 工作原理基本类似DNAT，又不完全相像，它是一种四层交换，默认情况下来通过用户请求的的地址和端口，来判断用户的请求，从而转发到后台真正提供服务的主机，而判断这种请求的是通过套接字来实现，所以四层就可以实现。

LVS的工作模式
1.DNAT
2.直接路由(DR)
3.隧道(TUN)

VIP：虚IP
RIP：节点IP
DIP：LVS的IP
CIP：客户端请求IP


DR模式
用户的请求必须经过Director，而realserver在响应的使用直接返回请求，Dirtector通过改写请求报文的MAC地址，将请求发送到Real Server,而Real Server将响应直接返回给客户
特点：
1.必须处于同一个物理网络中（连在同一个交换机上）
2.RIP可以使用公网地址（建议使用）
3.Director只转发请求，而realserver直接响应请求而不转发给Director
4.集群节点的网关，不能指向DIP
5.不能做端口转换（不支持）
6.绝大多数的操作系统都可以实现realserver,而realserver需要同一个网卡配置多个Ip地址


TUN模式

LB收到用户请求包后，根据IP隧道协议封装该包，然后传给某个选定的RS；RS解出请求信息，直接将应答内容传给用户。此时要求RS和LB都要支持IP隧道协议。
特点：
1.节点和Director不必在同一个网络
2.RIP必须使用公网地址
3.Director只需要处理进来的请求，不需要处理出去的请求
4.响应的请求一定不能经过Direcor.
5.Directory不支持端口映射
6.只能使用那些支持IP 隧道协议的操作系统做realserver

依赖算法
静态算法：不考虑节点的链接状态
1.Round-robin (RR) 轮询 既第一次访问A，第二次访问B,第三次再访问A…..循环下去
2.Weighted Round-Robin WRR 加权轮询：提高后台服务器的响应能力 根据后方服务器的响应能力，来定义权重，根据权重来转发请求，权重大的优先访问
3.目的地址哈希调度 (Destination Hashing) 以目的地址为关键字查找一个静态hash表来获得需要的RS，来自同一个用户的特定请求转发到固定的指定的主机（比如提供web服务），以提高缓存（网页文件缓存）利用率（命中率）。
4.源地址哈希调度(Source Hashing) 以源地址为关键字查找一个静态hash表来获得需要的RS。

动态算法：
活动连接：后台real-server当前处于活动状态（active）和ESTABLISHEDstate（想关联）的连接，像ssh，或者telnet会一直处于活动状态。
非活动连接：非活动的状态（inactive）或者非FIN的数据包。

1.最小连接数调度(Least-Connection) 同时检查一台主机上的活动连接数和非活动连接数，连接数最少（活动状态的连接数少）的将会接受下一个连接请求。
2.加权最小连接数调度(Weighted Least-Connection) 假设各台RS的权值依次为Wi（I = 1..n），当前的TCP连接数依次为Ti（I＝1..n），依次选取Ti/Wi为最小的RS作为下一个分配的RS
3.基于地址的最小连接数调度(Locality-Based Least-Connection) 将来自同一目的地址的请求分配给同一台RS如果这台服务器尚未满负荷，否则分配给连接数最小的RS，并以它为下一次分配的首先考虑。
4.基于地址的带重复最小连接数调度(Locality-Based Least-Connection with Replication) 对于某一目的地址，对应有一个RS子集。
对此地址的请求，为它分配子集中连接数最小的RS；如果子集中所有的服务器均已满负荷，则从集群中选择一个连接数较小的服务器，将它加入到此子集并分配连接；若一定时间内，这个子集未被做任何修改，则将子集中负载最大的节点从子集删除。




半虚拟化，全虚拟化

半虚拟化，支持通过 http ftp nfs 方式进行安装。
全虚拟化，支持通过 iso文件  光驱   网络pxe 安装。

半虚拟化的意思是需要修改被虚拟系统的内核，以实现系统能被完美的虚拟在Xen上面。完全虚拟化则是不需要修改系统内核则可以直接运行在Xen上面。

PV vs. FV
PV（Para-Vritralization）和FV（Full-Vritralization）的差别，主要以guest OS的硬件仿真程度做区分。
  FV：FV是一般较常看到的作法，所有的guest OS完全不会看到实际的硬件为何，只能使用由Supervisor所提供的所有虚拟硬件，因此，在这种机制下，guest OS动作的性能一定会大受虚拟接口的影响。另外还有一个特点，就是因为完全仿真的关系，不支持新的技术，连ACPI开关机的机制都无法使用，也就是当使用者在FV的guest OS下，若直接触动关机的按钮（这里的按钮是由VMM所提供的，不是主机上的）会直接断电，而不会进行关机程序。

 PV：至于PV的作法，有鉴于一般Virtual Machine工具都是以完全仿真的方式，造成性能上的降低，因此，XEN在设计上，希望各操作系统可以在开发时就已经将XEN的技术包括进去，这样在使用时，就可以用局部仿真的方式，让操作系统可以直接使用到硬件中的CPU、内存等，而不需要通过XEN做仿真的操作。XEN所主推的概念就是，当操作系统默认支持XEN时就可以通过XEN的机制，直接使用到底层的硬件，而不是每个OS都要通过Hypervisor的接口，性能上就可大为提高。

半虚拟化与全虚拟化的区别：
 
    全虚拟化：guest系统每一次对于硬件的请求，都会由hypervisor（系统管理程序）接管，并转换成hypervisor的代码，由hypervisor负责对最终硬件进行操作。好处是guest系统每一次对于物理硬件的请求都会变为对hypervisor管理的虚拟硬件的请求，很好的隔离了guest系统与物理系统，当物理系统进行升级或者硬件更换时，无需修改guest系统。坏处则是每一指令转化会使得资源消耗过多。
 
    半虚拟化：共享当前系统的硬件驱动，只负责guest系统请求的资源调度。好处是避免了对硬件请求的指令重写，提高了系统性能，有效的利用资源。坏处是每一次硬件系统升级要对guest系统改写。
 
半虚拟化工作方式：
半虚拟化由domain zero（零域）和hypervisor共同管理，零域负责guest系统的管理，类似于管理员的角色，hypervisor负责与底层硬件交互。
 
原生式和宿主式：
半虚拟化又分为原生式和宿主式，宿主式最典型的是vmware，vmware运行于一个安装好的操作系统中，并对安装在其上面的其他软件系统进行统一管理，由于宿主操作系统同时可能要运行其他的程序，导致虚拟系统效率不够稳定，优点则是安装方便。原生式比如Xen，由零域对其他guest系统进行管理，由于这种方式不需要一个宿主操作系统，所以原生式与硬件之间的管理层可以更轻薄。原生式类似于安装一个新的操作系统，宿主式类似于安装一个软件。


xen在早期要实现hypervisor功能，IT管理员需要把开源Xen作为主流内核的补丁来安装。xen在内核与硬件之间添加了一个层，可以理解成一个微内核，需要自己调度硬件等等。后来的半虚拟化则是需要客户操作系统本身支持xen技术，就可以直接使用底层硬件，不通过xen来模拟硬件


KVM完全内置于Linux。采用一个全虚拟化。IT管理员只需几个内核模块的支持就能安装KVM，而Linux内核都支持这种hypervisor（系统管理程序）。KVM架构中可以看到，KVM作为一个轻量级的虚拟化管理程序模块，利用Linux做大量hypervisor能做的事情，如任务调度、内存管理与硬件设备交互等，实际就是把内核变成一个hypervisor

