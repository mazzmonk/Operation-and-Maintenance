-bootstrap-expect		# 类似etcd集群,1,3,5个节点,防止脑裂

bin/consul agent \
-bind="172.20.20.3" 	# 必须是固定IP,不能是0.0.0.0
-bootstrap-expect=1   # 集群节点,1,3,5等等,在节点数目没有达到预期,集群不工作
-client="0.0.0.0" 			# 作为client与其他节点通讯
-data-dir="data/" 		# 数据目录
-log-file="logs/consul.log"			# 日志
-pid-file="run/consul.pid" 			# pid
-server		# server角色
-ui 				# 开启ui
-ui-content-path="ui/"		# ui内容目录


注意:
1. 如果有多个节点,后续节点需要加入时,使用-join=172.20.20.3
2. 如果做为client角色使用-retry-join=172.20.20.3,不需要-server参数
3. -datacenter,指定数据中心,用于区别多个consul集群,如果不设置默认为dc1


# server集群建立,可以先每个节点单独启动,然后将某个节点做为加入点,其他的加入此节点
先在172.20.20.3上启动服务,然后在172.20.20.2上启动服务,然后在172.20.20.2上执行如下操作,将172.20.20.2加入到172.20.20.3
$ consul join --http-addr 172.20.20.3:8500 172.20.20.2


Consul集群间使用了GOSSIP协议通信和raft一致性算法.

Agent——agent是一直运行在Consul集群中每个成员上的守护进程.通过运行consul agent来启动.agent可以运行在client或者server模式.指定节点作为client或者server是非常简单的,除非有其他agent实例.所有的agent都能运行DNS或者HTTP接口,并负责运行时检查和保持服务同步.

Client——一个Client是一个转发所有RPC到server的代理.这个client是相对无状态的.client唯一执行的后台活动是加入LAN gossip池.这有一个最低的资源开销并且仅消耗少量的网络带宽.

Server——一个server是一个有一组扩展功能的代理,这些功能包括参与Raft选举,维护集群状态,响应RPC查询,与其他数据中心交互WAN gossip和转发查询给leader或者远程数据中心.

DataCenter——虽然数据中心的定义是显而易见的,但是有一些细微的细节必须考虑.例如,在EC2中,多个可用区域被认为组成一个数据中心.我们定义数据中心为一个私有的,低延迟和高带宽的一个网络环境.这不包括访问公共网络,但是对于我们而言,同一个EC2中的多个可用区域可以被认为是一个数据中心的一部分.

Consensus——一致性,使用Consensus来表明就leader选举和事务的顺序达成一致.为了以容错方式达成一致,一般有超过半数一致则可以认为整体一致.Consul使用Raft实现一致性,进行leader选举,在consul中的使用bootstrap时,可以进行自选,其他server加入进来后bootstrap就可以取消.

Gossip——Consul建立在Serf的基础之上,它提供了一个用于多播目的的完整的gossip协议.Serf提供成员关系,故障检测和事件广播.Serf是去中心化的服务发现和编制的解决方案,节点失败侦测与发现,具有容错、轻量、高可用的特点.

LAN Gossip——它包含所有位于同一个局域网或者数据中心的所有节点.

WAN Gossip——它只包含Server.这些server主要分布在不同的数据中心并且通常通过因特网或者广域网通信.

RPC——远程过程调用.这是一个允许client请求server的请求/响应机制.

在每个数据中心,client和server是混合的.一般建议有3-5台server.这是基于有故障情况下的可用性和性能之间的权衡结果,因为越多的机器加入达成共识越慢.然而,并不限制client的数量,它们可以很容易的扩展到数千或者数万台.

同一个数据中心的所有节点都必须加入gossip协议.这意味着gossip协议包含一个给定数据中心的所有节点.这服务于几个目的：第一,不需要在client上配置server地址.发现都是自动完成的.第二,检测节点故障的工作不是放在server上,而是分布式的.这是的故障检测相比心跳机制有更高的可扩展性.第三：它用来作为一个消息层来通知事件,比如leader选举发生时.

每个数据中心的server都是Raft节点集合的一部分.这意味着它们一起工作并选出一个leader,一个有额外工作的server.leader负责处理所有的查询和事务.作为一致性协议的一部分,事务也必须被复制到所有其他的节点.因为这一要求,当一个非leader得server收到一个RPC请求时,它将请求转发给集群leader.

server节点也作为WAN gossip Pool的一部分.这个Pool不同于LAN Pool,因为它是为了优化互联网更高的延迟,并且它只包含其他Consul server节点.这个Pool的目的是为了允许数据中心能够以low-touch的方式发现彼此.这使得一个新的数据中心可以很容易的加入现存的WAN gossip.因为server都运行在这个pool中,它也支持跨数据中心请求.当一个server收到来自另一个数据中心的请求时,它随即转发给正确数据中想一个server.该server再转发给本地leader.

这使得数据中心之间只有一个很低的耦合,但是由于故障检测,连接缓存和复用,跨数据中心的请求都是相对快速和可靠的.



调用consul api注册
curl -X PUT -d '{"name": "nginx","port": 8080,"tags": ["web"],"checks": [{"http": "http://172.20.20.3:8080/","interval": "5s"}]}' http://172.20.20.2:8500/v1/agent/service/register

查询服务(服务名称nginx,对应上述的"name")
curl http://172.20.20.2:8500/v1/catalog/service/nginx

删除所有的服务
#!/bin/bash

rm -rf id.tmp
curl http://172.20.20.2:8500/v1/agent/services |sed s/"ID"/\\n\"ID\"/g |awk -F"," '{print $1}'|grep ID|sed 's/"ID"":"//g'|sed 's/"//g' > id.tmp

for i in `cat id.tmp`;do
  curl --request PUT http://172.20.20.2:8500/v1/agent/service/deregister/$i
done




# 使用consul-template来生成配置文件的模板

# nginx.conf.ctmpl - 模板文件

{{range services}} {{$name := .Name}} {{$service := service .Name}}		# 循环services,其实就是通过api时候的service,$name值通过api查询到的name
upstream {{$name}} {
  zone upstream-{{$name}} 64k;
  {{range $service}}server {{.Address}}:{{.Port}} max_fails=3 fail_timeout=60 weight=1;
  {{else}}server 127.0.0.1:65535; # force a 502{{end}}
} {{end}}

server {
  listen 80 default_server;

  location / {
    root /usr/share/nginx/html/;
    index index.html;
  }

  location /stub_status {
    stub_status;
  }

{{range services}} {{$name := .Name}}
  location /{{$name}} {
    proxy_pass http://{{$name}};
  }
{{end}}
}

通过
$ curl http://172.20.20.2:8500/v1/catalog/service/nginx
$ curl http://172.20.20.2:8500/v1/catalog/service/consul
得到的service数据
*************************************************************
[{"ID":"ccc66cda-ce6d-4a5c-8a2d-81170d9e8e02","Node":"ubu02","Address":"172.20.20.2","Datacenter":"dc1","TaggedAddresses":{"lan":"172.20.20.2","wan":"172.20.20.2"},"NodeMeta":{"consul-network-segment":""},"ServiceKind":"","ServiceID":"nginx","ServiceName":"nginx","ServiceTags":["web"],"ServiceAddress":"","ServiceWeights":{"Passing":1,"Warning":1},"ServiceMeta":{},"ServicePort":8080,"ServiceEnableTagOverride":false,"ServiceProxy":{"MeshGateway":{}},"ServiceConnect":{},"CreateIndex":53159,"ModifyIndex":53159}]

[{"ID":"87edb238-eb7b-4dd5-4bb3-b5bdf7cd3684","Node":"ubu03","Address":"172.20.20.3","Datacenter":"dc1","TaggedAddresses":{"lan":"172.20.20.3","wan":"172.20.20.3"},"NodeMeta":{"consul-network-segment":""},"ServiceKind":"","ServiceID":"consul","ServiceName":"consul","ServiceTags":[],"ServiceAddress":"","ServiceWeights":{"Passing":1,"Warning":1},"ServiceMeta":{"raft_version":"3","serf_protocol_current":"2","serf_protocol_max":"5","serf_protocol_min":"1","version":"1.6.1"},"ServicePort":8300,"ServiceEnableTagOverride":false,"ServiceProxy":{"MeshGateway":{}},"ServiceConnect":{},"CreateIndex":5,"ModifyIndex":5}]
*************************************************************


# 使用模板生成配置文件
# ./consul-template -consul-addr 172.20.20.3:8500 -template="nginx.conf.ctmpl:default.conf" -once

# 模板生成的配置文件

upstream consul {
  zone upstream-consul 64k;
  server 172.20.20.3:8300 max_fails=3 fail_timeout=60 weight=1;
  
}   
upstream nginx {
  zone upstream-nginx 64k;
  server 127.0.0.1:65535; # force a 502
} 

server {
  listen 80 default_server;

  location / {
    root /usr/share/nginx/html/;
    index index.html;
  }

  location /stub_status {
    stub_status;
  }

 
  location /consul {
    proxy_pass http://consul;
  }
 
  location /nginx {
    proxy_pass http://nginx;
  }

}


# registrator容器启动

docker run -d --name=registrator --net=host  --volume=/var/run/docker.sock:/tmp/docker.sock gliderlabs/registrator:latest --ip 172.20.20.2 -internal    consul://172.20.20.2:8500










