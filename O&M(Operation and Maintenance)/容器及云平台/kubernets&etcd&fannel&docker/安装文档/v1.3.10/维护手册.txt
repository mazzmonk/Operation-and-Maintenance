整个k8s+docker平台，运行的组件都是二进制文件，采用脚本的方式启动，脚本中都写明了启动参数，因此很稳定

1.容器docker维护
1.1 版本更新
  docker的版本更新过程，原则是下载相应新版本的二进制文件，然后更换线上的文件。
步骤：线上的docker相关的文件都在/usr/bin/下，更新前，先关闭docker所在服务器的kubelet,kube-proxy进程，直接杀死进程即可，然后备份所有的/usr/bin/docker*文件，便于升级失败回滚。
  日志切割，docker启动以后的日志文件在$HOME/apps/docker/logs/中，需要根据实际情况切割，比如：每天切割一次，防止日志过大，具体方式，可以用软件或者脚本方式都可以。
1.2 注意事项
  A.在更新docker版本之前，一定需要测试成功，更新的过程是先替换$HOME/bin/srv.docker(docker启动时候用的脚本)文件中的内容，写入新的版本启动用参数，然后替换docker*，执行./srv.docker查看是否有报错

1.3 私有库维护
  由于所有的应用服务及线上编译好的代码都会存储在私有库中，因此，私有库很重要，私有库的服务器采用单独部署的方式，私有库的服务器只启动了docker进程，同时有register容器运行,通过docker ps -a可以看见有容器运行在5000端口上，而私有库服务器使用了GlusterFS分布式存储，保证有多个副本。
  但是如果出现私有库服务器崩溃的情况，会比较麻烦，正常流程是修改所有服务器上的GLOBAL-ENV文件中的export REGISTRY_IPADDRESS="x.x.x.x"为新的IP地址，然后需要重启所有slave上的docker进程，此时，将导致所有的线上的容器停止，并且由于k8s检测到有容器宕掉，会尝试从私有库中拉取镜像，而镜像地址已经不存在，导致混乱。

  解决上述问题的办法
  a方法.就算是服务器崩溃，也一定不要更换ip地址
  b方法.将GLOBAL-ENV文件中的export REGISTRY_IPADDRESS="x.x.x.x"的ip地址改成机器名或者域名，但是必须所有服务器都可以识别到，此种方式建议方式。

2.k8s维护
1.1 版本更新
  更新过程和docker容器相同，更新对应的二进制文件即可。
步骤：master端的组件,对应的文件在$HOME/apps/apps/kubernetes/server/bin/下，包括kube-apiserver,kube-controller-manager,kube-scheduler,kube-dns,更新前备份，同时，kill掉4个进程即可，然后更换文件
  日志切割，日志文件在$HOME/apps/apps/kubernetes/server/logs/下，需要根据实际情况切割。
  注意事项：
  1.需要完成的测试整个k8s所有组件的功能，才可以更新，即替换对应的文件，同时更新对应的$HOME/bin/srv.X启动脚本中的内容。
  2.为了防止单点故障，在备用服务器上也部署了一套上述的master内容，因为目前海南移动云无法使用keepalive,无法完成热切换,因此如果出现主master端宕机故障，修改所有服务器上的GLOBAL-ENV文件中的export KUBE_MASTER_IP="x.x.x.x"为新的master地址，启动新的master端所有进程，
重启所有slave段的kubelet,kube-proxy进程，即杀死，然后启动
  3.slave端的如果出现docker或者k8s组件崩溃的情况下，优先杀死kubelet进程，然后杀死kube-proxy,如此，在master不会在将新的容器分配到此台服务器，并且，master检测到此台服务器无法联系，将会在其他正常的服务器上，建立在此服务器上原先的运行容器，完成故障迁移。

1.2 注意事项
  A.注意和docker版本之间的对应关系，每个版本支持的docker版本是不同的，具体可以查阅官方的文档。
  
   
3.etcd维护
1.1 版本更新
  etcd是整个k8s,flannel的核心，所有的容器，及运行pod的相关ip,机器名，在那台服务器上运行，每台服务器的运行的flannel的信息全部存储在etcd库中，类似一个数据库。
  版本更新的原则是先建立好新的etcd集群，至少3个节点，然后停止所有的master端操作，导出目前线上运行etcd库中的内容，然后导入到当前集群中，启动etcd集群，然后修改所有服务器上GLOBAL-ENV文件中的ETCD_NODE0{1,2,3}_IPADDRESS="x.x.x.x"为新的node01,node02,node03地址,依次重启master端的kube-apiserver服务，以及所有slave端的flannled

1.2 灾难恢复 
如果出现服务器崩溃的情况，假设所有服务器全部崩溃的情况，只能重建，并且用备份的数据导入。
如果单个服务器出现崩溃，原则是先从集群中remove点坏的节点，然后下线服务器

操作：
登录任意一台正常的etcd服务器，进入到$HOME/apps/etcd/bin/下，然后./etcdctl member list查看坏掉etcd节点的ID(第一个字段),
然后./etcdctl member remove xxxxxx  #xxxxxx表示上一步查出来的ID

另外：一定要保证有至少3个节点，否则很大程度会出现脑裂的现象。就是说坏掉一个节点，一定要补充一个，具体的操作见文档etcd安装配置，
然后修改GLOBAL-ENV中的ETCD_NODE0{1,2,3}_IPADDRESS="x.x.x.x"部分，然后重启master端的kube-apiserver服务，以及所有slave端的flannled

1.2 备份

备份命令，登录任意一台etcd服务器，进入到$HOME/apps/etcd/bin/下，执行

./etcdctl backup --data-dir $HOME/apps/kubernetes/etcd/data/ --wal-dir $HOME/apps/kubernetes/etcd/wal/  --backup-dir $HOME/backup/data --backup-wal-dir $HOME/backup/wal/

上述备份数据，可以使用脚本加计划任务的方式备份，一定要保证1小时的备份内容，每5分钟备份一次，将备份的数据同时异地备份。

1.3 日志切割
日志的切割根据实际的情况切割，比如：每小时切割一次，保存3天方案，具体方式，脚本或者软件都可以。



4.flannel维护
更新版本采用上述一样的方式，停止flannel进程，然后更新文件，启动


5.监控
请结合原有的备份方案，优先监控每台物理机或者虚拟机，特别是cpu,mem,disk资源
注意事项：
1.etcd服务器的监控是最重要的，优先级最高
2.REGISTRY服务器的磁盘要持续关注，特别是发布应用版本更新比较频繁的平台。因为发布一个新版本，就会产生一个新的镜像，每个新的镜像会占用磁盘空间。
3.每次包裹应用的镜像要尽量的小，没有必要的包不要安装，比如：vi,vim,ifconfig,编译运行环境库这样的包不要安装，线上运行的服务并不依赖这些包











