hadoop2.7.1取消了集群联盟模式，至少在官方的文档中已经看不见了。

软件环境：
centos 6.7 kernel 2.6.32-573.12.1.el6.x86_64
hadoop-2.7.1.tar.gz
hbase-1.1.2-bin.tar.gz
jdk-7u79-linux-x64.rpm
zookeeper-3.4.7.tar.gz

1.系统环境设置

编辑/etc/profile

####### java env ######
export JAVA_HOME=/usr/java/jdk1.7.0_79
export JRE_HOME=/usr/java/jdk1.7.0_79/jre
export CLASSPATH=/usr/java/jdk1.7.0_79/lib:/usr/java/jdk1.7.0_79/jre/lib:
export PATH=/usr/java/jdk1.7.0_79/bin:$PATH

###### hadoop env ######
export HADOOP_HOME=/home/blue/apps/hadoop
export PATH=$HADOOP_HOME/bin:$PATH

以下所有操作都基于普通账户blue

2 服务器及目录设置
如下内容需要在每台服务器的/etc/hosts中保持一致

192.168.1.1 m1
192.168.1.2 m2
192.168.1.3 s1
192.168.1.4 s2

目录结构
/home/blue/apps
/home/blue/hadoopdata/{hdfs  journal  mapreduce  tmp  zookeeper}

3.hadoop安装配置
3.1 角色及功能介绍
hadoop包括有namenode，datanode，secondary namenode，
resourcemanager，nodemanger，webappproxy，mapreduce job history server这些功能

其中namenode，secondary namenode，resourcemanager，nodemanger，
webappproxy，mapreduce job history server这些功能都在namenode这个角色的服务器完成，
datanode独立。

用官方的说法，为了高可用引入了2个namenode，保证一个active，一个standby
有2中方式来保证上述的功能
High Availability With QJM
High Availability With NFS 

通俗的说，就是采用不同的方式来保证2个namenode的数据一致性，
NFS使用一个共享目录，QJM使用JournalNode来保持数据一致，JournalNode是一个服务，JournalNode群被设计
成至少需要3个，并且必须是奇数个，这样可以保证在(n-1)/2个JournalNode失效的情况下，数据不会丢失，
而多个JournalNode之间的数据一致性，实质是使用日志的方式。

hadoop使用zookeeper这个框架用来保持2个namenode的active<=>standby的自动切换，因此2个namenode上
有ZooKeeper quorum和the ZKFailoverController process (abbreviated as ZKFC).

总结上述功能和机器数量有如下的角色对应
m1  namenode  zkfc
m2  namenode	zkfc	journalnode	
s1  datanode        journalnode
s2  datanode        journalnode

每台服务器运行的进程如下：
m1  hadoop  zookeeper
m2  hadoop  zookeeper
s1  hadoop  zookeeper
s2  hadoop  

在一个完整的全分布式结构中，结构应该如下：
以3个datanode为例

namenode(active)  namenode(standby)
datanode    datanode    datanode 
zookeeper   zookeeper   zookeeper   由于hbase也要用到zookeeper，担任很重要的功能，因此不能合并在其他角色
journalnode journalnode journalnode (此3个节点因为是轻量级，因此可以与上述任意的合并，但是不建议datanode)

就是说至少需要8台机器。

3.2 安装
安装的顺序按照zookeeper->hadoop->hbase的方式。

3.2.1 zookeeper
zookeeper解压以后，保证在conf/zoo.cfg中有如下内容

tickTime=2000
initLimit=5
syncLimit=2
dataDir=/home/blue/hadoopdata/zookeeper
dataLogDir=/home/blue/apps/zookeeper/logs
clientPort=2181

server.1=192.168.1.1:2888:3888
server.2=192.168.1.2:2888:3888
server.3=192.168.1.3:2888:3888

其中第一个端口用来集群成员的信息交换，第二个端口是在leader挂掉时专门用来进行选举leader所用。

另外需要注意：
编辑bin/zkEnv.sh文件的如下部分，保证日志能生成到logs/目录下

if [ "x${ZOO_LOG_DIR}" = "x" ]
then
    ZOO_LOG_DIR="${ZOOKEEPER_PREFIX}/logs"
fi

if [ "x${ZOO_LOG4J_PROP}" = "x" ]
then
    ZOO_LOG4J_PROP="INFO,ROLLINGFILE"

在zookeeper目录下建立logs目录，同时建立/home/blue/hadoopdata/zookeeper目录，在此目录中建立myid
文件，其中按照配置文件中的信息，192.168.1.1此台服务器中填入1，192.168.1.2填入2，192.168.1.3填入3

启动m1，m2，s1服务器上的zookeeper
/home/blue/apps/zookeeper/bin/zkServer.sh start

最后可以通过bin目录下的zkCli.sh来测试

3.2.2 hadoop的配置按照zkfc->journalnode->namenode->datanode的方式

* 总体配置
修改hadoop-env.sh
所有机器

export JAVA_HOME=/usr/java/jdk1.7.0_79
export HADOOP_PID_DIR=/home/blue/hadoopdata/tmp

* 关于zkfc
修改core-site.xml
所有机器

 <property>
    <name>ha.zookeeper.quorum</name>
    <value>m1:2181,m2:2181,s1:2181</value>
  </property>

修改hdfs-site.xml
所有机器

<property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

* 关于journalnode
修改core-site.xml
在m2,s1,s2上

 <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/blue/hadoopdata/journal/node/local/data</value>
  </property>

* 关于namenode
修改core-site.xml
在所有的机器上

<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/home/blue/hadoopdata/tmp</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
  </property>


修改hdfs-site.xml
在m1,m2上
<configuration>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/namenode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

在s1,s2上
<configuration>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/datanode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>


3.2.3 格式化，启动

a.格式化zkfc，启动
bin/hdfs zkfc -formatzk
sbin/hadoop-daemon.sh --script ../bin/hdfs start zkfc

b.启动QJM
sbin/hadoop-daemon.sh start journalnode

c.格式化namenode，启动
bin/hdfs namenode -format mycluster
sbin/start-dfs.sh     //这个命令会启动namenode,datanode,journalnode,zkfc,并且按照上述次序

停止
sbin/stop-dfs.sh

d.单独启动各个服务

$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
$HADOOP_PREFIX/sbin/hadoop-daemons.sh --config $HADOOP_CONF_DIR --script hdfs start datanode





/* 配置文件汇总 */

最终的core-site.xml，hdfs-site.xml文件如下
m1
##########################################################################
core-site.xml

  <property>
    <name>hadoop.tmp.dir</name>
    <value>/home/blue/hadoopdata/tmp</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
  </property>

<!--

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/blue/hadoopdata/journal/node/local/data</value>
  </property>

-->

<!-- automatic failover between two namenodes  -->
  
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>m1:2181,m2:2181,s1:2181</value>
  </property>

##########################################################################
##########################################################################
hdfs-site.xml

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/namenode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

<!-- automatic failover between two namenodes  -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

##########################################################################

m2
##########################################################################
core-site.xml

<property>
    <name>hadoop.tmp.dir</name>
    <value>/home/blue/hadoopdata/tmp</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/blue/hadoopdata/journal/node/local/data</value>
  </property>

<!-- automatic failover between two namenodes  -->
  
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>m1:2181,m2:2181,s1:2181</value>
  </property>

##########################################################################
##########################################################################
hdfs-site.xml

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/namenode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

<!-- automatic failover between two namenodes  -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>



##########################################################################

s1
##########################################################################
core-site.xml

 <property>
    <name>hadoop.tmp.dir</name>
    <value>/home/blue/hadoopdata/tmp</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/blue/hadoopdata/journal/node/local/data</value>
  </property>

<!-- automatic failover between two namenodes  -->
  
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>m1:2181,m2:2181,s1:2181</value>
  </property>
##########################################################################
##########################################################################
hdfs-site.xml

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/datanode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

<!-- automatic failover between two namenodes  -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
##########################################################################

s2
##########################################################################
core-site.xml

<configuration>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/home/blue/hadoopdata/tmp</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mycluster</value>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/blue/hadoopdata/journal/node/local/data</value>
  </property>

<!-- automatic failover between two namenodes  -->
  
  <property>
    <name>ha.zookeeper.quorum</name>
    <value>m1:2181,m2:2181,s1:2181</value>
  </property>
##########################################################################
##########################################################################
hdfs-site.xml

 <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///home/blue/hadoopdata/hdfs/datanode</value>
  </property>

<!-- HA CLUSTER with the Quorum Journal Manager  -->

  <property>
    <name>dfs.nameservices</name>
    <value>mycluster</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.mycluster</name>
    <value>nn1,nn2</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn1</name>
    <value>m1:8020</value>
  </property>
  <property>
    <name>dfs.namenode.rpc-address.mycluster.nn2</name>
    <value>m2:8020</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn1</name>
    <value>m1:50070</value>
  </property>
  <property>
    <name>dfs.namenode.http-address.mycluster.nn2</name>
    <value>m2:50070</value>
  </property>
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://m2:8485;s1:8485;s2:8485/mycluster</value>
  </property> 
  <property>
    <name>dfs.client.failover.proxy.provider.mycluster</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>

<!-- automatic failover between two namenodes  -->
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>

##########################################################################




HBASE1.1.2的完全分布式模式只能建立在HDFS上。
这里说明下zookeeper，hbase依赖于zookeeper保持数据一致型，有2种模式管理zookeeper，
第一种，使用HBASE来管理，第二种，zookeeper自己管理，不依赖HBASE管理
因为zookeeper还需要用来保持hadoop中的namenode之间的高可用，选择第二种

zookeeper的配置前述已经有

环境
m1 192.168.1.1   master
m2 192.168.1.2   back-master
s1 192.168.1.3   regionser
s2 192.168.1.4   regionser


1.HBASE安装，配置
conf/hbase-env.sh

export JAVA_HOME=/usr/java/jdk1.7.0_79
export HBASE_MANAGES_ZK=false                  //不用hbase管理zookeeper

多数的hbase文档都没有使用hadoop高可用模式，都是使用的1个namenode作为master
这里用的hadoop的高可用模式

hbase-site.sh文件及相关的配置如下
建立backup-masters文件，写入
m2
在regionservers文件中写入
s1
s2

hbase-site.xml

<property>
    <name>hbase.rootdir</name>
    <value>hdfs://mycluster/hbase</value>    //关键在这里，高可用集群这里写cluster id，如果是单个namenode这里写此
  </property>                                //namenode的地址和端口，比如: m1:8020

  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>                      //开启hbase的分布式模式
  </property>

  <property>
    <name>hbase.zookeeper.quorum</name>
    <value>m1,m2,s1</value>                //zookeeper节点
  </property>

  <property>
   <name>hbase.zookeeper.property.clientPort</name>      //zookeeper的端口，默认是2181
   <value>2181</value>
  </property>

  <property>
    <name>hbase.zookeeper.property.dataDir</name>
    <value>/home/blue/hadoopdata/zookeeper</value>    //这里比较重要，当zookeeper安装好以后，这个目录必须和zookeeper的data
  </property>                                           //一致

  <property>
    <name>hbase.master.port</name>         //web ui端口
    <value>60000</value>
  </property>

  <property>
    <name>hbase.master.info.port</name>
    <value>60010</value>
  </property>

  <property>
    <name>hbase.regionserver.port</name>
    <value>60020</value>
  </property>
  
  <property>
    <name>hbase.regionserver.info.port</name>
    <value>60030</value>
  </property>


为了能让hbase读取到hdfs的配置，把hdfs-site.xml链接到conf/下

ln -s /home/blue/apps/hadoop/etc/hadoop/hdfs-site.xml /home/blue/apps/hbase/conf/

启动










